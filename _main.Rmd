---
title: "Technical Documentation, State of the Ecosystem Report"
author: "Northeast Fisheries Science Center"
date: "`r format(Sys.Date(), '%e %B %Y')`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: ["introduction.bib", "conceptmods.bib", "trend_analysis.bib", "EPU.bib", "survey_data.bib", "Bennet_indicator.bib", "Revenue_Diversity.bib", "Aquaculture.bib", "NE_HAB.bib", "MAB_HAB.bib", "Comm_rel_vuln.bib", "Comm_climate_vuln.bib", "RW.bib", "thermal_hab_proj.bib", "occupancy.bib", "long_term_sst.bib", "seasonal_sst_anomaly_maps.bib", "Ich_div.bib", "Species_dist.bib", "CHL_PPD.bib", "zooplankton.bib", "Condition.bib", "productivity_tech_memo.bib","aggregate_groups.bib"]
biblio-style: apalike
link-citations: yes
github-repo: NOAA-EDAB/tech-doc
description: "This book documents each indicator and analysis used in State of the Ecosystem reporting"
---
# Introduction {-}

The purpose of this document is to collate the methods used to access, collect, process, and analyze derived data ("indicators") used to describe the status and trend of social, economical, ecological, and biological conditions in the Northeast Shelf Large Marine Ecosystem (see figure, below). These indicators are further synthesized in State of the Ecosystem Reports produced annually by the [Northeast Fisheries Science Center](https://www.nefsc.noaa.gov/) for the [New England Fisheries Management Council](https://www.nefmc.org/) and the [Mid-Atlantic Fisheries Management Council](http://www.mafmc.org/). The metadata for each indicator (in accordance with the [Public Access to Research Results (PARR) directive](http://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/ostp_public_access_memo_2013.pdf)) and the methods used to construct each indicator are described in the subsequent chapters, with each chapter title corresponding to an indicator or analysis present in State of the Ecosystem Reports.

Indicators included in this document were selected to clearly align with management objectives, which is required for integrated ecosystem assessment [@levin_integrated_2009], and has been advised many times in the literature [@degnbol_review_2004; @jennings_indicators_2005; @rice_framework_2005; @link_translating_2005]. A difficulty with pratical implementation of this in ecosystem reporting can be the lack of clearly specified ecosystem-level management objectives (although some have been suggested [@murawski_definitions_2000]). In our case, considerable effort had already been applied to derive both general goals and operational objectives from both US legislation such as the Magnuson-Stevens Fisheries Conservation and Management Act (MSA) and regional sources [@depiper_operationalizing_2017]. These objectives are somewhat general and would need refinement together with managers and stakeholders, however, they serve as a useful starting point to structure ecosystem reporting.

```{r neusmap, echo=F, eval=T, fig.align='center', fig.height=6}

image.dir <- './images'

knitr::include_graphics(file.path(image.dir, 'journal.pone.0146756.g002.PNG'))

```
Map of Northeast U.S. Continental Shelf Large Marine Ecosystem from @Hare2016.


<!-- **References** -->

<!-- Hare JA, Morrison WE, Nelson MW, Stachura MM, Teeters EJ, Griffis RB, et al. (2016) A Vulnerability Assessment of Fish and Invertebrates to Climate Change on the Northeast US Continental Shelf. PLoS ONE 11(2): e0146756. https://doi.org/10.1371/journal.pone.0146756 -->

<!-- -------------- -->

<!--chapter:end:index.Rmd-->

# Data and Code Access {#erddap}

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)


```


### About

The Technical Documentation for the State of the Ecosystem reports is a [bookdown](https://bookdown.org) document; hosted on the NOAA Northeast Fisheries Science Center Ecosystems Dynamics and Assessment Branch [Github page](https://github.com/NOAA-EDAB), and developed in R. Derived data used to populate figures in this document are queried directly from the NEFSC [ERDDAP server](https://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000) using the R package [rerddap](https://cran.r-project.org/web/packages/rerddap/vignettes/Using_rerddap.html).  

### Accessing source data and build code

Source data for the derived indicators in this document are linked to in the text unless there are privacy concerns involved. In that case, it may be possible to access source data by reaching out to the Point of Contact associated with that data set. Derived data sets make up the majority of the indicators present in the State of the Ecosystem reports. The majority of derived data sets included in State of the Ecosystem reports are available for download through the NEFSC [ERDDAP server](https://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000), although some data is also present in the [Github repository](https://github.com/NOAA-EDAB/tech-doc) associated with this project. 

In this technical documentation, we hope to shine a light on the processing and analytical steps involved to get from source data to final product. This means that whenever possible, we have included the code involved in source data extraction, processing, and analyses. We have also attempted to thoroughly describe all methods in place of or in supplement to provided code. 

There are multiple R scripts sourced throughout this document in an attempt to keep code concise. These scripts include [BasePlot_source.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/BasePlot_source.R), [GIS_source.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/GIS_source.R), and [get_erddap.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/get_erddap.R). These scripts along with the code presented in sections titled "Plotting" may be used to recreate the majority of figures present in State of the Ecosystem reports. The script [get_erddap.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/get_erddap.R) is used to both query individual data sets and to build a parent data set containing all indicator data. The exact usage in the latter case is shown in the R code block below. For querying individual data sets using get_erddap.R, see the [single-species status indicator](#stockstatus).

### Building the document

Start a local build of the SOE bookdown document by first cloning the project's associated [git repository](https://github.com/NOAA-EDAB/tech-doc). Next, if you would like to build a past version of the document, use `git checkout [version_commit_hash]` to revert the project to a past commit of interest after cloning the repository, and set `build_latest <- FALSE` in the following code chunk. This will ensure the project builds from a cached data set, and not the most updated versions present on the NEFSC ERDDAP server. Once the `tech-doc.Rproj` file is opened in RStudio, run `bookdown::serve_book()` from the console to build the document. 

```{r query, echo = T, eval = T, message=F, warning=F}

build_latest <- FALSE

if (build_latest){
  #Libraries
  library(dplyr)
  library(rerddap)
  library(stringr)
  library(data.table)
  
  # Relative working directories
  data.dir  <- here::here('data')
  r.dir <- here::here('R')
  
  #Source function for querying ERDDAP server
  source(file.path(r.dir,"get_erddap.R"))
  
  #Set URL for COMET (server where NEFSC ERDDAP lives)
  comet <- 'https://comet.nefsc.noaa.gov/erddap/'
  
  #List datasets on the NEFSC ERDDAP
  tab_list <- ed_datasets(url = comet)
  
  #Get updated data set IDs
  erddap_datasets <- tab_list %>% 
    filter(str_detect(Dataset.ID, "soe_v")) %>% 
    get_erddap(id = NULL)
  
  #Save and clean updated IDs for use in rest of report
  save(erddap_datasets, file = file.path(data.dir, "ERDDAP_datasets.Rdata"))
  
  # Exclude stock assessment status data, which have unique structure
  erddap_datasets <- erddap_datasets %>%
    dplyr::filter(!str_detect(Dataset.ID, "assess")) 
  
  #Create SOE parent data set, filter out NAs. This queries based on 
  #data set IDs that were collected above
  SOE.data.erd <- sprintf("http://comet.nefsc.noaa.gov/erddap/tabledap/%s.csv",
                           erddap_datasets$Dataset.ID) %>% 
    purrr::map(function(x) {
      readr::read_csv(url(x))
    }) %>% 
    do.call(rbind,.) %>% 
    mutate(Value = as.numeric(Value)) %>% 
      dplyr::filter(!is.na(Value))
  
  #Convert to data.table
  SOE.data <- as.data.table(SOE.data.erd)
  
  #Save data
  save(SOE.data, file = file.path(data.dir,"SOE_data_erddap.Rdata"))
}
```

#### A note on data structures

The majority of the derived time series used in State of the Ecosystem reports are in long format. This approach was taken so that all disparate data sets could be "bound" together for ease of use in our base plotting [functions]((https://github.com/NOAA-EDAB/tech-doc/blob/master/R/BasePlot_source.R)).

<!--chapter:end:chapters/erddap_query_and_build.Rmd-->

# Conceptual Models

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

data.dir  <- './data'
image.dir <- './images'

```

**Description**: Conceptual models for the New England (Georges Bank and Gulf of Maine) and Mid-Atlantic regions of the Northeast US Large Marine Ecosystem

**Indicator category**: Synthesis of published information, Extensive analysis; not yet published

**Contributor(s)**: Sarah Gaichas, Patricia Clay, Geret DePiper, Gavin Fay, Michael Fogarty, Paula Fratantoni, Robert Gamble, Sean Lucey, Charles Perretti, Patricia Pinto da Silva, Vincent Saba, Laurel Smith, Jamie Tam, Steve Traynor, Robert Wildermuth 

**Data steward**: Sarah Gaichas, <sarah.gaichas@noaa.gov>

**Point of contact**: Sarah Gaichas, <sarah.gaichas@noaa.gov>

**Public availability statement**: All source data aside from confidential commercial fisheries data (relevant only to some components of the conceptual models) are available to the public (see Data Sources below).


## Methods
Conceptual models were constructed to facilitate multidisciplinary analysis and discussion of the linked social-ecological system for integrated ecosystem assessment. The overall process was to first identify the components of the model (focal groups, human activities, environmental drivers, and objectives), and then to document criteria for including groups and linkages and what the specific links were between the components.

The prototype conceptual model used to design Northeast US conceptual models for each ecosystem production unit (EPU) was designed by the California Current IEA program. The California Current IEA developed an [overview conceptual model for the Northern California Current Large Marine Ecosystem (NCC)](https://www.integratedecosystemassessment.noaa.gov/Assets/iea/california/conceptual-models/Integrated-SocioEcological-System-Overview6.png), with models for each [focal ecosystem component](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-overview.html#) that detailed the [ecological](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-ecological.html#), [environmental](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-environmental.html#), and [human system](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-human.html#) linkages. Another set of conceptual models outlined [habitat](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/mediating-components/habitat.html) linkages. 

An inital conceptual model for Georges Bank and the Gulf of Maine was outlined at the 2015 ICES WGNARS meeting. It specified four categories: Large scale drivers, focal ecosystem components, human activities, and human well being. Strategic management objectives were included in the conceptual model, which had not been done in the NCC. Focal ecosystem components were defined as aggregate species groups that had associated US management objectives (outlined within WGNARS for IEAs, see @depiper_operationalizing_2017): groundfish, forage fish, fished invertebrates, living habitat, and protected species. These categories roughly align with Fishery Managment Plans (FMPs) for the New England Fishery Management Council. The Mid-Atlantic conceptual model was developed along similar lines, but the Focal groups included demersals, forage fish, squids, medium pelagics, clams/quahogs, and protected species to better align with the Mid Atlantic Council's FMPs.

```{r draftmod, echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GBGOMconceptual1.png'))
```

After the initial draft model was outlined, working groups were formed to develop three submodels following the CCE example: ecological, environmental, and human dimensions. The general approach was to specify what was being included in each group, what relationship was represented by a link between groups, what threshold of the relationship was used to determine whether a relationship was significant enough to be included (we did not want to model everything), the direction and uncertainty of the link, and documentation supporting the link between groups. This information was recorded in a [spreadsheet](https://comet.nefsc.noaa.gov/erddap/tabledap/concept_model_2018.html). Submodels were then merged together by common components using the "merge" function in the (currently unavailable) desktop version of Mental Modeler (http://www.mentalmodeler.org/#home; @gray_mental_2013). The process was applied to Georges Bank (GB), the Gulf of Maine (GOM), and the Mid-Atlantic Bight (MAB) [Ecological Production Units](#epu). 

### Data sources

#### Ecological submodels
Published food web (EMAX) models for each subregion [@link_documentation_2006; @link_northeast_2008], food habits data collected by NEFSC trawl surveys [@smith_trophic_2010], and other literature sources [@smith_consumption_2015] were consulted. Expert judgement was also used to adjust historical information to current conditions, and to include broad habitat linkages to Focal groups. 

#### Environmental submodels
Published literature on the primary environmental drivers (seasonal and interannual) in each EPU was consulted. 
Sources for Georges Bank included @backus_georges_1987 and @townsend_oceanography_2006. 
Sources for the Gulf of Maine included @smith_mean_1983, @smith_interannual_2001, @mupparapu_role_2002, @townsend_oceanography_2006, @smith_regime_2012, and @mountain_labrador_2012.  
Sources for the Mid Atlantic Bight included @houghton_middle_1982, @beardsley_nantucket_1985, @lentz_climatology_2003, @mountain_variability_2003,   @glenn_biogeochemical_2004, @sullivan_evidence_2005, @castelao_seasonal_2008, @shearman_long-term_2009, @castelao_temperature_2010, @gong_seasonal_2010, @gawarkiewicz_direct_2012, @forsyth_recent_2015, @fratantoni_description_2015, @zhang_dynamics_2015, @miller_state-space_2016, and @lentz_seasonal_2017.

#### Human dimensions submodels
Fishery catch and bycatch information was drawn from multiple regional datasets, incuding the Greater Atlantic Regional Office Vessel Trip Reports & Commercial Fisheries Dealer databases, Northeast Fishery Observer Program & Northeast At-Sea Monitoring databases, Northeast Fishery Science Center Social Sciences Branch cost survey, and the Marine Recreational Informational Program database. Further synthesis of human welfare derived from fisheries was drawn from @fare_adjusting_2006, @walden_productivity_2012, @lee_inverse_2013, @lee_hedonic_2014, and @lee_applying_2017. Bycatch of protected species was taken from @waring_us_2015, with additional insights from @bisack_measuring_2014. The top 3 linkages were drawn for each node. For example, the top 3 recreational species for the Mid-Atlantic were used to draw linkages between the recreational fishery and species focal groups. A similar approach was used for relevant commercial fisheries in each region.

Habitat-fishery linkages were drawn from unpublished reports, including:  

1. Mid-Atlantic Fishery Management Council. 2016. Amendment 16 to the Atlantic Mackerel, Squid, and Butterfish Fishery Management Plan: Measures to protect deep sea corals from Impacts of Fishing Gear. Environmental Assessment, Regulatory Impact Review, and Initial Regulatory Flexibility Analysis. Dover, DE. August, 2016. 

2. NOAA. 2016. Deep sea coral research and technology program 2016 Report to Congress. http://www.habitat.noaa.gov/protection/corals/deepseacorals.html retrieved February 8, 2017.  

3. New England Fishery Management Council. 2016. Habitat Omnibus Deep-Sea Coral Amendment: Draft. http://www.nefmc.org/library/omnibus-deep-sea-coral-amendment Retrieved Feb 8, 2017.

4. Bachman et al. 2011. The Swept Area Seabed Impact (SASI) Model: A Tool for Analyzing the Effects of Fishing on Essential Fish Habitat. New England Fisheries Management Council Report. Newburyport, MA.

Tourism and habitat linkages were drawn from unpublished reports, including: 

1. http://neers.org/RESOURCES/Bibliographies.html                               

2. Great Bay (GoM) resources  http://greatbay.org/about/publications.htm        

3. Meaney, C.R. and C. Demarest. 2006. Coastal Polution and New England Fisheries. Report for the New England Fisheries Management Council. Newburyport, MA.

4. List of valuation studies, by subregion and/or state, can be found at http://www.oceaneconomics.org/nonmarket/valestim.asp.

Published literature on human activities in each EPU was consulted. 

Sources for protected species and tourism links included @hoagland_demand_2000 and @lee_economic_2010. 

Sources for links between environmental drivers and human activities included @adams_uncertainty_1973, @matzarakis_proceedings_2001, @scott_climate_2004, @hess_climate_2008, @colburn_social_2012, @jepson_development_2013, and @colburn_indicators_2016. 

Sources for cultural practices and attachments links included @pauly_putting_1997, @mcgoodwin_understanding_2001, @st_martin_making_2001, @norris-raynbird_for_2004, @pollnac_toward_2006, @clay_defining_2007, @clay_definingfishing_2008, @everett_role_2008, @donkersloot_politics_2010, @lord_understanding_2011, @halpern_index_2012, @wynveen_natural_2012, @cortes-vazquez_identity_2013, @koehn_progress_2013, @potschin_landscapes_2013, @reed_beyond_2013, @urquhart_constructing_2013, @blasiak_paradigms_2014, @klain_what_2014, @poe_cultural_2014, @brown_we_2015, @donatuto_evaluating_2015, @khakzad_role_2016, @oberg_surviving_2016, and @seara_perceived_2016.  

### Data extraction 

#### Ecological submodels
"Data" included model estimated quantities to determine whether inclusion thresholds were met for each potential link in the conceptual model. A matrix with diet composition for each modeled group is an input to the food web model. A matrix of mortalities caused by each predator and fishery on each modeled group is a direct ouput of a food web model (e.g. Ecopath). Food web model biomasss flows between species, fisheries, and detritus were summarized using algorithms implemented in visual basic by Kerim Aydin, NOAA NMFS Alaska Fisheries Science Center. Because EMAX model groups were aggregated across species, selected diet compositions for individual species were taken from the NEFSC food habits database using the FEAST program for selected species (example query below). These diet queries were consulted as supplemental information. 

Example FEAST sql script for Cod weighted diet on Georges Bank. Queries for different species are standardized by the FEAST application and would differ only in the svspp code. 
```{sql FEAST, eval = F, echo = T}
Select svspp,year,cruise6,stratum,station,catsex,pdid,pdgutw,pdlen,pdwgt,perpyw,pyamtw,COLLCAT,numlen,pyamtv  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station,pdid,COLLCAT
Select distinct svspp,year,cruise6,stratum,station from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station
Select distinct svspp,year,cruise6,stratum,station,catsex,catnum from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station
Select distinct COLLCAT from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by COLLCAT
Select distinct svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,pdgutw,pdwgt  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station,catsex,pdid
Select svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT,sum(perpyw),sum(pyamtw),sum(pyamtv)  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT order by svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT
Select svspp,year,cruise6,stratum,station,COLLCAT,sum(pyamtv) sumpvol from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,COLLCAT order by svspp,year,cruise6,stratum,station,COLLCAT
Select svspp,year,cruise6,stratum,station, count(distinct pdid) nstom  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,catsex order by svspp,year,cruise6,stratum,station
Select svspp,year,cruise6,stratum,station,pdlen,numlen,count(distinct pdid) nstom  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and numlen is not null and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,pdlen,numlen,catsex order by svspp,year,cruise6,stratum,station,pdlen
Select svspp,year,cruise6,stratum,station,pdlen,COLLCAT,sum(pyamtv) sumpvol  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,pdlen,COLLCAT order by svspp,year,cruise6,stratum,station,pdlen,COLLCAT
Select distinct svspp,year,cruise6,stratum,station,pdid,pdlen from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and numlen is null and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB'
Select distinct year,cruise6,stratum,station,beglat,beglon  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by year,cruise6,stratum,station

```


#### Environmental submodels
Information was synthesized entirely from published sources and expert knowledge; no additional data extraction was completed for the environmental submodels.

#### Human dimensions submodels
Recreational fisheries data were extracted from the 2010-2014 MRIP datasets. Original data can be found [here]( data/top10_prim1_common_mode.xlsx) for each region (New England or Mid-Atlantic as defined by states). 

Commercial fishing data was developed as part of the State of the Ecosystem Report, including revenue and food production estimates, with data extraction metodology discussed in the relevant sections of the technical memo. In addition, the Northeast Regional Input/Output Model [@steinback_scott_northeast_2006] was used as the basis for the strength of the employment linkages.

### Data analysis
<!--Text description of analysis methods, similar in structure and detail to a peer-reviewed paper methods section.-->
#### Ecological submodels
Aggregated diet and mortality information was examined to determine the type of link, direction of link, and which links between which groups should be inclded in the conceptual models. Two types of ecological links were defined using food web models: prey links and predation/fishing mortality links. Prey links resulted in positve links between the prey group and the focal group, while predation/fishing mortality links resulted in negative links to the focal group to represent energy flows. The intent was to include only the most important linkages between focal groups and with other groups supporting or causing mortality on focal species groups. Therefore, threshold levels of diet and mortality were established (based on those that would select the top 1-3 prey and predators of each focal group): 10% to include a link (or add a linked group) in the model and 20% to include as a strong link. A Primary Production group was included in each model and linked to pelagic habitat to allow environmental effects on habitat to be connected to the ecologial submodel. Uncertainty for the inclusion of each link and for the magnitude of each link was qualitatively assessed and noted in the [spreadsheet](https://comet.nefsc.noaa.gov/erddap/tabledap/concept_model_2018.html). 

Four habitat categories (Pelagic, Seafloor and Demersal, Nearshore, and Freshwater and Estuarine) were included in ecological submodels as placeholders to be developed further along with habitat-specific research. Expert opinion was used to include the strongest links between each habitat type and each Focal group (noting that across species and life stages, members of these aggregate groups likely occupy many if not all of the habitat types). Link direction and strength were not specified. Environmental drivers were designed to link to habitats, rather than directly to Focal groups, to represent each habitat's important mediation function.

EMAX model groups were aggregated to focal groups for the Georges Bank (GB), Gulf of Maine (GOM) and Mid-Atlantic Bight (MAB) conceptual models according to Table \@ref(tab:groups). "Linked groups" directly support or impact the Focal groups as described above.

```{r groups,eval = T, echo = F}
#read in EMAXconceptualmodgroups.csv and kable it
emaxgroups <- read.csv(file.path(data.dir, "EMAXconceptualmodgroups.csv"))
names(emaxgroups) <- c("Group Type", "Region", "Conceptual model group", "EMAX group(s)", "Notes")
kable(emaxgroups, caption="Relationship between food web model groups and conceptual model focal groups")
```


Ecological submodels were constructed and visualized in Mental Modeler (Fig. \@ref(fig:draftGOMeco)). Here, we show only the Gulf of Maine submodels as examples.

```{r draftGOMeco, fig.cap="Gulf of Maine Ecological submodel", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MM_GoM_Ecological.png'))
```

#### Environmental submodels
Environmental submodels were designed to link key oceanographic processes in each ecosystem production unit to the four general habitat categories (Pelagic, Seafloor and Demersal, Nearshore, and Freshwater and Estuarine) with emphasis on the most important physical processes in each ecosystem based on expert knowledge as supported by literature review. The basis of each submodel were environmental variables observable at management-relevant scales as identified by [WGNARS](http://ices.dk/sites/pub/Publication%20Reports/Expert%20Group%20Report/SSGRSP/2014/WGNARS14.pdf): Surface and Bottom Water Temperature and Salinity, Freshwater Input, and Stratification (as well as sea ice timing and cover, which is not relevant to the northeast US shelf). Key drivers changing these observable variables and thus structuring habitat dynamics in each [Ecological Production Units](#epu) were added to the model using expert consensus. 

Environmental submodels were initially constructed and visualized in Mental Modeler (Fig. \@ref(fig:draftGOMenv)).
```{r draftGOMenv, fig.cap="Gulf of Maine Environmental submodel", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MM_GoM_Climate.png'))
```

#### Human dimensions submodels
The top 3 species from each mode of recreational fishing (shoreside, private boat, party/charter) were used to assess the potential for missing links between the recreational fishing activity and biological focal components. Given the predominance of Mid-Atlantic groundfish in recreational fishing off New England (summer flounder, bluefish, striped bass), a Mid-Atlantic groundfish focal component was added to the Georges Bank EPU model. The magnitude of benefits generated from recreational fishing was scaled to reflect expert knowledge of target species, coupled with the MRIP data highlighted above. Scales were held consistent across the focal components within recreational fishing.

No additional biological focal components were added to the commercial fishing activity, beyond what was developed in the ecological submodel. Benefits derived from commercial fishing were scaled to be consistent with the State of the Ecosystem revenue estimates, as modulated by expert knowledge and additional data sources. For example,the percentage of landings sold as food was used to map fishing activity to the commercial fishery food production objective, and the Northeast Regional Input/Output Model [@steinback_scott_northeast_2006] was used to define the strength of the employment linkages. For profitability, expert knowledge was used to reweight revenue landings, based on ancillary cost data available [@das_chhandita_northeast_2013; @das_chhandita_overview_2014]. Human activities and objectives for the conceptual sub model are defined in @depiper_operationalizing_2017. As shown in Figure \@ref(fig:draftGOMhuman), human dimensions submodels were also initially constructed and visualized in Mental Modeler.

```{r draftGOMhuman, fig.cap="Gulf of Maine Human dimensions submodel", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MM_GoM_Human_Connections.png'))
```

#### Merged models
All links and groups from each submodel were preserved in the full merged model for each system. Mental modeler was used to merge the submodels. Full models were then re-drawn in Dia (http://dia-installer.de/) with color codes for each model component type for improved readability. Examples for each system are below. 

```{r diaGB, fig.cap="Georges Bank conceptual model", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GBoverview5.png'))
```


```{r diaGOM, fig.cap="Gulf of Maine conceptual model", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GoMoverview4.png'))
```


```{r diaMAB, fig.cap="Mid-Atlantic Bight conceptual model", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MAB_3.png'))
```

#### Communication tools
The merged models were redrawn for use in communications with the public. These versions lead off the State of the Ecosystem reports for both Fishery Management Councils to provide an overview of linkages between environmental drivers, ecological, and human systems. 

```{r prettyNE, fig.cap="New England conceptual model for public communication", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GOM_GB_conmod_overview.jpg'))
```

```{r prettyMA, fig.cap="Mid-Atlantic conceptual model for public communication", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MAB_conmod_overview.jpg'))
```

<!--
What packages or libraries did you use in your work flow?
```{r, echo = T}
sessionInfo(package = NULL)


#Use this to output a detailed list of the package information
current.session <- sessionInfo(package = NULL)
current.session$otherPkgs
```


Include accompanying R code, pseudocode, flow of scripts, and/or link to location of code used in analyses.
```{r, echo = T, eval = F}
# analysis code
```
-->












<!--chapter:end:chapters/conceptualmodels.Rmd-->

# Trend Analysis

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Time series trend analysis

**Indicator category**: Extensive analysis, not yet published

**Contributor(s)**: Sean Hardison, Charles Perretti, Geret DePiper

**Data steward**: NA

**Point of contact**: Sean Hardison, sean.hardison@noaa.gov

**Public availability statement**: NA


## Methods
Summarizing trends for ecosystem indicators is desirable, but the power of statistical tests to detect a trend is hampered by low sample size and autocorrelated observations [see @Nicholson2004; @Wagner2013; @VonStorch1999a].  Prior to 2018, time series indicators in State of the Ecosystem reports were presented with trend lines based on a Mann-Kendall test for monotonic trends to test significance (p < 0.05) of both long term (full time series) and recent (2007â€“2016) trends, although not all time series were considered for trend analysis due to limited series lengths. There was also concern that a Mann-Kendall test would not account for any autocorrelation present in SOE indicators.

In a simulation study [@HardisonInReview], we explored the effect of time series length and autocorrelation strength on statistical power of three trend detection methods: a generalized least squares model selection approach, the Mann-Kendall test, and Mann-Kendall test with trend-free pre-whitening. Methods were applied to simulated time series of varying trend and autocorrelation strengths. Overall, when sample size was low (N = 10) there were high rates of false trend detection, and similarly, low rates of true trend detection. Both of these forms of error were further amplified by autocorrelation in the trend residuals. Based on these findings, we selected a minimum series length of N = 30 for indicator time series before assessing trend.

We also chose to use a GLS model selection (GLS-MS) approach to evaluate indicator trends in the 2018 (and future) State of the Ecosystem reports, as this approach performed best overall in the simulation study. GLS-MS also allowed for both linear and quadratic model fits and quantification of uncertainty in trend estimates. The model selection procedure for the GLS approach fits four models to each time series and selects the best fitting model using AICc. The models are, 1) linear trend with uncorrelated residuals, 2) linear trend with correlated residuals, 3) quadratic trend with uncorrelated residuals, and 4) quadratic trend with correlated residuals. I.e., the models are of the form

$$ Y_t = \alpha_0 + \alpha_1X_t + \alpha_2X_t^2 + \epsilon_t$$
$$\epsilon_t = \rho\epsilon_{t-1} + \omega_t$$

$$w_t \sim N(0, \sigma^2)$$

Where $Y_t$ is the observation in time $t$, $X_t$ is the time index, $\epsilon_t$ is the residual in time $t$, and $\omega_t$ is a normally distributed random variable. Setting $\alpha_2 = 0$ yields the linear trend model, and $\rho = 0$ yields the uncorrelated residuals model.

The best fit model was tested against the null hypothesis of no trend through a likelihood ratio test (p < 0.05). All models were fit using the R package *nlme* [@Pinheiro2017] and AICc was calculated using the R package *AICcmodavg* [@Mazerolle2017a]. In SOE time series figures, significant positive trends were colored orange, and negative trends purple. 

### Data source(s)
NA

### Data extraction
NA

### Data analysis


```{r, echo=T, message=FALSE, warning=FALSE, include=T}

#R packages
library(dplyr)
library(nlme)
library(AICcmodavg)
library(data.table)
```

```{r, echo = T, message= F, warning=F, include=T}

# data.dir <- "./data"
# load(file.path(data.dir, "SOE_data_2018.Rdata"))

#--------------------------------GLS Model Selection-----------------------------#
fit_lm <- function(dat) {

  constant_norm <-
    nlme::gls(series ~ 1, 
              data = dat)
  
  constant_ar1 <-
    try(nlme::gls(series ~ 1,
                  data = dat,
                  correlation = nlme::corAR1(form = ~time)))
  if (class(constant_ar1) == "try-error"){
    return(best_lm <- data.frame(model = NA,
                                 aicc  = NA,
                                 coefs..Intercept = NA,
                                 coefs.time = NA,
                                 coefs.time2 = NA,
                                 pval = NA)) 
  } 
  
  
  
  # Linear model with normal error
  linear_norm <- 
    nlme::gls(series ~ time, 
              data = dat)
  
  # Linear model with AR1 error
  linear_ar1 <- 
    try(nlme::gls(series ~ time, 
                  data = dat,
                  correlation = nlme::corAR1(form = ~time)))
  if (class(linear_ar1) == "try-error"){
    return(best_lm <- data.frame(model = NA,
                                 aicc  = NA,
                                 coefs..Intercept = NA,
                                 coefs.time = NA,
                                 coefs.time2 = NA,
                                 pval = NA))
    
  }
  
  # Polynomial model with normal error
  dat$time2 <- dat$time^2
  poly_norm <- 
    nlme::gls(series ~ time + time2, 
              data = dat)
  
  # Polynomial model with AR1 error
  poly_ar1 <- 
    try(nlme::gls(series ~ time + time2, 
                  data = dat,
                  correlation = nlme::corAR1(form = ~time)))
  if (class(poly_ar1) == "try-error"){
    return(best_lm <- data.frame(model = NA,
                                 aicc  = NA,
                                 coefs..Intercept = NA,
                                 coefs.time = NA,
                                 coefs.time2 = NA,
                                 pval = NA))
    
  }
  
  # Calculate AICs for all models
  df_aicc <-
    data.frame(model = c("poly_norm",
                         "poly_ar1",
                         "linear_norm",
                         "linear_ar1"),
               aicc  = c(AICc(poly_norm),
                         AICc(poly_ar1),
                         AICc(linear_norm),
                         AICc(linear_ar1)),
               coefs = rbind(coef(poly_norm),
                             coef(poly_ar1),
                             c(coef(linear_norm), NA),
                             c(coef(linear_ar1),  NA)),
               # Calculate overall signifiance (need to use
               # ML not REML for this)
               pval = c(anova(update(constant_norm, method = "ML"),
                              update(poly_norm, method = "ML"))$`p-value`[2],
                        anova(update(constant_ar1, method = "ML"),
                              update(poly_ar1, method = "ML"))$`p-value`[2],
                        anova(update(constant_norm, method = "ML"),
                              update(linear_norm, method = "ML"))$`p-value`[2],
                        anova(update(constant_ar1, method = "ML"),
                              update(linear_ar1, method = "ML"))$`p-value`[2]))
  
  best_lm <-
    df_aicc %>%
    dplyr::filter(aicc == min(aicc))
  
  
  if (best_lm$model == "poly_norm") {
    model <- poly_norm
  } else if (best_lm$model == "poly_ar1") {
    model <- poly_ar1
  } else if (best_lm$model == "linear_norm") {
    model <- linear_norm
  } else if (best_lm$model == "linear_ar1") {
    model <- linear_ar1
  }
  
  return(list(p = best_lm$pval,
              model = model))
}

#-------------------------------------Plotting code------------------------------------#
soe.plot <- function(data, x.var, y.var, x.label = '', y.label = '', tol = 0.1,
                     x.start = NA, x.end = NA, end.start = 2008, bg.col = background, mean_line = T,
                     end.col = recent, stacked = NA, x.line = 2.5, y.line = 3.5, scale.axis = 1,
                     rel.y.num = 1.5, rel.y.text = 1.5, suppressAxis = FALSE,status  = F,anomaly = F,
                     endshade = TRUE, full.trend = TRUE, point.cex = 1.5, lwd = 2, ymax = TRUE,ymin = TRUE,
                     y.upper = y.upper, y.lower = y.lower, extra = FALSE, x.var2 = x.var2, y.var2 = y.var2,
                     line.forward = FALSE, mean_line.2 = T, cex.stacked = 1, website = T) {
  

  #Select Data
  x <- data[Var == y.var, ]
  x <- x[order(x[, get(x.var)]), ]
  setnames(x, x.var, 'X')
  
  #Set common time step if necessary
  if(is.na(x.start)) x.start <- min(x[, X])
  if(is.na(x.end))   x.end   <- max(x[, X])
  x <- x[X >= x.start, ]
  
  #Set up plot parameters
  if (ymax == TRUE){
    y.max <- max(x[, Value]) + tol * max(x[, Value])
  } else {
    y.max <- as.numeric(y.upper)
  }
  
  if (ymin == TRUE){
    y.min <- min(x[, Value]) - tol * abs(min(x[, Value]))
  } else if (ymin == FALSE){
    y.min <- as.numeric(y.lower)
  }
  
  y.mean <- mean(x[, Value])
  y.sd <- sd(x[, Value])
  
  #Plot blank plot
  plot(x[X >= x.start, list(X, Var)], xlim = c(x.start, x.end),
       ylim = c(y.min,y.max), xlab = '', ylab = '', axes = F, ty = 'n')


  #Add background
  u <- par('usr')
  rect(u[1], u[3], u[2], u[4], border = NA, col = bg.col)
  
  #Add end period shading
  if (endshade == TRUE){
    rect(end.start - 0.5, u[3], u[2], u[4], border = NA, col = end.col)
  }
  
  #Add mean line
  if (anomaly == F){
      if (mean_line == TRUE){
      abline(h = y.mean, col = 'grey', lwd = 3, lty = 2)
      } 
  } else if (anomaly == TRUE){
      abline(h = 0, col = 'grey', lwd = 3, lty = 2)
  }
  
  #Add x y lines
  abline(h = u[3], lwd=3)
  abline(v = u[1], lwd=3)
  
  #Add data points/lines
  points(x[, list(X, Value)], pch = 16, cex = point.cex)
  lines( x[, list(X, Value)], lwd = lwd)
  
  #extra lines
  if (extra == TRUE){
    x2 <- data[Var == y.var2, ]
    x2 <- x2[order(x2[, get(x.var2)]), ]
    setnames(x2, x.var2, 'X2')
    x2 <- x2[X2 >= x.start, ]
    if (mean_line.2 == TRUE){
     abline(h = mean(x2[, Value]), col = 'lightcoral', lwd = 3, lty = 2) 
    }
    points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = "indianred")
    lines( x2[, list(X2, Value)], lwd = lwd, col = "indianred")
    }
    
  
  #Add axis
  if (suppressAxis == FALSE){
    if(is.na(stacked)) axis(1, cex.axis = 1)
    if(!is.na(stacked)){
      if(stacked!= 'A') axis(3, cex.axis = 1.5, tck = 0.1, labels = F)
    }
  }

  #Stacked axes with 0 overlap so need to remove
  labels <- round((axTicks(2) / scale.axis), 5)
  if(labels[1] == 0) labels[1] <- ''
  axis(2, at = axTicks(2), labels = labels, cex.axis = rel.y.num,
       las = T)

    #Add axis labels
    if(!website){
      if(!is.na(stacked)) text(u[1], u[4], labels = stacked, cex = cex.stacked, adj = c(-0.5, 1.5))
    } else if (website){
      text(u[1], u[4], labels = "", cex = cex.stacked, adj = c(-0.5, 1.5))
    }
    if(is.na(stacked)){
      mtext(1, text = x.label, line = x.line, cex = 1)
      mtext(2, text = y.label, line = y.line, cex = rel.y.text)
    }
  
    if (full.trend == T){
    #Split data into past decade and full time series
    dat <- as.data.frame(x[, list(X, Value)])
    
    dat <- dat %>% dplyr::rename(series = Value) %>%
      mutate(time = seq(1,nrow(dat),1))
    
    # Fit linear model
    lm_out <- fit_lm(dat = dat)
    p <- lm_out$p
    if (p < .05){
        
      newtime <- seq(min(dat$time), max(dat$time), length.out=length(dat$time))
      newdata <- data.frame(time = newtime,
                      time2 = newtime^2)
      lm_pred <- AICcmodavg::predictSE(lm_out$model, 
                                 newdata = newdata,
                                 se.fit = TRUE)

      year <- seq(x$X[1],x$X[length(x$X)],length.out = length(dat$time))

      # Make plot
      if (lm_pred$fit[length(lm_pred$fit)] > lm_pred$fit[1]){
        lines(year, lm_pred$fit, col = main.pos, lwd = 7)
        points(x[, list(X, Value)], pch = 16, cex = point.cex)
        lines( x[, list(X, Value)], lwd = lwd)

        if (line.forward == TRUE){
           lines(year, lm_pred$fit, col = main.pos, lwd = 7)
        }
      } else if (lm_pred$fit[length(lm_pred$fit)] < lm_pred$fit[1]){
        lines(year, lm_pred$fit, col = main.neg, lwd = 7)
        points(x[, list(X, Value)], pch = 16, cex = point.cex)
        lines( x[, list(X, Value)], lwd = lwd)
        if (line.forward == TRUE){
           lines(year, lm_pred$fit, col = main.neg, lwd = 7)
        }
      }
    }
    
    if (extra == TRUE){
      
      # Second variable
      dat <- as.data.frame(x2[, list(X2, Value)])
    
      dat <- dat %>% dplyr::rename(series = Value) %>%
      mutate(time = seq(1,nrow(dat),1))
    
     # Fit linear model
      lm_out <- fit_lm(dat = dat)
      p <- lm_out$p
      points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = "indianred")
      lines( x2[, list(X2, Value)], lwd = lwd, col = "indianred")
      if (p < .05){
    
        newtime <- seq(min(dat$time), max(dat$time), length.out=length(dat$time))
        newdata <- data.frame(time = newtime,
                      time2 = newtime^2)
        lm_pred <- AICcmodavg::predictSE(lm_out$model, 
                                 newdata = newdata,
                                 se.fit = TRUE)

        year <- seq(x2$X2[1],x2$X2[length(x2$X2)],length.out =length(dat$time))
   
    # Make plot
        if (lm_pred$fit[length(lm_pred$fit)] > lm_pred$fit[1] ){
          lines(year, lm_pred$fit, col = main.pos, lwd = 7)
          points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = "indianred")
          lines( x2[, list(X2, Value)], lwd = lwd, col = "indianred")
        } else if (lm_pred$fit[length(lm_pred$fit)] < lm_pred$fit[1]){
          lines(year, lm_pred$fit, col = main.neg, lwd = 7)
          points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = "indianred")
          lines( x2[, list(X2, Value)], lwd = lwd, col = "indianred")
        } 
     }
    }

  }


 
}  


#Add axis labels for stacked plots
soe.stacked.axis <- function(x.label, y.label, x.line = 2.5,rel.x.text = 1.5,
                             y.line = 3.5, rel.y.text = 1.5, outer = TRUE){
  axis(1, cex.axis = rel.x.text)
  mtext(1, text = x.label, line = x.line, cex = rel.x.text, outer = outer)
  mtext(2, text = y.label, line = y.line, cex = rel.y.text, outer = outer)
  
}


#Background colors
background   <- 'white'
recent       <- '#E6E6E6'
main.pos <- rgb(253/255, 184/255, 99/255,  alpha = .9)
main.neg <- rgb(178/255, 171/255, 210/255, alpha = .9)

```

**Example plot**
```{r examplefig, fig.cap="Example plot formatted for the report", echo = T, eval = T, warning=F,message=F}
data.dir <- "data"
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

opar <- par(mar = c(4, 6, 2, 6))

soe.plot(SOE.data, "Time", "Mid-Atlantic Rec catch", scale.axis = 10^6,
         end.start = 2008, x.label = 'Year', rel.y.text = 1.5, rel.y.num = 1.1,
         y.line = 2.5, y.label = expression("Fish caught, 10"^6 *" n"))
```



<!--chapter:end:chapters/Trend_analysis.Rmd-->

# Ecological Production Units {#epu}

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)
library(magick)
library(readr)

```
**Description**: Ecological Production Units

**Indicator category**: Extensive analysis, not yet published

<!-- 1. Database pull -->
<!-- 2. Database pull with analysis -->
<!-- 3. Synthesis of published information -->
<!-- 4. Extensive analysis, not yet published -->
<!-- 5. Published methods -->
**Contributor(s)**: Robert Gamble

**Data steward**: NA

**Point of contact**: Robert Gamble, <robert.gamble@noaa.gov>

**Public availability statement**: Ecological production unit (EPU) shapefiles are available [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis). More information about source data used to derive EPUs can be found [here](https://www.integratedecosystemassessment.noaa.gov/sites/default/files/pdf/ne-ecological-production-units-paper.pdf).


## Methods
To define ecological production units, we assembled a set of physiographic, oceanographic and biotic variables on the Northeast U.S. Continental Shelf, an area of approximately 264,000 km within the 200 m isobath. The physiographic and hydrographic variables selected have been extensively used in previous analyses of oceanic provinces and regions [e.g @Roff2000]. Primary production estimates have also been widely employed for this purpose in conjunction with physical variables [@Longhurst2007] to define ecological provinces throughout the world ocean. 

We did not include information on higher trophic levels or fishing patterns in our. The biomass and production of higher trophic levels in this region has been sharply perturbed by fishing and other anthropogenic influences. Similarly, fishing patterns are affected by regulatory change, market and economic factors and other external influences. 

Because these malleable patterns of change are often unconnected with underlying productivity, we excluded factors directly related to fishing practices. The physiographic variables considered in this analysis are listed in Table \@ref(tab:epuinputs). They include bathymetry and surficial sediments. The physical oceanographic and hydrographic measurements include sea surface temperature, annual temperature span, and temperature gradient water derived from satellite observations for the period 1998 to 2007. 

### Data sources
Shipboard observations for surface and bottom water temperature and salinity in surveys conducted in spring and fall. Daily sea surface temperature (SST, &deg;C) measurements at 4 km resolution were derived from nighttime scenes composited from the AVHRR sensor on NOAA's polar-orbiting satellites and from NASA's MODIS TERRA and MODIS AQUA sensors. We extracted information for the annual mean SST, temperature span, and temperature gradients from these sources. The latter metric provides information on frontal zone locations. 


```{r epuinputs,  echo = F, include = T, warning = F, message = F, results='asis'}
#Table: (\#label) Variables used in derivation of Ecological Production Units
tab <- '
|Variables|Sampling Method|Units|
|:-----------------------|:-----------------------|:-----------------------|
|Bathymetry|Soundings/Hydroacoustics|Meters|
|Surficial Sediments|Benthic Grab|Krumbian Scale|
|Sea Surface Temperature|Satellite Imagery (4km grid)|&deg;C annual average|
|Sea Surface Temperature|Satellite Imagery (4km grid)|dimensionless|
|Sea Surface Temperature|Satellite Imagery (4km grid)|&deg;C annual average|
|Surface Temperature|Shipboard hydrography (point)|&deg;C (Spring and Fall)|
|Bottom Temperature|Shipboard hydrography (point)|&deg;C (Spring and Fall)|
|Surface Salinity|Shipboard hydrography (point)|psu (Spring and Fall)|
|Bottom Salinity|Shipboard hydrography (point)|psu (Spring and Fall)|
|Stratification|Shipboard hydrography (point)|Sigma-t units (Spring and Fall)|
|Chlorophyll-a|Satellite Imagery (1.25 km grid)|mg/C/m^3^ (annual average)|
|Chlorophyll-a gradient|Satellite Imagery (1.25 km grid)|dimensionless|
|Chlorophyll-a span|Satellite Imagery (1.25 km grid)|mg/C/m^3^ (annual average)|
|Primary Production|Satellite Imagery (1.25 km grid)|gC/m^3^/year (cumulative)|
|Primary Production gradient|Satellite Imagery (1.25 km grid)|dimensionless|
|Primary Production span|Satellite Imagery (1.25 km grid)|gC/m^3^/year (cumulative)|
'
#cat(tab)
df<-read_delim(tab, delim="|")
df<-df[-c(1,2) ,c("Variables","Sampling Method","Units")]
knitr::kable(
  df, booktabs = TRUE,
  caption = 'Variables used in derivation of Ecological Production Units.'
)
```


The biotic measurements included satellite-derived estimates of chlorophyll *a* (CHLa) mean concentration, annual span, and CHLa gradients and related measures of primary production. Daily merged SeaWiFS/MODIS-Aqua CHLa (CHL, mg m^-3^) and SeaiWiFS photosynthetically available radiation (PAR, Einsteins m^-2^ d^-1^) scenes at 1.25 km resolution were obtained from NASA Ocean Biology Processing Group. 

### Data extraction
NA

### Data analysis
In all cases, we standardized the data to common spatial units by taking annual means of each observation type within spatial units of 10' latitude by 10' longitude to account for the disparate spatial and temporal scales at which these observations are taken. There are over 1000 spatial cells in this analysis. Shipboard sampling used to obtain direct hydrographic measurements is constrained by a minimum sampling depth of 27 m specified on the basis of prescribed safe operating procedures. As a result nearshore waters are not fully represented in our initial specifications of ecological production units. 

The size of the spatial units employed further reflects a compromise between retaining spatial detail and minimizing the need for spatial interpolation of some data sets. For shipboard data sets characterized by relatively coarse spatial resolution, where necessary, we first constructed an interpolated map using an inverse distance weighting function before including it in the analysis. Although alternative interpolation schemes based on geostatistical approaches are possible, we considered the inverse distance weighting function to be both tractable and robust for this application. 

We first employed a spatial principal components analysis [PCA; e.g. @Pielou1984; @Legendre1998] to examine the multivariate structure of the data and to account for any inter-correlations among the variables to be used in subsequent analysis. The variables included in the analysis exhibited generally skewed distributions and we therefore transformed each to natural logarithms prior to analysis. 

The PCA was performed on the correlation matrix of the transformed observations. We selected the eigenvectors associated with eigenvalues of the dispersion matrix with scores greater than 1.0 [the Kaiser-Guttman criterion; @Legendre1998] for all subsequent analysis. These eigenvectors represent orthogonal linear combinations of the original variables used in the analysis. 

We delineated ecological subunits by applying a disjoint cluster based on Euclidean distances using the K-means procedure [@Legendre1998] on the principal component scores The use of non-independent variables can strongly influence the results of classification analyses of this type [@Pielou1984], hence the interest in using the PCA results in the cluster. 

The eigenvectors were represented as standard normal deviates. We used a Pseudo-F Statistic described by @Milligan1985 to objectively define the number of clusters to use in the analysis. The general approach employed is similar to that of @Host1996 for the development of regional ecosystem classifications for terrestrial systems.

After the analyses were done, we next considered options for interpolation of nearshore boundaries resulting from depth-related constraints on shipboard observations. For this, we relied on information from satellite imagery. For the missing nearshore areas in the Gulf of Maine and Mid-Atlantic Bight, the satellite information for chlorophyll concentration and sea surface temperature indicated a direct extension from adjacent observations. For the Nantucket Shoals region south of Cape Cod, similarities in tidal mixing patterns reflected in chlorophyll and temperature observations indicated an affinity with Georges Bank and the boundaries were changed accordingly.

Finally, we next considered consolidation of ecological subareas so that nearshore regions are considered to be special zones nested within the adjacent shelf regions. Similar consideration led to nesting the continental slope regions within adjacent shelf regions in the Mid-Atlantic and Georges Bank regions. This led to four major units: Mid-Atlantic Bight, Georges Bank, Western-Central Gulf of Maine (simply "Gulf of Maine" in the SOE), and Scotian Shelf-Eastern Gulf of Maine. As the State of the Ecosystem reports are specific to FMC managed regions, the Scotian Shelf-Eastern Gulf of Maine EPU is not considered in SOE indicator analyses. 


```{r EPUmap, fig.cap="Map of the four Ecological Production Units, including the Mid-Atlantic Bight (light blue), Georges Bank (red), Western-Central Gulf of Maine (or Gulf of Maine; green), and Scotian Shelf-Eastern Gulf of Maine (dark blue)", fig.align='center', echo = F}

image.dir <- './images'

knitr::include_graphics(file.path(image.dir,"EPUs.jpg"))

```


<!--chapter:end:chapters/EPU.Rmd-->

# Single Species Status Indicator {#stockstatus}

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```


**Description**: Summary of the most recent stock assessment results for each assessed species.

**Indicator category**: Synthesis of published information

**Contributor(s)**: Sarah Gaichas, based on code and spreadsheets originally provided by Chris Legault

**Data steward**: Sarah Gaichas <sarah.gaichas@noaa.gov>

**Point of contact**: Sarah Gaichas <sarah.gaichas@noaa.gov>

**Public availability statement**: All stock assessment results are publicly available (see Data Sources). Summarized data are available [here](http://comet.nefsc.noaa.gov/erddap/tabledap/assess_soe_v1.htmlTable?No,Entity_Name,Science_Center,Assessment_Year,Last_Data_Year,Assessment_Level,Citation,Comments,Best_F,F_Year,Flimit,Fmsy,F_Flimit,F_Fmsy,Best_B,B_Year,B_Blimit,B_Bmsy,Stock_Level_Relative_to_Bmsy,Bmsy,Blim).

## Methods

### Data sources
<!--Please provide a text description of data sources, inlcuding primary collection methods. What equipment was used to turn signal to data? From which vessel were data collected and how? What quality control procedures were employed, if any?--> 

"Data" used for this indicator are the outputs of stock assessment models and review processes, including reference points (proxies for fishing mortality limits and stock biomass targets and limits), and the current fishing mortality rate and biomass of each stock. The spreadsheet summarizes the most recent stock assessment updates for each species, which are available on the Northeast Fisheries Science Center (NEFSC) website at:
https://www.nefsc.noaa.gov/saw/reports.html  
https://www.nefsc.noaa.gov/publications/crd/crd1717/

Additional assessments are reported directly to the New England Fishery Management Council (NEFMC):
http://s3.amazonaws.com/nefmc.org/Document-2-SAFE-Report-for-Fishing-Year-2016.pdf  
http://s3.amazonaws.com/nefmc.org/4_NEFSC_SkateMemo_July_2017.pdf



### Data extraction
<!--Text overview description of extraction methods. What information was extracted and how was it aggregated? Can point to other indicator extraction methods if the same.-->

Each assessment document was searched to find the following information (often but not always summarized under a term of reference to determine stock status in the executive summary):

*    **Bcur**: current year biomass, (most often spawning stock biomass (SSB) or whatever units the reference points are in)

*    **Fcur**: current year fishing mortality, F

*    **Bref**: biomass reference point, a proxy of Bmsy (the target)

*    **Fref**: fishing mortality reference point, a proxy of Fmsy

<!--Write SQL query here
```{sql, eval = F, echo = T}
SELECT * FROM...
```
--> 

### Data analysis
<!--Text description of analysis methods, similar in structure and detail to a peer-reviewed paper methods section-->

For each assessed species, Bcur is divided by Bref and Fcur is divided by Fref. They are then plotted for each species on an x-y plot, with Bcur/Bref on the x axis, and Fcur/Fref on the y axis. 

<!--What packages or libraries did you use in your work flow?-->
```{r, eval=F, echo = F}
#all packages in the SOE file that ran my code snippet, probably only need data.table
library(Kendall);library(data.table);library(zoo)
library(dplyr);library(nlme);library(AICcmodavg)
library(colorRamps);library(Hmisc);library(rgdal)
library(maps);library(mapdata);library(raster)
library(grid);library(stringr);library(png)
library(ncdf4);library(marmap); library(magick)


```

<!--Include accompanying R code, pseudocode, flow of scripts, and/or link to location of code used in analyses.-->

### Plotting

The script used to develop the figure in the SOE is below, with an example figure. Different lines are commented out of the script to produce the Mid- Atlantic or New England figures. The positioning table used to make the plot (the data frame named "decoder" below) is available [here](http://comet.nefsc.noaa.gov/erddap/tabledap/assess_support_soe_v1.htmlTable?Entity_Name,Count_of_No,Min_of_No2,Code,Council,my_pos,my_pos2,my_pos3).

```{r, echo = T, eval = T, fig.cap="Summary of single species status for NEFMC stocks \\label{KOBE}", fig.height=8, fig.width=11, fig.align='center', fig.show='hold', message=F, warning=F}

# Relative working directories
data.dir  <- here::here('data')
image.dir <- here::here('images')
r.dir <- here::here('R')

#Load current SOE ERDDAP data set IDs
load(file.path(data.dir, "ERDDAP_datasets.Rdata"))

#Source get_erddap function to query data
source(file.path(r.dir,"get_erddap.R"))

#Data queries
dat <- get_erddap(df = erddap_datasets,
                  id = "assess_soe_v1")

decoder <- get_erddap(erddap_datasets,
                      id = "assess_support_soe_v1")

#Plotting code
stocks <- unique(dat$Entity_Name)
n.stocks <- length(stocks)

# set up the headers for most.recent structure - will contain oldest Assessment.Year
most.recent <- dat[1,]
most.recent <- most.recent[-1,]

for (i in 1:n.stocks){
  temp <- dat[dat$Entity_Name == decoder$Entity_Name[i],]
  most.recent <- rbind(most.recent,temp[temp$Assessment_Year == max(temp$Assessment_Year),])
}
#cbind(n.stocks,length(most.recent[,1]))

# get the max of F.Flimit and F.Fmsy and the max of B.Blimit and B.Bmsy
Frat <- most.recent$F_Fmsy
Brat <- most.recent$B_Bmsy
#Frat[6] <- most.recent$F.Flimit[6]  # Ocean Quahog use Flimit because no Fmsy
#Frat[18] <- 0.88  # hardwire average of the two GOM models for GOM cod
#Brat[18] <- 0.155 # hardwire average of the two GOM models for GOM cod
#model averages have been calculated and included in F/Fmsy column in 2017assess.csv
#cbind(most.recent$Entity.Name,most.recent$F.Flimit,most.recent$F.Fmsy)

# figure out appropriate range for axes WARNING HARD CODED
#max(Frat,na.rm=T)
#max(Brat,na.rm=T)
x.r <- c(0,7)
y.r <- c(0,3.5)

# set some colors
my.col <- rep(NA,n.stocks)
MA.col <- "blue"
NE.col <- "blue"
BO.col <- "purple"
for (i in 1:n.stocks){
  if(decoder$Council[i] == "MAFMC") my.col[i] <- MA.col
  if(decoder$Council[i] == "NEFMC") my.col[i] <- NE.col
  if(decoder$Council[i] == "Both")  my.col[i] <- BO.col
}

# create new matrix for plotting
xx <- as.data.frame(cbind(decoder[,2:8],Frat,Brat,my.col))
newlist <- as.factor(decoder$Code)
others <- as.factor("7 Skates")
sppcodes <- unlist(list(newlist, others))
MA.unknown <- sppcodes[c(1,3,4,47,48)] #mackerel, Loligo, Illex, monkfish unknown
#MA.unknown <- decoder$Code[c(1,3,4,8)] #BSB known
#NE.unknown <- c(decoder$Code[c(14,23,33)],"7 Skates")
NE.overfished <- sppcodes[c(17,19,35)] # GB cod, halibut, witch, thorny skate
NE.unknown <- sppcodes[c(14,23,33,47,48)] #Red deepsea crab,  Offshore hake, GOM winter flounder, GB YT,2 monkfish, 7 skates unknown
my.col[50] <- NE.col


# break out the Councils
MA <- xx[xx$Council == "MAFMC",]
NE <- xx[xx$Council == "NEFMC",]
BO <- xx[xx$Council == "Both",]
MABO<-rbind(MA, BO)
NEBO<-rbind(NE, BO)

#if(saveplots) png(file = "MAFMC_joint_stocks_2017.png",  units="in", width = 6, height = 6, res=1200)
# plot(MABO$Brat,MABO$Frat,xlim=x.r,ylim=y.r,xlab="B/Bmsy",ylab="F/Fmsy",pch=16,col=my.col[c(1:11,47:49)])
#   abline(v=0.5,lty=2)
#   abline(v=1,lty=3)
#   abline(h=1,lty=2)
#   title(expression("MAFMC " * phantom("and Joint Stocks")), col.main="blue")
#   title(expression(phantom("MAFMC ") * "and " * phantom("Joint Stocks")), col.main="black")
#   title(expression(phantom("MAFMC and ") * "Joint" * phantom(" Stocks")), col.main="purple")
#   title(expression(phantom("MAFMC and Joint") * " Stocks"), col.main="black")  
#   legend('topright',legend=MA.unknown,pch=NA,text.col=my.col[c(1,3,4,47,48)], title="Unknown Status", title.col = "black")
#   text(MABO$Brat,MABO$Frat,labels=MABO$Code,pos=MABO$my.pos3,col=my.col[c(1:11,47:49)],cex=0.8)
#if(saveplots) savePlot("MAFMC_Joint_stocks.png", type='png')
#if(saveplots) dev.off()

#if(saveplots) pdf(file = "NEFMC_joint_stocks.pdf",  width = 6, height = 6)
#if(saveplots) png(file = "NEFMC_joint_stocks.png",  units="in", width = 6, height = 9, res=1200)
plot(NEBO$Brat,NEBO$Frat,xlim=x.r,ylim=y.r,xlab="B/Bmsy",ylab="F/Fmsy",pch=16,col=my.col[12:49])
   abline(v=0.5,lty=2)
   abline(v=1,lty=3)
   abline(h=1,lty=2)
   title(expression("NEFMC " * phantom("and Joint Stocks")), col.main="blue")
   title(expression(phantom("NEFMC ") * "and " * phantom("Joint Stocks")), col.main="black")
   title(expression(phantom("NEFMC and ") * "Joint" * phantom(" Stocks")), col.main="purple")
   title(expression(phantom("NEFMC and Joint") * " Stocks"), col.main="black")
   legend('topright',legend=NE.overfished,pch=NA,text.col=my.col[c(17,19,35)], title="Overfished, F \nStatus Unknown",title.col = "black", bty="n", text.width=strwidth("Offshore Hake"), inset=c(0,.05))
   legend('right',legend=NE.unknown,pch=NA,text.col=my.col[c(14,23,33,47,48)],title="Unknown Status", title.col = "black",bty="n")
   text(NEBO$Brat,NEBO$Frat,labels=NEBO$Code,pos=NEBO$my_pos3,col=my.col[12:49],cex=0.8)
#if(saveplots) savePlot("NEFMC_Joint_stocks.png", type='png')
#if(saveplots) dev.off()

```


<!--chapter:end:chapters/singlespp_status_indicator.Rmd-->

# Aggregate Groups {#aggroups}

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```


**Description**: Mappings of species into aggregate group categories for different analyses

**Indicator category**: Synthesis of published information 

**Contributor(s)**: Geret DePiper, Sarah Gaichas, Sean Hardison, Sean Lucey
  
**Data steward**: Sean Lucey <Sean.Lucey@noaa.gov>
  
**Point of contact**: Sean Lucey <Sean.Lucey@noaa.gov>
  
**Public availability statement**: Source data is available to the public (see Data Sources). 


## Methods
The State of the Ecosystem (SOE) reports are delivered to the New England Fishery Management Council (NEFMC) and Mid-Atlantic Fishery Management Council (MAFMC) to provide ecosystems context.  To better understand that broader ecosystem context, many of the indicators are reported at an aggregate level rather than at a single species level.  Species were assigned to an aggregate group following the classification scheme of @garrison2000dietary and @link2006EMAX.  Both works classified species into feeding guilds based on food habits data collected at the Northeast Fisheries Science Center (NEFSC).  In 2017, the SOE used seven specific feeding guilds (plus an "other" category; Table \@ref(tab:soe2017class)).  These seven were the same guilds used in @garrison2000dietary, which also distinguished ontogentic shifts in species diets.  

For the purposes of the SOE, species were only assigned to one category based on the most prevalent size available to commercial fisheries.  However, several of those categories were confusing to the management councils, so in 2018 those categories were simplified to five (plus "other"; Table \@ref(tab:soe2018class)) along the lines of @link2006EMAX.  In addition to feeding guilds, species managed by the councils have been identified.  This is done to show the breadth of what a given council is responsible for within the broader ecosystem context.

```{r soe2017class, eval = T, echo = F}
soe.17.class <- data.frame('Feeding Guild' = c('Apex Predator', 'Piscivore', 
                                               'Macrozoo-piscivore', 'Macroplanktivore', 
                                               'Mesoplanktivore', 'Benthivore',
                                               'Benthos', 'Other'),
                           Description = c('Top of the food chain', 'Fish eaters', 
                                           'Shrimp and small fish eaters', 'Amphipod and shrimp eaters',
                                           'Zooplankton eaters', 'Bottom eaters',
                                           'Things that live on the bottom', 
                                           'Things not classified above'))

kable(soe.17.class, booktabs = TRUE,
      caption = "Aggregate groups use in 2017 SOE.  Classifications are based on Garrison and Link 2000. \\label{}")
```

```{r soe2018class, eval = T, echo = F}
soe.18.class <- data.frame('Feeding Guild' = c('Apex Predator', 'Piscivore', 
                                               'Planktivore', 'Benthivore',
                                               'Benthos', 'Other'),
                           Description = c('Top of the food chain', 'Fish eaters', 
                                           'Zooplankton eaters', 'Bottom eaters',
                                           'Things that live on the bottom', 
                                           'Things not classified above'))

kable(soe.18.class, booktabs = TRUE,
      caption = "Aggregate groups use in 2018 SOE.  Classifications are based on Link et al. 2006.")
```

### Data sources

In order to match aggregate groups with various data sources, a look-up table was generated which includes species' common names (COMNAME) along with their scientific names (SCINAME) and several species codes. SVSPP codes are used by the NEFSC Ecosystems Surveys Branch in their fishery-independent Survey Database (SVDBS), while NESPP3 codes refer to the codes used by the Commercial Fisheries Database System (CFDBS) for fishery-dependent data. A third species code provided is the ITISSPP, which refers to species identifiers used by the Integrated Taxonomic Information System (ITIS). Digits within ITIS codes are hierarchical, with different positions in the identifier referring to higher or lower taxonomic levels. More information about the SVDBS, CFDBS, and ITIS species codes are available in the links provided below.

Management responsibilities for different species are listed under the column "Fed.managed" (NEFMC, MAFMC, or JOINT for jointly managed species). More information about these species is available on the FMC websites listed below. Species groupings listed in the "NEIEA" column were developed for presentation on the Northeast Integrated Ecosystem Assessment (NE-IEA) website. These groupings are based on EMAX groupings [@link2006EMAX], but were adjusted based on conceptual models developed for the NE-IEA program that highlight focal components in the NE-LME (i.e. those components with the largest potential for perturbing ecosystem dynamics). NE-IEA groupings were further simplified to allow for effective communication through the NE-IEA website.

#### Supplemental information

See the following links for more information regarding the NEFSC ESB Bottom Trawl Survey, CFDBS, and ITIS:  

*    https://www.itis.gov/  
*    https://inport.nmfs.noaa.gov/inport/item/22561  
*    https://inport.nmfs.noaa.gov/inport/item/22560  
*    https://inport.nmfs.noaa.gov/inport/item/27401  	

More information about the NE-IEA program is available [here](http://integratedecosystemassessment.noaa.gov).

More information about the New Engalnd Fisheries Management Council is available [here](https://www.nefmc.org/).

More information about the Mid-Atlantic Fisheries Management Council is available [here](http://www.mafmc.org/).

### Data extraction 

Species lists are pulled from SVDBS and CFDBS.  They are merged using the ITIS code.  Classifications from Garrison and Link [@garrison2000dietary] and Link et al. [@link2006EMAX] are added manually. The R code used in the extraction process is presented below.


```{r, echo = T, eval = F}
#Species list
if(Sys.info()['sysname']=="Windows"){
  data.dir <- "L:\\EcoAP\\Data\\survey"
  out.dir  <- "L:\\EcoAP\\Data\\survey"
  memory.limit(4000)
}
if(Sys.info()['sysname']=="Linux"){
  data.dir  <- "/home/slucey/slucey/EcoAP/Data/survey"
  out.dir   <- "/home/slucey/slucey/EcoAP/Data/survey"
  out.dir.2 <- "/home/slucey/slucey/EcoAP/Data/Commercial"
  uid       <- 'slucey'
  cat("Oracle Password: ")
  pwd <- scan(stdin(), character(), n = 1)
}

library(RODBC); library(data.table)

if(Sys.info()['sysname']=="Windows"){
  channel <- odbcDriverConnect()
} else {
  channel <- odbcConnect('sole', uid, pwd)
}

#Grab svspp code by itis code
svspp <- as.data.table(sqlQuery(channel, "select SVSPP, ITISSPP, COMNAME, SCINAME 
                                from ITIS_Lookup"))

#Grab cfspp by itis code
cfspp <- as.data.table(sqlQuery(channel, "select NESPP4, SPECIES_ITIS, 
                                COMMON_NAME, SCIENTIFIC_NAME 
                                from CFDBS.Species_itis_ne"))
setnames(cfspp, 'SPECIES_ITIS', 'ITISSPP')
cfspp[, NESPP3 := as.numeric(substr(sprintf('%04d', NESPP4), 1, 3))]
setkey(cfspp, NESPP3)
cfspp <- unique(cfspp)
cfspp[, NESPP4 := NULL]

setkey(cfspp, ITISSPP, NESPP3)
cfspp <- unique(cfspp, by = key(cfspp))

#Merge to master species list
spp <- merge(svspp, cfspp, by = 'ITISSPP', all = T)
spp <- spp[!(is.na(SVSPP) & is.na(NESPP3)), ]

#Fix known issues
spp <- spp[!SVSPP %in% c(193, 310), ]

spp[ITISSPP == 630979, SVSPP  := 193] #Ocean Pout
spp[ITISSPP == 620992, SVSPP  := 310] #Deepsea red crab
spp[ITISSPP == 172783, SVSPP  := 104] #Fourspot flounder
spp[ITISSPP == 166283, SVSPP  := 112] #John Dory
spp[ITISSPP == 161731, SVSPP  :=  36] #Menhaden
spp[ITISSPP ==  98671, SVSPP  := 311] #Cancer Crabs unk
spp[ITISSPP ==  98455, SVSPP  := 317] #Spider crabs
spp[ITISSPP == 159772, NESPP3 := 150] #Hagfish
spp[ITISSPP == 166284, NESPP3 := 188] #John Dory
spp[ITISSPP == 98670,  NESPP3 := 714] #Cancer Crabs unk

spp[ITISSPP %in% c(630979, 620992), COMNAME := COMMON_NAME]
spp[ITISSPP %in% c(630979, 620992), SCINAME := SCIENTIFIC_NAME]

#Drop extra columns
spp[is.na(COMNAME), COMNAME := COMMON_NAME]
spp[is.na(SCINAME), SCINAME := SCIENTIFIC_NAME]
spp[, c('COMMON_NAME', 'SCIENTIFIC_NAME') := NULL]
setcolorder(spp, c('ITISSPP', 'SVSPP', 'NESPP3', 'COMNAME', 'SCINAME'))
setkey(spp, SVSPP, NESPP3)

#-------------------------------------------------------------------------------
#Add management authority
mafmc <- c(103, 121, 131, 135, 141, 143, 151, 403, 409, 502, 503, 621)
nefmc <- c(22:28, 32, 69, 72:77, 101, 102, 105:107, 155, 193, 310, 401, 894)
joint <- c(15, 197)
spp[SVSPP %in% mafmc, MANAGE := 'MAFMC']
spp[SVSPP %in% nefmc, MANAGE := 'NEFMC']
spp[SVSPP %in% joint, MANAGE := 'JOINT']

#-------------------------------------------------------------------------------
#Add functional groups
#From NEFMC EBFM PDT work
#See Functional_group_table_Mike.csv in slucey/EcoAP/EBFM_PDT
spp[, EBFM.PDT := factor(NA, levels = c('Apex Predator', 'Benthivore', 'Benthos',
                                        'Macroplanktivore', 'Macrozoo-piscivore',
                                        'Mesoplanktivore', 'Piscivore', 'Other'))]
spp[SVSPP %in% c(11, 700, 704, 747), EBFM.PDT := 'Apex Predator']
spp[SVSPP %in% c(14, 22, 25, 74, 102, 105, 106, 107, 141, 143, 151, 164, 172,
                   176, 177, 192, 193, 301, 310, 312, 314, 317, 322, 501),
    EBFM.PDT := 'Benthivore']
spp[SVSPP %in% c(331, 336, 401, 403, 409), EBFM.PDT := 'Benthos']
spp[NESPP3 %in% c(775, 781, 805, 806), EBFM.PDT := 'Benthos']  
spp[SVSPP %in% c(76, 163, 168, 171, 502, 503), EBFM.PDT := 'Macroplanktivore']
spp[SVSPP %in% c(13, 24, 26, 27, 75, 77, 84, 108, 112, 155, 156, 311),
    EBFM.PDT := 'Macrozoo-piscivore']
spp[SVSPP %in% c(32, 33, 34, 35, 36, 121, 131), EBFM.PDT := 'Mesoplanktivore']
spp[SVSPP %in% c(15, 23, 28, 69, 72, 73, 101, 103, 104, 135, 139, 145, 197),
    EBFM.PDT := 'Piscivore']
spp[is.na(EBFM.PDT), EBFM.PDT := 'Other']

#Fix known issues
spp[SVSPP == 160, EBFM.PDT := 'Macroplanktivore']

#Add EMAX
load(file.path(data.dir, 'EMAX_groups.RData'))
emax <- emax[!is.na(SVSPP), ]
spp <- merge(spp, emax[, list(SVSPP, EMAX, Fall.q, Spring.q)], by = 'SVSPP', all.x = T)

#reduce rows
setkey(spp, SVSPP, NESPP3)
species <- unique(spp, by = key(spp))

#Remove EMAX q's
species[, c('Fall.q', 'Spring.q') := NULL]

#Expand EMAX abbreviations
species[EMAX == 'BG', EMAX := 'Benthivore Gadiformes']
species[EMAX == 'BE', EMAX := 'Benthivore Elasmobranchs']
species[EMAX == 'BP', EMAX := 'Benthivore Pleuronectiformes']
species[EMAX == 'BS', EMAX := 'Benthivore Scorpaeniformes']
species[EMAX == 'BO', EMAX := 'Benthivore Others']
species[EMAX == 'BC', EMAX := 'Benthivore Perciformes']
species[EMAX == 'PG', EMAX := 'Piscivore Gadiformes']
species[EMAX == 'PE', EMAX := 'Piscivore Elasmobranchs']
species[EMAX == 'PO', EMAX := 'Piscivore Others']
species[EMAX == 'OE', EMAX := 'Omnivore Elasmobranchs']
species[EMAX == 'OO', EMAX := 'Omnivore Others']
species[EMAX == 'SC', EMAX := 'Southern Perciformes']
species[EMAX == 'SP', EMAX := 'Southern Pleuronectiformes']
species[EMAX == 'SE', EMAX := 'Southern Elasmobranchs']
species[EMAX == 'SG', EMAX := 'Southern Gadiformes']
species[EMAX == 'SO', EMAX := 'Southern Others']

#Rename columns
setnames(species, c('MANAGE', 'EBFM.PDT', 'Feeding.guild'), c('Fed.Managed', 'SOE.17', 'SOE.18'))

#Add classification from Garrison/Link 2000
size <- data.table(COMNAME = c(rep('SMOOTH DOGFISH',           2), 
                               rep('SPINY DOGFISH',            3),
                               rep('WINTER SKATE',             4),
                               rep('LITTLE SKATE',             2),
                               rep('SMOOTH SKATE',             1),
                               rep('THORNY SKATE',             4),
                               rep('ATLANTIC HERRING',         2),
                               rep('ALEWIFE',                  1),
                               rep('SILVER HAKE',              3),
                               rep('ATLANTIC COD',             4),
                               rep('HADDOCK',                  3),
                               rep('POLLOCK',                  4),
                               rep('WHITE HAKE',               3),
                               rep('RED HAKE',                 3),
                               rep('SPOTTED HAKE',             2),
                               rep('AMERICAN PLAICE',          3), 
                               rep('SUMMER FLOUNDER',          2),
                               rep('FOURSPOT FLOUNDER',        2),
                               rep('YELLOWTAIL FLOUNDER',      3),
                               rep('WINTER FLOUNDER',          3),
                               rep('WITCH FLOUNDER',           2), 
                               rep('WINDOWPANE',               2),
                               rep('GULF STREAM FLOUNDER',     1), 
                               rep('ATLANTIC MACKEREL',        3),
                               rep('BUTTERFISH',               1),
                               rep('BLUEFISH',                 3),
                               rep('ATLANTIC CROAKER',         2),
                               rep('BLACK SEA BASS',           2), 
                               rep('SCUP',                     1),
                               rep('WEAKFISH',                 2),
                               rep('ACADIAN REDFISH',          1),
                               rep('LONGHORN SCULPIN',         2),
                               rep('SEA RAVEN',                2),
                               rep('NORTHERN SAND LANCE',      1),
                               rep('OCEAN POUT',               1),
                               rep('FAWN CUSK-EEL',            1),
                               rep('GOOSEFISH',                1),
                               rep('ATLANTIC SHARPNOSE SHARK', 1), 
                               rep('NORTHERN SHORTFIN SQUID',  1),
                               rep('LONGFIN SQUID',            1)),
                   SizeCat = c('M', 'L', 'S', 'M', 'L', 'S', 'M', 'L', 'XL', 'S',
                               'M', 'M', 'S', 'M', 'L', 'XL', 'S','M','M', 'S',
                               'M', 'L', 'S', 'M', 'L', 'XL', 'S', 'M', 'L', 'S',
                               'M', 'L', 'XL', 'S', 'M', 'L', 'S', 'M', 'L', 'S',
                               'M',  'S', 'M', 'L', 'M', 'L', 'S', 'M', 'S', 'M',
                               'L', 'S', 'M', 'L', 'M', 'L', 'S', 'M', 'S', 'S',
                               'M', 'L', 'S', 'S', 'M', 'L', 'S', 'M', 'S', 'M',
                               'M', 'S', 'M', 'M', 'S', 'M', 'S', 'M', 'M', 'L',
                               'L', 'L', 'L', 'L', 'L'),
                   Min.size = c(41, 61, 10, 41, 61, 10, 31, 61, 81, 10, 31, 31,
                                10, 31, 61, 81, 10, 21, 21, 10, 21, 41, 10, 21, 
                                51, 81, 10, 21, 51, 10, 21, 51, 81, 10, 21, 41, 10, 21, 41, 10, 21, 
                                10, 21, 41, 21, 41, 10, 21, 10, 21, 41, 10, 21, 
                                41, 21, 41, 10, 21, 10, 10, 21, 36, 10, 10, 31,
                                71, 10, 26, 10, 26, 26, 10, 26, 26, 10, 26, 10, 
                                26, 11, 61, 61, 61, 61, 31, 31),
                   Garrison.Link = as.character(c(1, 1, 2, 2, 6, 3, 3, 6, 6, 3, 3, 4, 5, 5, 6, 
                                     6, 2, 2, 2, 4, 4, 6, 3, 3, 6, 6, 5, 5, 5, 4, 
                                     4, 4, 4, 3, 4, 6, 3, 3, 4, 3, 6, 5, 5, 5, 6,
                                     6, 3, 6, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 2,
                                     2, 2, 2, 6, 6, 6, 5, 5, 1, 1, 5, 6, 6, 4, 3, 
                                     3, 6, 6, 2, 5, 3, 6, 6, 2, 2))
                                )

#Give Garrison.link guilds meaniful names
size[Garrison.Link == 1, Garrison.Link := 'Crab Eaters']
size[Garrison.Link == 2, Garrison.Link := 'Planktivores']
size[Garrison.Link == 3, Garrison.Link := 'Amphipod/Shrimp Eaters']
size[Garrison.Link == 4, Garrison.Link := 'Shrimp/Small Fish Eaters']
size[Garrison.Link == 5, Garrison.Link := 'Benthivores']
size[Garrison.Link == 6, Garrison.Link := 'Piscivores']

#Merge Garrison Link into species list
species <- merge(species, size, by = 'COMNAME', all = T)

#Set column order
setcolorder(species, c('ITISSPP', 'SVSPP', 'NESPP3', 'COMNAME', 'SCINAME', 
                       'SizeCat', 'Min.size', 'Fed.Managed', 'Garrison.Link',
                       'SOE.17', 'SOE.18', 'EMAX'))

#Add NE.IEA.web category
#get rerddap
devtools::install_github("ropensci/rerddap")
library(rerddap) 

#Set URL for COMET (server where NEFSC ERDDAP lives)
comet <- 'https://comet.nefsc.noaa.gov/erddap/'

#List datasets on the NEFSC ERDDAP
tab_list <- ed_datasets(url = comet)

#download a tabular dataset (input is dataset id; this works on vectors)
ed_spp <- as.data.table(sprintf("https://comet.nefsc.noaa.gov/erddap/tabledap/%s.csv", "species_groups_2018") %>% 
  
  purrr::map(function(x) {
    
    readr::read_csv(url(x))
    
  }))

neiea <- unique(ed_spp[!is.na(NEIEA), list(COMNAME, NEIEA)])

species <- merge(species, neiea, by = 'COMNAME', all = T)

save(species, file = file.path(out.dir, 'SOE_species_list.RData'))
```








<!--chapter:end:chapters/aggregate_groups.rmd-->

# Survey Data {#survdat}

```{r,  echo = F, message=F}

#devtools::install_github("slucey/Rsurvey/Survdat")

#Load packages
library(knitr)
library(rmarkdown)
library(Survdat)
```

**Description**: Survdat (Survey database)

**Indicator category**: Database pull

**Contributor(s)**: Sean Lucey

**Data steward**: Sean Lucey <sean.lucey@noaa.gov>

**Point of contact**: Sean Lucey <sean.lucey@noaa.gov>

**Public availability statement**: Source data are available to qualified researchers upon request (see "Access Information" [here](https://inport.nmfs.noaa.gov/inport/item/22560)). Derived data used in SOE reports are available [here](https://comet.nefsc.noaa.gov/erddap/tabledap/group_landings_soe_v1.html).


## Methods
The Northeast Fisheries Science Center (NEFSC) has been conducting standardized bottom trawl surveys
in the fall since 1963 and spring since 1968.  The surveys follow a stratified random design.  Fish
species and several invertebrate species are enumerated on a tow by tow basis [@Azarovitz1981].  
The data are housed in the NEFSC's survey database (SVDBS) maintained by the Ecosystem Survey Branch.  

Direct pulls from the database are not advisable as there have been several gear modifications and
vessel changes over the course of the time series [@Miller_2010].  Survdat was developed as a database 
query that applies the appropriate calibration factors for a seamless time series since the 1960s.
As such, it is the base for many of the other analyses conducted for the State of the Ecosystem
report that involve fisheries independent data.

The Survdat script can be broken down into two sections.  The first pulls the raw data from SVDBS.
While the script is able to pull data from more than just the spring and fall bottom trawl surveys,
for the purposes of the State of the Ecosystem reports only the spring and fall data are used.
Survdat identifies those research cruises associated with the seasonal bottom trawl surveys and pulls
the station and biological data.  Station data includes tow identification (cruise, station, 
and stratum), tow location and date, as well as several environmental variables (depth, surface/bottom salinity, 
and surface/bottom temperature).  Stations are filtered for representativness using a station, haul, gear
(SHG) code for tows prior to 2009 and a tow, operations, gear, and aquisition (TOGA) code from 2009
onward.  The codes that correspond to a representative tow (SHG <= 136 or TOGA <= 1324) are the same
used by assessment biologists at the NEFSC.  Biological data includes the total biomass and abundance
by species, as well as lengths and number at length.

The second section of the Survdat script applies the calibration factors.  There are four calibrartion
factors applied (Table \@ref(tab:calibration)).  Calibration factors are pulled directly from SVDBS.  Vessel conversions were made from 
either the NOAA Ship *Delaware II* or NOAA Ship *Henry Bigelow* to the NOAA Ship *Albatross IV* which was 
the primary vessel for most of the time series.  The Albatross was decommisioned in 2009 and the Bigelow is 
now the primary vessel for the bottom trawl survey.

```{r, eval = T, echo = F}
cal.factors <- data.frame(Name = c('Door Conversion', 'Net Conversion', 'Vessel Conversion I', 'Vessel Conversion II'),
                          Code = c('DCF', 'GCF', 'VCF', 'BCF'),
                          Applied = c('<1985', '1973 - 1981 (Spring)', 'Delaware II records', 'Henry Bigelow records'))
kable(cal.factors, booktabs = TRUE,
      caption = "Calibration factors for NEFSC trawl survey data")
```

The output from Survdat is an RData file that contains all the station and biological data, corrected
as noted above, from the NEFSC Spring Bottom Trawl Survey and NEFSC Fall Bottom Trawl Survey.  The RData
file is a data.table, a powerful wrapper for the base data.frame (https://cran.r-project.org/web/packages/data.table/data.table.pdf).
There are also a series of tools that have been developed in order to utilize the Survdat data set
(https://github.com/slucey/RSurvey).

### Data sources
Survdat is a database query of the NEFSC survey database (SVDBS).These data are available to qualified researchers upon request. More information on the data request process is available under the "Access Information" field [here](https://inport.nmfs.noaa.gov/inport/item/22560).

### Data extraction 
Extraction methods are described above. The following is the R code used in the data extraction process.

```{r survdat, echo = T, eval = F}
#Survdat.r
#This script will generate data from the NEFSC bottom trawl surveys
#SML

#-------------------------------------------------------------------------------
#User parameters
if(Sys.info()['sysname']=="Windows"){
  data.dir <- "L:\\Rworkspace\\RSurvey"
  out.dir  <- "L:\\EcoAP\\Data\\survey"
  memory.limit(4000)
}
if(Sys.info()['sysname']=="Linux"){
  data.dir <- "/home/slucey/slucey/Rworkspace/RSurvey"
  out.dir  <- "/home/slucey/slucey/EcoAP/Data/survey"
  uid      <- 'slucey'
  cat("Oracle Password: ")
  pwd <- scan(stdin(), character(), n = 1)
}

shg.check  <- 'y' # y = use only SHG <=136 or TOGA <= 1324 (>2008)
raw.check  <- 'n' # y = save data without conversions (survdat.raw), will still 
                  #     save data with conversions (survdat)
all.season <- 'n' # y = save data with purpose code 10 not just spring/fall 
                  #     (survdat.allseason), will not save survdat regular
use.SAD    <- 'y' # y = grab data from Survey Analysis Database (SAD) for 
                  #     assessed species
#-------------------------------------------------------------------------------
#Required packages
library(RODBC); library(data.table)

#-------------------------------------------------------------------------------
#Created functions
  #Convert output to text for RODBC query
sqltext <- function(x){
  out <- x[1]
  if(length(x) > 1){
    for(i in 2:length(x)){
      out <- paste(out, x[i], sep = "','")
    }
  }
  out <- paste("'", out, "'", sep = '')
  return(out)
}

#-------------------------------------------------------------------------------
#Begin script
if(Sys.info()['sysname']=="Windows"){
  channel <- odbcDriverConnect()
} else {
  channel <- odbcConnect('sole', uid, pwd)
}

#Generate cruise list
if(all.season == 'n'){
  cruise.qry <- "select unique year, cruise6, svvessel, season
    from mstr_cruise
    where purpose_code = 10
    and year >= 1963
    and (season = 'FALL'
      or season = 'SPRING')
    order by year, cruise6"
  }

if(all.season == 'y'){
  cruise.qry <- "select unique year, cruise6, svvessel, season
    from mstr_cruise
    where purpose_code = 10
    and year >= 1963
    order by year, cruise6"
  }
    
cruise <- as.data.table(sqlQuery(channel, cruise.qry))
cruise <- na.omit(cruise)
setkey(cruise, CRUISE6, SVVESSEL)

#Use cruise codes to select other data
cruise6 <- sqltext(cruise$CRUISE6)

#Station data
if(shg.check == 'y'){
  preHB.station.qry <- paste("select unique cruise6, svvessel, station, stratum,
                             tow, decdeg_beglat as lat, decdeg_beglon as lon, 
                             begin_est_towdate as est_towdate, avgdepth as depth, 
                             surftemp, surfsalin, bottemp, botsalin
                             from Union_fscs_svsta
                             where cruise6 in (", cruise6, ")
                             and SHG <= 136
                             and cruise6 <= 200900
                             order by cruise6, station", sep='')
  
  HB.station.qry <- paste("select unique cruise6, svvessel, station, stratum,
                          tow, decdeg_beglat as lat, decdeg_beglon as lon, 
                          begin_est_towdate as est_towdate, avgdepth as depth, 
                          surftemp, surfsalin, bottemp, botsalin
                          from Union_fscs_svsta
                          where cruise6 in (", cruise6, ")
                          and TOGA <= 1324
                          and cruise6 > 200900
                          order by cruise6, station", sep='')
  
  preHB.sta <- as.data.table(sqlQuery(channel, preHB.station.qry))
  HB.sta    <- as.data.table(sqlQuery(channel, HB.station.qry))
  station   <- rbindlist(list(preHB.sta, HB.sta))
  }

if(shg.check == 'n'){
  station.qry <- paste("select unique cruise6, svvessel, station, stratum, tow,
                       decdeg_beglat as lat, decdeg_beglon as lon, 
                       begin_est_towdate as est_towdate, avgdepth as depth, 
                       surftemp, surfsalin, bottemp, botsalin
                       from UNION_FSCS_SVSTA
                       where cruise6 in (", cruise6, ")
                       order by cruise6, station", sep='')
  station <- as.data.table(sqlQuery(channel, station.qry))
  }
  
setkey(station, CRUISE6, SVVESSEL)

#merge cruise and station
survdat <- merge(cruise, station)


#Catch data
catch.qry <- paste("select cruise6, station, stratum, tow, svspp, catchsex, 
                   expcatchnum as abundance, expcatchwt as biomass
                   from UNION_FSCS_SVCAT
                   where cruise6 in (", cruise6, ")
                   and stratum not like 'YT%'
                   order by cruise6, station, svspp", sep='')

catch <- as.data.table(sqlQuery(channel, catch.qry))
setkey(catch, CRUISE6, STATION, STRATUM, TOW)

#merge with survdat
setkey(survdat, CRUISE6, STATION, STRATUM, TOW)
survdat <- merge(survdat, catch, by = key(survdat))

#Length data
length.qry <- paste("select cruise6, station, stratum, tow, svspp, catchsex, 
                    length, expnumlen as numlen
                    from UNION_FSCS_SVLEN
                    where cruise6 in (", cruise6, ")
                    and stratum not like 'YT%'
                    order by cruise6, station, svspp, length", sep='')

len <- as.data.table(sqlQuery(channel, length.qry))
setkey(len, CRUISE6, STATION, STRATUM, TOW, SVSPP, CATCHSEX)

#merge with survdat
setkey(survdat, CRUISE6, STATION, STRATUM, TOW, SVSPP, CATCHSEX)
survdat <- merge(survdat, len, all.x = T)

if(raw.check == 'y'){
  survdat.raw <- survdat
  save(survdat.raw, file = paste(out.dir, "Survdat_raw.RData", sep =''))
  }

#Conversion Factors
#need to make abundance column a double instead of an integer
survdat[, ABUNDANCE := as.double(ABUNDANCE)]

#Grab all conversion factors off the network
convert.qry <- "select *
  from survan_conversion_factors"

convert <- as.data.table(sqlQuery(channel,convert.qry))

#DCF < 1985 Door Conversion
dcf.spp <- convert[DCF_WT > 0, SVSPP]
for(i in 1:length(dcf.spp)){
  survdat[YEAR < 1985 & SVSPP == dcf.spp[i],
      BIOMASS := BIOMASS * convert[SVSPP == dcf.spp[i], DCF_WT]]
  }
dcf.spp <- convert[DCF_NUM > 0, SVSPP]
for(i in 1:length(dcf.spp)){
  survdat[YEAR < 1985 & SVSPP == dcf.spp[i],
      ABUNDANCE := round(ABUNDANCE * convert[SVSPP == dcf.spp[i], DCF_NUM])]
  }

#GCF Spring 1973-1981  Net Conversion
gcf.spp <- convert[GCF_WT > 0, SVSPP]
for(i in 1:length(gcf.spp)){
  survdat[SEASON == 'SPRING' & YEAR > 1972 & YEAR < 1982 & SVSPP == gcf.spp[i],
      BIOMASS := BIOMASS / convert[SVSPP == gcf.spp[i], GCF_WT]]
  }
gcf.spp <- convert[GCF_NUM > 0, SVSPP]
for(i in 1:length(gcf.spp)){
  survdat[SEASON == 'SPRING' & YEAR > 1972 & YEAR < 1982 & SVSPP == gcf.spp[i],
      ABUNDANCE := round(ABUNDANCE / convert[SVSPP == gcf.spp[i], GCF_NUM])]
  }

#VCF SVVESSEL = DE  Vessel Conversion
vcf.spp <- convert[VCF_WT > 0, SVSPP]
for(i in 1:length(vcf.spp)){
  survdat[SVVESSEL == 'DE' & SVSPP == vcf.spp[i],
      BIOMASS := BIOMASS * convert[SVSPP == vcf.spp[i], VCF_WT]]
  }
vcf.spp <- convert[VCF_NUM > 0, SVSPP]
for(i in 1:length(vcf.spp)){
  survdat[SVVESSEL == 'DE' & SVSPP == vcf.spp[i],
      ABUNDANCE := round(ABUNDANCE * convert[SVSPP == vcf.spp[i], VCF_NUM])]
  }

#Bigelow >2008 Vessel Conversion - need flat files (not on network)
#Use Bigelow conversions for Pisces as well (PC)
big.fall   <- as.data.table(read.csv(file.path(data.dir, 
                                               'bigelow_fall_calibration.csv')))
big.spring <- as.data.table(read.csv(file.path(data.dir, 
                                               'bigelow_spring_calibration.csv')))

bf.spp <- big.fall[pW != 1, svspp]
for(i in 1:length(bf.spp)){
  survdat[SVVESSEL %in% c('HB', 'PC') & SEASON == 'FALL' & SVSPP == bf.spp[i],
      BIOMASS := BIOMASS / big.fall[svspp == bf.spp[i], pW]]
  }
bf.spp <- big.fall[pw != 1, svspp]
for(i in 1:length(bf.spp)){
  survdat[SVVESSEL %in% c('HB', 'PC') & SEASON == 'FALL' & SVSPP == bf.spp[i],
      ABUNDANCE := round(ABUNDANCE / big.fall[svspp == bf.spp[i], pw])]
  }

bs.spp <- big.spring[pW != 1, svspp]
for(i in 1:length(bs.spp)){
  survdat[SVVESSEL %in% c('HB', 'PC') & SEASON == 'SPRING' & SVSPP == bs.spp[i],
      BIOMASS := BIOMASS / big.spring[svspp == bs.spp[i], pW]]
  }
bs.spp <- big.spring[pw != 1, svspp]
for(i in 1:length(bs.spp)){
  survdat[SVVESSEL %in% c('HB', 'PC') & SEASON == 'SPRING' & SVSPP == bs.spp[i],
      ABUNDANCE := round(ABUNDANCE / big.spring[svspp == bs.spp[i], pw])]
  }

if(use.SAD == 'y'){
  sad.qry <- "select svspp, cruise6, stratum, tow, station, sex as catchsex,
             catch_wt_B_cal, catch_no_B_cal, length, length_no_B_cal
             from STOCKEFF.I_SV_MERGED_CATCH_CALIB_O"
  sad     <- as.data.table(sqlQuery(channel, sad.qry))
  
  setkey(sad, CRUISE6, STRATUM, TOW, STATION, SVSPP, CATCHSEX, LENGTH)
  sad <- unique(sad)
  survdat <- merge(survdat, sad, by = key(sad), all.x = T)
  
  #Carry over SAD values to survdat columns and delete SAD columns
  survdat[!is.na(CATCH_WT_B_CAL),  BIOMASS   := CATCH_WT_B_CAL]
  survdat[!is.na(CATCH_NO_B_CAL),  ABUNDANCE := CATCH_NO_B_CAL]
  survdat[, NUMLEN := as.double(NUMLEN)]
  survdat[!is.na(LENGTH_NO_B_CAL), NUMLEN    := LENGTH_NO_B_CAL]
  survdat[, c('CATCH_WT_B_CAL', 'CATCH_NO_B_CAL', 'LENGTH_NO_B_CAL') := NULL]
}

odbcClose(channel)

if(all.season == 'n') save(survdat, file = file.path(out.dir, "Survdat.RData"))
if(all.season == 'y') save(survdat, file = file.path(out.dir, 
                                                     "Survdat_allseason.RData"))

```

### Data analysis
The fisheries independent data contained within the Survdat is used in a variety of
products; the more complicated analyses are detailed in their own sections.  The most straightforward use of this data is for the resource species aggregate biomass 
indicators.  For the purposes of the aggregate biomass indicators, fall and spring 
survey data are treated separately.  Additionally, all length data is dropped and 
species seperated by sex at the catch level are merged back together.

For the aggregate biomass indicators, Survdat is first post stratified into [Ecological Production Units](#epu). Stations are labeled by the EPU they fall within
using the *over* function from the *rdgal* R package [@rgdal].  Next, the total number 
of stations within each EPU per year is counted using unique station records. Biomass
is summed by species per year per EPU.  Those sums are divided by the appropriate
station count to get the EPU mean.  Finally, the mean biomasses are summed by [aggregate groups](#aggroups).

### Plotting

```{r fall-survey-MAB, fig.cap="Fall (left) and spring (right) MAB Survey Biomass (A: Piscivore, B: Planktivore, C: Benthivore, D: Benthos).",  echo = T, fig.show='hold', fig.align='default',warning = F, message = F,fig.pos='H',out.width='50%', fig.height=6}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mfrow = c(4, 1), mar = c(0, 0, 0, 0), oma = c(4, 6.5, 2, 6))

soe.plot(SOE.data, "Time", "Piscivore Fall Biomass Index MAB", 
         stacked = "A", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Planktivore Fall Biomass Index MAB", 
         stacked = "B", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Benthivore Fall Biomass Index MAB", 
         stacked = "C", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Benthos Fall Biomass Index MAB", 
         stacked = "D", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
#soe.plot(SOE.data, "Time", "Total Fall Biomass MAB", stacked = "E",status  = F,
#         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 1)
soe.stacked.axis("Year", expression("Biomass, kg tow"^-1), y.line = 3.0,
                 x.line = 2.7)

opar <- par(mfrow = c(4, 1), mar = c(0, 0, 0, 0), oma = c(4, 6.5, 2, 6))

soe.plot(SOE.data, "Time", "Piscivore Spring Biomass Index MAB", 
         stacked = "A", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Planktivore Spring Biomass Index MAB", 
         stacked = "B", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Benthivore Spring Biomass Index MAB", 
         stacked = "C", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Benthos Spring Biomass Index MAB", 
         stacked = "D", rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5)
#soe.plot(SOE.data, "Time", "Total Spring Biomass MAB", stacked = "E",status  = F,
#         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 1)
soe.stacked.axis("Year", expression("Biomass, kg tow"^-1*''), y.line = 3.0,
                 x.line = 2.7)

```

<!--chapter:end:chapters/survey_data.rmd-->

# Recreational Fishing Indicators

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```

**Description**: Total catch and total angler trips by region (North Atlantic and Mid-Atlantic)

**Indicator category**: Database pull

**Contributor(s)**: Geret DePiper
  
**Data steward**: Geret DePiper, <geret.depiper@noaa.gov>
 
**Point of contact**: Geret DePiper, <geret.depiper@noaa.gov>

**Public availability statement**: Data sets are publically available (see Data Sources below).
  

## Methods
We use total recreational harvest as an indicator of seafood production and total recreational trips and total recreational anglers as proxies for recreational value generated from the Mid-Atlantic and New England regions respectively.

### Data sources
All recreational fishing indicator data, including number of recreationally harvested fish, number of angler trips, and number of anglers, were download from the [MRIP Recreational Fisheries Statistics Queries](https://www.st.nmfs.noaa.gov/recreational-fisheries/data-and-documentation/queries/index) portal on December 12, 2017. Relevant metadata including information regarding data updates are available at the query site. Note that 2017 data were considered preliminary at the time of the data pull.

Data sets are queried by region on the MRIP site, and for the purposes of the SOE, the "NORTH ATLANTIC" and "MID-ATLANTIC" regions were mapped to the New England and Mid-Atlantic report versions respectively. All query pages are accessible through the [MRIP Recreational Fisheries Statistics](https://www.st.nmfs.noaa.gov/recreational-fisheries/data-and-documentation/queries/index) site. 

The number of recreationally harvested fish was found by selecting "TOTAL HARVEST (A + B1)" on the Catch Time Series Query page. Angler trips (listed as "TOTAL" trips) were pulled from the MRIP Effort Time Series Query page, and included data from 1981 - 2017.The number of anglers was total number of anglers from the MRFSS Participation Time Series Query, and includes data from 1981 - 2016.

### Data extraction 
Data were download as .CSV files before being brought into R.

### Data analysis
NA

### Plotting

```{r recreation, fig.cap="Number of anglers (A) and number of fishing trips (B) taken in the Mid Atlantic Bight.", echo=T, message=FALSE, warning=FALSE,fig.pos='H'}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(4, 6, 2, 6))


soe.plot(SOE.data, "Time", "Mid-Atlantic Rec participation", stacked = "A",
         rel.y.num = 0.9, end.start = 2008, tol = 0.15, full.trend = F, 
         cex.stacked = 1.5, scale.axis = 10^6)
soe.stacked.axis('Year', expression('Anglers, 10'^6*'n'), 
                 rel.x.text = 0.1, rel.y.text = 1, y.line = 2.5, outer = F)

soe.plot(SOE.data, "Time", "Mid-Atlantic angler trips", stacked = "B",
         rel.y.num = 0.9, scale.axis = 10^6, x.end = 2016, end.start = 2007,
         tol = 0.15, y.lower = 0, ymin = F,  cex.stacked = 1.25)

soe.stacked.axis('Year', expression('Trips, 10'^6*'n'), 
                 rel.x.text = 1, rel.y.text = 1, y.line = 2.5, outer = F)

```





<!--chapter:end:chapters/Recreational_Data.Rmd-->

# Commercial Landings Data {#comdat}

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)
library(dplyr)
```

**Description**: Commercial landings data pull

**Indicator category**: Database pull

**Contributor(s)**: Sean Lucey

**Data steward**: Sean Lucey, <Sean.Lucey@noaa.gov>

**Point of contact**: Sean Lucey, <Sean.Lucey@noaa.gov>

**Public availability statement**: Raw data are not publically available due to confidentiality of individual fishery participants.  Derived indicator outputs are
available [here](https://comet.nefsc.noaa.gov/erddap/tabledap/group_landings_soe_v1.html).


## Methods

Fisheries dependent data for the Northeast Shelf extend back several decades. Data from the 1960s on are housed in the Commercial database (CFDBS) of the Northeast Fisheries Science Center which contains the commercial fisheries dealer purchase records (weigh-outs) collected by NMFS Statistical Reporting Specialists and state agencies from Maine to Virginia. The data format has changed slightly over the time series with three distinct time frames as noted in table \@ref(tab:calibration) below.  

```{r, eval = T, echo = F}
com.tables <- data.frame(Table = c('WOLANDS', 'WODETS', 'CFDETS_AA'),
                         Years = c('1964 - 1981', '1982 - 1993', '> 1994'))
kable(com.tables,caption="Data formats")
```

Comlands is an R database pull that consolidates the landings records from 1964 on and attempts to associate them with NAFO statistical areas (Figure \@ref(fig:StatAreaMap)). The script is divided into three sections. The first pulls domestic landings data from the yearly landings tables and merges them into a single data source. The second section applies an algorithm to associate landings that are not allocated to a statistical area using similar characteristics of the trip to trips with known areas. The final section pulls foreign landings from the NAFO website and rectifies species and gear codes so they can be merged along with domestic landings.

```{r StatAreaMap, fig.cap="Map of the North Atlantic Fisheries Organization (NAFO) Statistical Areas.  Colors represent the Ecological Production Unit (EPU) with which the statistical area is associated.", echo=F, eval=T}

image.dir <- './images'

knitr::include_graphics(file.path(image.dir, 'Stat_Area_Map.jpg'))
```

During the first section, the Comlands script pulls the temporal and spatial information as well as vessel and gear characteristics associated with the landings in addition to the weight, value, and utilization code of each species in the landings record.  The script includes a toggle to use landed weights as opposed to live weights.  For all but shellfish species, live weights are used for the State of the Ecosystem report.  Due to the volume of data contained within each yearly landings table, landings are aggregated by species, utilization code, and area as well as by month, gear, and tonnage class.  All weights are then converted from pounds to metric tons.  Landings values are also adjusted for inflation using the Producer Price Index by Commodity for Processed Foods and Feeds: Unprocessed and Packaged Fish.  Inflation is based on January of the terminal year of the data pull ensuring that all values are in current dollar prices.

Several species have additional steps after the data is pulled from CFDBS.  Skates are typically landed as a species complex.  In order to segregate the catch into species, the ratio of individual skate species in the NEFSC bottom trawl survey is used to disaggregate the landings. A similar algorithm is used to separate silver and offshore hake which can be mistaken for one another.  Finally, Atlantic herring landings are pulled from a separate database as the most accurate weights are housed by the State of Maine.  Comlands pulls from the State database and replaces the less accurate numbers from the federal database.

The majority of landings data are associated with a NAFO Statistical Area.  For those that are not, Comlands attempts to assign them to an area using similar characteristics of trips where the area is known.  To simplify this task, landings data are further aggregated into quarter and half year, small and large vessels, and eight major gear categories (Table \@ref(tab:gears)).  Landings are then proportioned to areas that meet similar characteristics based on the proportion of landings in each area by that temporal/vessel/gear combination.  If a given attribute is unknown, the algorithm attempts to assign it one, once again based on matched characteristics of known trips.  Statistical areas are then assigned to their respective [Ecological Production Unit](#epu) (Table \@ref(tab:statareas)).  

```{r gears, eval = T, echo = F}
gear.table <- data.frame('Major gear' = c('Otter Trawls', 'Scallop Dredges', 
                                        'Other Dredges', 'Gillnets', 'Longlines',
                                        'Seines', 'Pots/Traps', 'Midwater', 'Other'))
names(gear.table) <- "Major gear"
knitr::kable(gear.table,
      caption="Gear types", format = "html") %>%
  kableExtra::kable_styling(full_width = F) %>% 
  kableExtra::column_spec(1,width = "8cm")
```

```{r statareas, eval = T, echo = F}
area.table <- data.frame(EPU = c('Gulf of Maine', 'Georges Bank', 'Mid-Atlantic'),
                         'Stat Areas' = c('500, 510, 512, 513, 514, 515',
                                        '521, 522, 523, 524, 525, 526, 551, 552, 561, 562',
                                        '537, 539, 600, 612, 613, 614, 615, 616, 621, 622, 625, 626, 631, 632'))
names(area.table)[2] <- "Stat Areas"
kable(area.table, caption = "Statistical areas making up each EPU", format = "html")
```

The final step of Comlands is to pull the foreign landings from the [NAFO database](https://www.nafo.int/Data/frames).  US landings are removed from this extraction so as not to be double counted.  NAFO codes and CFDBS codes differ so the script rectifies those codes to ensure that the data is seamlessly merged into the domestic landings.  Foreign landings are flagged so that they can be removed if so desired.


### Data sources
Comland is a database query of the NEFSC commercial fishery database (CFDBS). More information about the CFDBS is available [here](https://inport.nmfs.noaa.gov/inport/item/27401).  

### Data extraction 

R code used in the extraction process described above:
```{r comland, echo = T, eval = F}
#Comland.r
#Version now controlled by git - originally part of comcatch.r
#Grab commercial landings data from US and Foreign countries (NAFO)
#Need to fix menhaden data
#SML

#Requires the following files:
# data.dir.2\\Comland_skates_hakes.R
# data.dir\\Menhaden.csv
# data.dir.3\\SS_NAFO_21A.csv
# data.dir.3\\species.txt

#User parameters
if(Sys.info()['sysname']=="Windows"){
  data.dir   <- "L:\\EcoAP\\Data\\Commercial"
  data.dir.2 <- "L:\\Rworkspace\\RCom"
  data.dir.3 <- "L:\\EcoAP\\Data\\NAFO"
  out.dir    <- "L:\\EcoAP\\Data\\Commercial"
  memory.limit(4000)
  channel <- odbcDriverConnect()
}

if(Sys.info()['sysname']=="Linux"){
  data.dir   <- "/home/slucey/slucey/EcoAP/Data/Commercial"
  data.dir.2 <- "/home/slucey/slucey/Rworkspace/RCom"
  data.dir.3 <- "/home/slucey/slucey/EcoAP/Data/NAFO"
  out.dir    <- "/home/slucey/slucey/EcoAP/Data/Commercial"
  uid <- 'slucey'
  cat("Oracle Password: ")
  pwd <- scan(stdin(), character(), n = 1)
}

landed       <- 'y' #use landed weight for scallops and clams instead of live weight
foreign      <- 'y' #Mark foreign landings and keep seperate
adjust.ppi   <- 'y' #Adjust value for inflation
use.existing <- 'n' #use raw data from a previous run - saves time
sum.by       <- 'EPU' #Variable to sum landings by [EPU, stat.area]

#Final year of query
endyear <- 2016
#If adjusting for inflation
refyear  <- 2016
refmonth <- 1

#-------------------------------------------------------------------------------
#Required packages
library(RODBC); library(data.table); library(rgdal)

#-------------------------------------------------------------------------------
#User created functions 
#Convert NA's to zeros
na.zero <- function(x){
  for(i in 1:length(x[1, ])){
    if(length(which(is.na(x[, i]))) > 0){
      x[which(is.na(x[, i])), i] <- 0}
    }
    return(x)
  } 
  
#-------------------------------------------------------------------------------
#Connect to database
if(Sys.info()['sysname'] == "Windows")channel <- odbcDriverConnect()
if(Sys.info()['sysname'] == "Linux")  channel <- odbcConnect('sole', uid, pwd)

if(use.existing == 'n'){
  #Landings
  tables <- c(paste0('WOLANDS', 64:81), 
              paste0('WODETS',  82:93), 
              paste0('CFDETS',  1994:endyear, 'AA'))
  
  #Generate one table
  comland <- c()
  for(i in 1:length(tables)){
    landings.qry <- paste("select year, month, negear, toncl1, nespp3, nespp4, area, 
                           spplivlb, spplndlb, sppvalue, utilcd
                           from", tables[i])
    
    comland.yr <- as.data.table(sqlQuery(channel, landings.qry))
    
    setkey(comland.yr,
           YEAR,
           MONTH,
           NEGEAR,
           TONCL1,
           NESPP3,
           NESPP4,
           AREA,
           UTILCD)
  
    if(landed == 'y') comland.yr[NESPP3 %in% 743:800, SPPLIVLB := SPPLNDLB]
    
    #Sum landings and value
    #landings
    comland.yr[, V1 := sum(SPPLIVLB), by = key(comland.yr)]
    #value
    #Fix null values
    comland.yr[is.na(SPPVALUE), SPPVALUE := 0]
    comland.yr[, V2 := sum(SPPVALUE), by = key(comland.yr)]
    
    #Remove extra rows/columns
    comland.yr <- unique(comland.yr, by = key(comland.yr))
    comland.yr[, c('SPPLIVLB', 'SPPLNDLB', 'SPPVALUE') := NULL]
    
    #Rename summed columns
    setnames(comland.yr, c('V1', 'V2'), c('SPPLIVLB', 'SPPVALUE'))
    
    comland <- rbindlist(list(comland, comland.yr))
  }
  
  if(landed == 'n') save(comland, file = file.path(out.dir, "comland_raw_US.RData"))
  #Last run 8/31/16
  if(landed == 'y') save(comland, file = file.path(out.dir, "comland_raw_US_meatwt.RData"))
  #Last run 1/25/18 
}
if(use.existing == 'y'){
  if(landed == 'n') load(file = file.path(out.dir, "comland_raw_US.RData"))
  if(landed == 'y') load(file = file.path(out.dir, "comland_raw_US_meatwt.RData"))  
}

#-------------------------------------------------------------------------------
#Convert from lbs to metric tons
comland[, SPPLIVMT := SPPLIVLB * 0.00045359237]
comland[, SPPLIVLB := NULL]

#fix years
comland[YEAR < 100, YEAR := YEAR + 1900L]

if(adjust.ppi == 'y'){
    #Adjust SPPVALUE for inflation
    temp <- tempfile()
    download.file("http://download.bls.gov/pub/time.series/wp/wp.data.3.ProcessedFoods", temp)
    inflate <- as.data.table(read.delim(temp))
    unlink(temp)
    
    inflate[, series_id := gsub(" ", "", inflate[, series_id])]
    deflate <- inflate[series_id == "WPU0223", ]
    deflate[, MONTH := as.numeric(substr(period, 2, 3))]
    setnames(deflate, c('year', 'value'), c('YEAR', 'PPI'))
    deflate <- deflate[, list(YEAR, MONTH, PPI)]
    
    #Set yearly deflator to 0 instead of 13 to match unknown month designation
    deflate[MONTH == 13, MONTH := 0]
    deflate.base <- deflate[YEAR == refyear & MONTH == refmonth, PPI]
    
    comland <- merge(comland, deflate, by = c('YEAR', 'MONTH'), all.x = T)
    comland[, SPPVALUE := round((SPPVALUE * deflate.base) / PPI)]
    
    #Remove extra column
    comland[, PPI := NULL]
}
#Remove market categories of parts
comland <- comland[!NESPP4 %in% c(119, 123, 125, 127, 812, 819, 828, 829, 1731, 2351,
                                  2690, 2699, 3472, as.numeric(paste(348:359, 8, sep = '')), 
                                  3868, as.numeric(paste(469:471, 4, sep = '')), 
                                  as.numeric(paste(480:499, 8, sep ='')), 5018, 5039, 
                                  5261, 5265), ]

#Generate NESPP3 and MKTCAT in comland data
comland[NESPP4 < 100,                MKTCAT := as.numeric(substring(NESPP4, 2, 2))]
comland[NESPP4 > 99 & NESPP4 < 1000, MKTCAT := as.numeric(substring(NESPP4, 3, 3))]
comland[NESPP4 > 999,                MKTCAT := as.numeric(substring(NESPP4, 4, 4))]

#drop NESPP4
comland[, NESPP4 := NULL]

#Deal with Hakes and Skates------------------------------------------------------------------
source(file.path(data.dir.2, 'Comland_skates_hakes.R'))

#get little skates and winter skates from skates(ns) - use survey in half years
#Generate Half year variable in comland
comland.skates <- comland[NESPP3 == 365, ]
comland.skates[MONTH %in% 1:6,  Half := 1]
comland.skates[MONTH %in% 7:12, Half := 2]

setkey(skate.hake.us,
       YEAR,
       Half,
       AREA)

comland.skates <- merge(comland.skates, skate.hake.us, by = key(skate.hake.us), all.x = T)

comland.skates[, little       := little.per * SPPLIVMT]
comland.skates[, little.value := round(little.per * SPPVALUE)]
comland.skates[is.na(little),       little       := 0]
comland.skates[is.na(little.value), little.value := 0]

comland.skates[, winter       := winter.per * SPPLIVMT]
comland.skates[, winter.value := round(winter.per * SPPVALUE)]
comland.skates[is.na(winter),       winter       := 0]
comland.skates[is.na(winter.value), winter.value := 0]

comland.skates[, other.skate       := SPPLIVMT - (little       + winter)]
comland.skates[, other.skate.value := SPPVALUE - (little.value + winter.value)]

#Little (366), winter (367), skates(ns) (365)
#put skates in comland format to merge back
little <- comland.skates[, list(YEAR, Half, AREA, MONTH, NEGEAR,
                                TONCL1, NESPP3, UTILCD, MKTCAT, little, 
                                little.value)]
little[, NESPP3 := 366]
setnames(little, c('little', 'little.value'), c('SPPLIVMT', 'SPPVALUE'))
little <- little[SPPLIVMT > 0, ]

winter <- comland.skates[, list(YEAR, Half, AREA, MONTH, NEGEAR,
                                TONCL1, NESPP3, UTILCD, MKTCAT, winter, 
                                winter.value)]
winter[, NESPP3 := 367]
setnames(winter, c('winter', 'winter.value'), c('SPPLIVMT', 'SPPVALUE'))
winter <- winter[SPPLIVMT > 0, ]

other <- comland.skates[, list(YEAR, Half, AREA, MONTH, NEGEAR,
                               TONCL1, NESPP3, UTILCD, MKTCAT, other.skate, 
                               other.skate.value)]
other[, NESPP3 := 365]
setnames(other, c('other.skate', 'other.skate.value'), c('SPPLIVMT', 'SPPVALUE'))
other <- other[SPPLIVMT > 0, ]

#merge all three and reformat for comland
skates.add.back <- rbindlist(list(little, winter, other))

skates.add.back[, Half := NULL]
setcolorder(skates.add.back, names(comland))

comland <- rbindlist(list(comland[NESPP3 != 365, ], skates.add.back))  

#get silver hake from mixed hakes - use survey in half years
#Generate Half year variable in comland
comland.hakes <- comland[NESPP3 == 507, ]
comland.hakes[MONTH %in% 1:6,  Half := 1]
comland.hakes[MONTH %in% 7:12, Half := 2]

comland.hakes <- merge(comland.hakes, skate.hake.us, by = key(skate.hake.us), all.x = T)

comland.hakes[, silver       := silver.per * SPPLIVMT]
comland.hakes[, silver.value := round(silver.per * SPPVALUE)]
comland.hakes[is.na(silver),       silver       := 0]
comland.hakes[is.na(silver.value), silver.value := 0]

comland.hakes[, off.hake       := SPPLIVMT - silver]
comland.hakes[, off.hake.value := SPPVALUE - silver.value]

#Silver hake (509), mix hakes (507)
#put hakes in comland format to merge back
silver <- comland.hakes[, list(YEAR, Half, AREA, MONTH, NEGEAR,
                               TONCL1, NESPP3, UTILCD, MKTCAT, silver, 
                               silver.value)]
silver[, NESPP3 := 509]
setnames(silver, c('silver', 'silver.value'), c('SPPLIVMT', 'SPPVALUE'))
silver <- silver[SPPLIVMT > 0, ]

offshore <- comland.hakes[, list(YEAR, Half, AREA, MONTH, NEGEAR,
                                 TONCL1, NESPP3, UTILCD, MKTCAT, off.hake, 
                                 off.hake.value)]
offshore[, NESPP3 := 507]
setnames(offshore, c('off.hake', 'off.hake.value'), c('SPPLIVMT', 'SPPVALUE'))
offshore <- offshore[SPPLIVMT > 0, ]

#merge both and reformat for comland
hakes.add.back <- rbindlist(list(silver, offshore))

hakes.add.back[, Half := NULL]
setcolorder(hakes.add.back, names(comland))

comland <- rbindlist(list(comland[NESPP3 != 507, ], hakes.add.back))


#Herring---------------------------------------------------------------------------------
#Herring data is housed by the state of Maine.
herr.qry <- "select year, month, stock_area, negear, gearname, keptmt, discmt
             from maine_herring_catch"

herr.catch <- as.data.table(sqlQuery(channel, herr.qry))
setkey(herr.catch, YEAR, MONTH, STOCK_AREA, NEGEAR)

herring <- herr.catch[, list(sum(KEPTMT), sum(DISCMT)), by = key(herr.catch)]
setnames(herring, c('STOCK_AREA', 'V1', 'V2'),
                  c('AREA', 'SPPLIVMT', 'DISCMT'))

#Using averages from comland to fill in categories
herring[, MKTCAT := 5]
herring[, TONCL1 := 2]
herring[, UTILCD := 0]

#compute price/utilization from CF tables
herring.comland <- comland[NESPP3 == 168, ]
#Price from comland
herring.price <- herring.comland[, (sum(SPPVALUE) / sum(SPPLIVMT)), by = c('YEAR', 'MONTH')]
setnames(herring.price, 'V1', 'price')
herring <- merge(herring, herring.price, by = c('YEAR', 'MONTH'), all.x = T)
#Use 1964 prices for < 1964
herring[YEAR < 1964, price := mean(herring[YEAR == 1964, price])]
#Calculate SPPVALUE from price
herring[, SPPVALUE := round(price * SPPLIVMT)]

#Utilization from comland
herring.util <- herring.comland[, sum(SPPLIVMT), by = c('YEAR', 'MONTH', 'UTILCD')]
setnames(herring.util, 'V1', 'SPPLIVMT')
herring.util[, SPPLIVMT.ALL := sum(SPPLIVMT), by = c('YEAR', 'MONTH')]
herring.util[, Prop := SPPLIVMT/SPPLIVMT.ALL]
setorder(herring.util, YEAR, MONTH, Prop)
herring.util[, cum.prop := cumsum(Prop), by = c('YEAR', 'MONTH')]

#Apply proportions to Maine data set
#Not pulled all the time - current through 2017
herring[, Total := sum(SPPLIVMT), by = c('YEAR', 'MONTH')]
herring[, Prop := SPPLIVMT / Total]
setorder(herring, YEAR, MONTH, Prop)
herring[, cum.prop := cumsum(Prop), by = c('YEAR', 'MONTH')]

for(iyear in unique(herring.util[, YEAR])){
  for(imonth in unique(herring.util[YEAR == iyear, MONTH])){
    cum.prop.low <- 0
    for(iutil in herring.util[YEAR == iyear & MONTH == imonth, UTILCD]){
      cum.prop.high <- herring.util[YEAR == iyear & MONTH == imonth & 
                                      UTILCD == iutil, cum.prop]
      herring[YEAR == iyear & MONTH == imonth & cum.prop <= cum.prop.high &
                cum.prop > cum.prop.low, UTILCD := iutil]
      cum.prop.low <- cum.prop.high
    }
  }
}

#fix column headings
herring[, c('Total', 'Prop', 'cum.prop', 'price', 'DISCMT') := NULL]
herring[, NESPP3 := 168]
setcolorder(herring, names(comland))

#remove herring from data pull and add in Maine numbers
comland <- rbindlist(list(comland[NESPP3 != 168, ], herring))

#Menhaden------------------------------------------------------------------------------------
##fix menhaden records - data from Tom Miller/ Andre Bouchheister
#menhaden <- as.data.table(read.csv(paste(data.dir, "Menhaden.csv", sep = '')))
#menhaden.mab <- menhaden[, MA.Total + CB.Total, by = Year]
##file metric is 1000s of lbs - convert to mt
#menhaden.mab[, SPPLIVMT := (V1 * 1000) *  0.00045359237]
#menhaden.mab[, V1 := NULL]
#
#menhaden.gom <- menhaden[, list(Year, NE.Total)]
#menhaden.gom[, SPPLIVMT := (NE.Total * 1000) *  0.00045359237]
#menhaden.gom[, NE.Total := NULL]

#save(comland, file = paste(out.dir, "Comland_unkA.RData", sep = ''))

#Deal with unknowns-------------------------------------------------------------------------
comland[NEGEAR == 999,  NEGEAR := 0]
comland[is.na(TONCL1),  TONCL1 := 0]
comland[is.na(AREA),    AREA   := as.factor(0)]
comland[AREA == 999,    AREA   := as.factor(0)]
comland[is.na(MKTCAT),  MKTCAT := 0]
comland[is.na(UTILCD),  UTILCD := 0]

#1 - drop unknown species/landings
comland <- comland[NESPP3 != 0 & SPPLIVMT != 0, ]

#Sumarry tables
#missing area
#known.area <-   comland[AREA != 0, sum(SPPLIVMT), by = NESPP3]
#unknown.area <- comland[AREA == 0, sum(SPPLIVMT), by = NESPP3]
#setnames(known.area,   "V1", "AREA.MT.known")
#setnames(unknown.area, "V1", "AREA.MT.unknown")
#missing.table <- merge(known.area, unknown.area, by = 'NESPP3', all = T)
#
#missing.table[is.na(AREA.MT.known),   AREA.MT.known   := 0]
#missing.table[is.na(AREA.MT.unknown), AREA.MT.unknown := 0]
#missing.table[, AREA.Ratio := AREA.MT.unknown / AREA.MT.known]
#
##missing month
#known.month <-   comland[MONTH != 0, sum(SPPLIVMT), by = NESPP3]
#unknown.month <- comland[MONTH == 0, sum(SPPLIVMT), by = NESPP3]
#setnames(known.month,   "V1", "MONTH.MT.known")
#setnames(unknown.month, "V1", "MONTH.MT.unknown")
#missing.table <- merge(missing.table, known.month,   by = 'NESPP3', all = T)
#missing.table <- merge(missing.table, unknown.month, by = 'NESPP3', all = T)
#
#missing.table[is.na(MONTH.MT.known),   MONTH.MT.known   := 0]
#missing.table[is.na(MONTH.MT.unknown), MONTH.MT.unknown := 0]
#missing.table[, MONTH.Ratio := MONTH.MT.unknown / MONTH.MT.known]
#
##missing gear
#known.gear <-   comland[NEGEAR != 0, sum(SPPLIVMT), by = NESPP3]
#unknown.gear <- comland[NEGEAR == 0, sum(SPPLIVMT), by = NESPP3]
#setnames(known.gear,   "V1", "GEAR.MT.known")
#setnames(unknown.gear, "V1", "GEAR.MT.unknown")
#missing.table <- merge(missing.table, known.gear,   by = 'NESPP3', all = T)
#missing.table <- merge(missing.table, unknown.gear, by = 'NESPP3', all = T)
#
#missing.table[is.na(GEAR.MT.known),   GEAR.MT.known   := 0]
#missing.table[is.na(GEAR.MT.unknown), GEAR.MT.unknown := 0]
#missing.table[, GEAR.Ratio := GEAR.MT.unknown / GEAR.MT.known]
#
##missing tonnage class
#known.tc <-   comland[TONCL1 != 0, sum(SPPLIVMT), by = NESPP3]
#unknown.tc <- comland[TONCL1 == 0, sum(SPPLIVMT), by = NESPP3]
#setnames(known.tc,   "V1", "TC.MT.known")
#setnames(unknown.tc, "V1", "TC.MT.unknown")
#missing.table <- merge(missing.table, known.tc,   by = 'NESPP3', all = T)
#missing.table <- merge(missing.table, unknown.tc, by = 'NESPP3', all = T)
#
#missing.table[is.na(TC.MT.known),   TC.MT.known   := 0]
#missing.table[is.na(TC.MT.unknown), TC.MT.unknown := 0]
#missing.table[, TC.Ratio := TC.MT.unknown / TC.MT.known]
#
#write.csv(missing.table, paste(out.dir, "\\Missing_table.csv", sep = ''), row.names = F)
#

#2 - aggregate by quarter year, half year, major gear, and small/large TC
comland[MONTH %in% 1:3,   QY := 1]
comland[MONTH %in% 4:6,   QY := 2]
comland[MONTH %in% 7:9,   QY := 3]
comland[MONTH %in% 10:12, QY := 4]
comland[MONTH == 0,       QY := 0]

comland[MONTH %in% 1:6,  HY := 1]
comland[MONTH %in% 7:12, HY := 2]
comland[MONTH == 0,      HY := 0]

otter     <- 50:59
dredge.sc <- 131:132
pot       <- c(189:190, 200:219, 300, 301)
longline  <- c(10, 40)
seine     <- c(70:79, 120:129, 360)
gillnet   <- c(100:119, 500, 510, 520)
midwater  <- c(170, 370)
dredge.o  <- c(281, 282, 380:400)

comland[NEGEAR %in% otter,     GEAR := 'otter']
comland[NEGEAR %in% dredge.sc, GEAR := 'dredge.sc']
comland[NEGEAR %in% pot,       GEAR := 'pot']
comland[NEGEAR %in% longline,  GEAR := 'longline']
comland[NEGEAR %in% seine,     GEAR := 'seine']
comland[NEGEAR %in% gillnet,   GEAR := 'gillnet']
comland[NEGEAR %in% midwater,  GEAR := 'midwater']
comland[NEGEAR %in% dredge.o,  GEAR := 'dredge.o']
comland[NEGEAR == 0,           GEAR := 'unknown']
comland[is.na(GEAR),           GEAR := 'other']
comland[, GEAR := as.factor(GEAR)]

comland[TONCL1 %in% 1:3, SIZE := 'small']
comland[TONCL1 > 3,      SIZE := 'large']
comland[TONCL1 == 0,     SIZE := 'unknown']
comland[, SIZE := as.factor(SIZE)]

setkey(comland,
       YEAR,
       QY,
       HY,
       GEAR,
       SIZE,
       AREA,
       NESPP3,
       UTILCD)

comland.agg <- comland[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland)]

setnames(comland.agg, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))

#3 - Use proportions of known catch to assign unknown catch
#3.A QY/HY------------------------------------------------------------------------------
  unk.month <- comland.agg[QY == 0, ]
  k.month   <- comland.agg[QY != 0, ]
  
  #3.A.1 - All match
  match.key <- c('YEAR', 'NESPP3', 'GEAR', 'SIZE', 'AREA')
  
  unk.month.all <- unk.month[GEAR != 'unknown']
  unk.month.all <- unk.month.all[SIZE != 'unknown', ]
  unk.month.all <- unk.month.all[AREA != 0, ]
  
  k.month.all <- k.month[GEAR != 'unknown', ]
  k.month.all <- k.month.all[SIZE != 'unknown', ]
  k.month.all <- k.month.all[AREA != 0, ]
  
  setkeyv(unk.month.all, match.key)
  setkeyv(k.month.all,   match.key)
  
  month.all <- k.month.all[unk.month.all]
  
  #No match - need to match with larger aggregation
  no.match  <- month.all[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match, YEAR, NESPP3, AREA, GEAR)
  setkeyv(k.month.all, key(no.match))
  month.all.2 <- k.month.all[no.match]
  no.match.2 <- month.all.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match.2, YEAR, NESPP3, AREA)
  setkeyv(k.month.all, key(no.match.2))
  month.all.3 <- k.month.all[no.match.2]
  no.match.3 <- month.all.3[is.na(SPPLIVMT), ]
  no.match.3[, c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match.3, YEAR, NESPP3)
  setkeyv(k.month.all, key(no.match.3))
  month.all.4 <- k.month.all[no.match.3]
  no.match.4 <- month.all.4[is.na(SPPLIVMT), ]
  no.match.4[, c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.AREA', 'i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 
                         'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.4[, c('QY', 'HY') := 1]
  
  #Merge all together and proportion catch to known months
  month.all   <- month.all  [!is.na(SPPLIVMT), ]
  month.all.2 <- month.all.2[!is.na(SPPLIVMT), ]
  month.all.2[, SIZE   := i.SIZE]
  month.all.2[, i.SIZE := NULL]
  setcolorder(month.all.2, names(month.all))
  month.all.3 <- month.all.3[!is.na(SPPLIVMT), ]
  month.all.3[, GEAR   := i.GEAR]
  month.all.3[, SIZE   := i.SIZE]
  month.all.3[, i.GEAR := NULL]
  month.all.3[, i.SIZE := NULL]
  setcolorder(month.all.3, names(month.all))
  month.all.4 <- month.all.4[!is.na(SPPLIVMT), ]
  month.all.4[, AREA   := i.AREA]
  month.all.4[, GEAR   := i.GEAR]
  month.all.4[, SIZE   := i.SIZE]
  month.all.4[, i.AREA := NULL]
  month.all.4[, i.GEAR := NULL]
  month.all.4[, i.SIZE := NULL]
  setcolorder(month.all.4, names(month.all))
  
  month.all <- rbindlist(list(month.all, month.all.2, month.all.3, month.all.4))
  
  month.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.all[, unk  := i.SPPLIVMT * prop]
  month.all[, unk2 := i.SPPVALUE * prop]
  month.all[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 
                'i.QY', 'i.UTILCD', 'prop') := NULL]
  setnames(month.all, c('unk', 'unk2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setcolorder(no.match.4, names(month.all))
  month.solved <- rbindlist(list(month.all, no.match.4))
  rm(list = c(ls(pattern = 'month.all'), ls(pattern = 'no.match')))
  
  #3.A.2 - GEAR/SIZE
  match.key <- c('YEAR', 'NESPP3', 'GEAR', 'SIZE')
  
  unk.month.g.s <- unk.month[GEAR != 'unknown']
  unk.month.g.s <- unk.month.g.s[SIZE != 'unknown', ]
  unk.month.g.s <- unk.month.g.s[AREA == 0, ]
  unk.month.g.s <- unk.month.g.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                                 by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.g.s, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.g.s <- k.month[GEAR != 'unknown', ]
  k.month.g.s <- k.month.g.s[SIZE != 'unknown', ]
  k.month.g.s <- k.month.g.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.g.s, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.g.s, match.key)
  setkeyv(k.month.g.s,   match.key)
  
  month.g.s <- k.month.g.s[unk.month.g.s]
  
  #No match - need to match with larger aggregation
  no.match  <- month.g.s[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match, YEAR, NESPP3, GEAR)
  setkeyv(k.month.g.s, key(no.match))
  month.g.s.2 <- k.month.g.s[no.match]
  no.match.2 <- month.g.s.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match.2, YEAR, NESPP3)
  setkeyv(k.month.g.s, key(no.match.2))
  month.g.s.3 <- k.month.g.s[no.match.2]
  no.match.3 <- month.g.s.3[is.na(SPPLIVMT), ]
  no.match.3[, c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.3[, c('QY', 'HY') := 1]
  no.match.3[, AREA := 0]
  
  #Merge all together and proportion catch to known months
  month.g.s   <- month.g.s  [!is.na(SPPLIVMT), ]
  month.g.s.2 <- month.g.s.2[!is.na(SPPLIVMT), ]
  if(nrow(month.g.s.2) > 0){
    month.g.s.2[, SIZE   := i.SIZE]
    month.g.s.2[, i.SIZE := NULL]
    setcolorder(month.g.s.2, names(month.g.s))
    month.g.s <- rbindlist(list(month.g.s, month.g.s.2))  
  }
  month.g.s.3 <- month.g.s.3[!is.na(SPPLIVMT), ]
  if(nrow(month.g.s.3) > 0){
    month.g.s.3[, GEAR   := i.GEAR]
    month.g.s.3[, SIZE   := i.SIZE]
    month.g.s.3[, i.GEAR := NULL]
    month.g.s.3[, i.SIZE := NULL]
    setcolorder(month.g.s.3, names(month.g.s))
    month.g.s <- rbindlist(list(month.g.s, month.g.s.3))
  }
  
  month.g.s[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.g.s[, unk  := i.SPPLIVMT * prop]
  month.g.s[, unk2 := i.SPPVALUE * prop]
  month.g.s[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 
                'i.QY', 'i.UTILCD', 'prop') := NULL]
  setnames(month.g.s, c('unk', 'unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.g.s[, AREA := 0]
  
  setcolorder(month.g.s,  names(month.solved))
  setcolorder(no.match.3, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.g.s, no.match.3))
  rm(list = c(ls(pattern = 'month.g.s'), ls(pattern = 'no.match')))
  
  #3.A.3 - AREA/GEAR
  match.key <- c('YEAR', 'NESPP3', 'GEAR', 'AREA')
  
  unk.month.a.g <- unk.month[GEAR != 'unknown']
  unk.month.a.g <- unk.month.a.g[SIZE == 'unknown', ]
  unk.month.a.g <- unk.month.a.g[AREA != 0, ]
  unk.month.a.g <- unk.month.a.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                                 by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.a.g, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.a.g <- k.month[GEAR != 'unknown', ]
  k.month.a.g <- k.month.a.g[AREA != 0, ]
  k.month.a.g <- k.month.a.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.a.g, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.a.g, match.key)
  setkeyv(k.month.a.g,   match.key)
  
  month.a.g <- k.month.a.g[unk.month.a.g]
  
  #No match - need to match with larger aggregation
  no.match  <- month.a.g[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match, YEAR, NESPP3, AREA)
  setkeyv(k.month.a.g, key(no.match))
  month.a.g.2 <- k.month.a.g[no.match]
  no.match.2 <- month.a.g.2[is.na(SPPLIVMT), ]
  no.match.2[, c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.GEAR', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match.2, YEAR, NESPP3)
  setkeyv(k.month.a.g, key(no.match.2))
  month.a.g.3 <- k.month.a.g[no.match.2]
  no.match.3 <- month.a.g.3[is.na(SPPLIVMT), ]
  no.match.3[, c('AREA', 'GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.AREA', 'i.GEAR', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA', 'GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.3[, c('QY', 'HY') := 1]
  no.match.3[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  
  #Merge all together and proportion catch to known months
  month.a.g   <- month.a.g  [!is.na(SPPLIVMT), ]
  month.a.g.2 <- month.a.g.2[!is.na(SPPLIVMT), ]
  if(nrow(month.a.g.2) > 0){
    month.a.g.2[, GEAR   := i.GEAR]
    month.a.g.2[, i.GEAR := NULL]
    setcolorder(month.a.g.2, names(month.a.g))
    month.a.g <- rbindlist(list(month.a.g, month.a.g.2))  
  }
  month.a.g.3 <- month.a.g.3[!is.na(SPPLIVMT), ]
  if(nrow(month.a.g.3) > 0){
    month.a.g.3[, AREA   := i.AREA]
    month.a.g.3[, GEAR   := i.GEAR]
    month.a.g.3[, i.AREA := NULL]
    month.a.g.3[, i.GEAR := NULL]
    setcolorder(month.a.g.3, names(month.a.g))
    month.a.g <- rbindlist(list(month.a.g, month.a.g.3))  
  }
  
  month.a.g[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.a.g[, unk  := i.SPPLIVMT * prop]
  month.a.g[, unk2 := i.SPPVALUE * prop]
  month.a.g[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 
                'i.QY', 'i.UTILCD', 'prop') := NULL]
  setnames(month.a.g, c('unk', 'unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.a.g[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  
  setcolorder(month.a.g,  names(month.solved))
  setcolorder(no.match.3, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.a.g, no.match.3))
  rm(list = c(ls(pattern = 'month.a.g'), ls(pattern = 'no.match')))
  
  #3.A.4 - AREA/TC
  match.key <- c('YEAR', 'NESPP3', 'SIZE', 'AREA')
  
  unk.month.a.s <- unk.month[GEAR == 'unknown']
  unk.month.a.s <- unk.month.a.s[SIZE != 'unknown', ]
  unk.month.a.s <- unk.month.a.s[AREA != 0, ]
  unk.month.a.s <- unk.month.a.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                                 by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.a.s, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.a.s <- k.month[SIZE != 'unknown', ]
  k.month.a.s <- k.month.a.s[AREA != 0, ]
  k.month.a.s <- k.month.a.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.a.s, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.a.s, match.key)
  setkeyv(k.month.a.s,   match.key)
  
  month.a.s <- k.month.a.s[unk.month.a.s]
  
  #No match - need to match with larger aggregation
  no.match  <- month.a.s[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match, YEAR, NESPP3, AREA)
  setkeyv(k.month.a.s, key(no.match))
  month.a.s.2 <- k.month.a.s[no.match]
  no.match.2 <- month.a.s.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match.2, YEAR, NESPP3)
  setkeyv(k.month.a.s, key(no.match.2))
  month.a.s.3 <- k.month.a.s[no.match.2]
  no.match.3 <- month.a.s.3[is.na(SPPLIVMT), ]
  no.match.3[, c('AREA', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.AREA', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.3[, c('QY', 'HY') := 1]
  no.match.3[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  
  #Merge all together and proportion catch to known months
  month.a.s   <- month.a.s  [!is.na(SPPLIVMT), ]
  month.a.s.2 <- month.a.s.2[!is.na(SPPLIVMT), ]
  if(nrow(month.a.s.2) > 0){
    month.a.s.2[, SIZE   := i.SIZE]
    month.a.s.2[, i.SIZE := NULL]
    setcolorder(month.a.s.2, names(month.a.s))
    month.a.s <- rbindlist(list(month.a.s, month.a.s.2))
  }
  month.a.s.3 <- month.a.s.3[!is.na(SPPLIVMT), ]
  if(nrow(month.a.s.3) > 0){
    month.a.s.3[, AREA   := i.AREA]
    month.a.s.3[, SIZE   := i.SIZE]
    month.a.s.3[, i.AREA := NULL]
    month.a.s.3[, i.SIZE := NULL]
    setcolorder(month.a.s.3, names(month.a.s))
    month.a.s <- rbindlist(list(month.a.s, month.a.s.3))  
  }
  
  month.a.s[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.a.s[, unk  := i.SPPLIVMT * prop]
  month.a.s[, unk2 := i.SPPVALUE * prop]
  month.a.s[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 
                'i.QY', 'i.UTILCD', 'prop') := NULL]
  setnames(month.a.s, c('unk', 'unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.a.s[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  
  setcolorder(month.a.s,  names(month.solved))
  setcolorder(no.match.3, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.a.s, no.match.3))
  rm(list = c(ls(pattern = 'month.a.s'), ls(pattern = 'no.match')))

#3.A.5 - SIZE
  match.key <- c('YEAR', 'NESPP3', 'SIZE')
  
  unk.month.si <- unk.month[GEAR == 'unknown']
  unk.month.si <- unk.month.si[SIZE != 'unknown', ]
  unk.month.si <- unk.month.si[AREA == 0, ]
  unk.month.si <- unk.month.si[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                               by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.si, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.si <- k.month[SIZE != 'unknown', ]
  k.month.si <- k.month.si[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                           by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.si, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.si, match.key)
  setkeyv(k.month.si,   match.key)
  
  month.si <- k.month.si[unk.month.si]
  
  #No match - need to match with larger aggregation
  no.match  <- month.si[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match, YEAR, NESPP3)
  setkeyv(k.month.si, key(no.match))
  month.si.2 <- k.month.si[no.match]
  no.match.2 <- month.si.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.2[, c('QY', 'HY') := 1]
  no.match.2[, AREA := 0]
  no.match.2[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  
  #Merge all together and proportion catch to known months
  month.si   <- month.si  [!is.na(SPPLIVMT), ]
  month.si.2 <- month.si.2[!is.na(SPPLIVMT), ]
  if(nrow(month.si.2) > 0){
    month.si.2[, SIZE   := i.SIZE]
    month.si.2[, i.SIZE := NULL]
    setcolorder(month.si.2, names(month.si))
    month.si <- rbindlist(list(month.si, month.si.2))
    }
   
  month.si[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.si[, unk  := i.SPPLIVMT * prop]
  month.si[, unk2 := i.SPPVALUE * prop]
  month.si[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 'i.QY', 
               'i.UTILCD', 'prop') := NULL]
  setnames(month.si, c('unk', 'unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.si[, AREA := 0]
  month.si[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  
  setcolorder(month.si,  names(month.solved))
  setcolorder(no.match.2, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.si, no.match.2))
  rm(list = c(ls(pattern = 'month.si'), ls(pattern = 'no.match')))
  
  #3.A.6 - GEAR
  match.key <- c('YEAR', 'NESPP3', 'GEAR')
  
  unk.month.g <- unk.month[GEAR != 'unknown']
  unk.month.g <- unk.month.g[SIZE == 'unknown', ]
  unk.month.g <- unk.month.g[AREA == 0, ]
  unk.month.g <- unk.month.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.g, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.g <- k.month[GEAR != 'unknown', ]
  k.month.g <- k.month.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                         by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.g, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.g, match.key)
  setkeyv(k.month.g,   match.key)
  
  month.g <- k.month.g[unk.month.g]
  
  #No match - need to match with larger aggregation
  no.match  <- month.g[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match, YEAR, NESPP3)
  setkeyv(k.month.g, key(no.match))
  month.g.2 <- k.month.g[no.match]
  no.match.2 <- month.g.2[is.na(SPPLIVMT), ]
  no.match.2[, c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.GEAR', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.2[, c('QY', 'HY') := 1]
  no.match.2[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  no.match.2[, AREA := 0]
  
  #Merge all together and proportion catch to known months
  month.g   <- month.g  [!is.na(SPPLIVMT), ]
  month.g.2 <- month.g.2[!is.na(SPPLIVMT), ]
  if(nrow(month.g.2) > 0){
    month.g.2[, GEAR   := i.GEAR]
    month.g.2[, i.GEAR := NULL]
    setcolorder(month.g.2, names(month.g))
    month.g <- rbindlist(list(month.g, month.g.2))
    }
  
  month.g[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.g[, unk  := i.SPPLIVMT * prop]
  month.g[, unk2 := i.SPPVALUE * prop]
  month.g[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 'i.QY', 
              'i.UTILCD', 'prop') := NULL]
  setnames(month.g, c('unk', 'unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.g[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  month.g[, AREA := 0]
  
  setcolorder(month.g,  names(month.solved))
  setcolorder(no.match.2, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.g, no.match.2))
  rm(list = c(ls(pattern = 'month.g'), ls(pattern = 'no.match')))
  
  #3.A.7 - AREA
  match.key <- c('YEAR', 'NESPP3', 'AREA')
  
  unk.month.a <- unk.month[GEAR == 'unknown']
  unk.month.a <- unk.month.a[SIZE == 'unknown', ]
  unk.month.a <- unk.month.a[AREA != 0, ]
  unk.month.a <- unk.month.a[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.a, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.a <- k.month[AREA != 0, ]
  k.month.a <- k.month.a[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                         by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.a, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.a, match.key)
  setkeyv(k.month.a,   match.key)
  
  month.a <- k.month.a[unk.month.a]
  
  #No match - need to match with larger aggregation
  no.match  <- month.a[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match, YEAR, NESPP3)
  setkeyv(k.month.a, key(no.match))
  month.a.2 <- k.month.a[no.match]
  no.match.2 <- month.a.2[is.na(SPPLIVMT), ]
  no.match.2[, c('AREA', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.AREA', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('AREA', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to first QY/HY
  no.match.2[, c('QY', 'HY') := 1]
  no.match.2[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  no.match.2[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  
  #Merge all together and proportion catch to known months
  month.a   <- month.a  [!is.na(SPPLIVMT), ]
  month.a.2 <- month.a.2[!is.na(SPPLIVMT), ]
  if(nrow(month.a.2) > 0){
    month.a.2[, AREA   := i.AREA]
    month.a.2[, i.AREA := NULL]
    setcolorder(month.a.2, names(month.a))
    month.a <- rbindlist(list(month.a, month.a.2))
    }
  
  month.a[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.a[, unk  := i.SPPLIVMT * prop]
  month.a[, unk2 := i.SPPVALUE * prop]
  month.a[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 
              'i.QY', 'i.UTILCD', 'prop') := NULL]
  setnames(month.a, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.a[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  month.a[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  
  setcolorder(month.a,  names(month.solved))
  setcolorder(no.match.2, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.a, no.match.2))
  rm(list = c(ls(pattern = 'month.a'), ls(pattern = 'no.match')))
    
  #3.A.8 - Species only - no other match
  match.key <- c('YEAR', 'NESPP3')
  
  unk.month.sp <- unk.month[GEAR == 'unknown']
  unk.month.sp <- unk.month.sp[SIZE == 'unknown', ]
  unk.month.sp <- unk.month.sp[AREA == 0, ]
  unk.month.sp <- unk.month.sp[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                               by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(unk.month.sp, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  k.month.sp <- k.month[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                        by = c(match.key, 'QY', 'HY', 'UTILCD')]
  setnames(k.month.sp, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.month.sp, match.key)
  setkeyv(k.month.sp,   match.key)
  
  month.sp <- k.month.sp[unk.month.sp]
  
  #No match - assign to first QY/HY
  no.match  <- month.sp[is.na(SPPLIVMT), ]
  no.match[, c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  no.match[, c('QY', 'HY') := 1]
  no.match[, AREA := 0]
  no.match[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  no.match[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  
  #proportion catch to known months
  month.sp   <- month.sp  [!is.na(SPPLIVMT), ]
  
  month.sp[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  month.sp[, unk  := i.SPPLIVMT * prop]
  month.sp[, unk2 := i.SPPVALUE * prop]
  month.sp[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.HY', 'i.QY', 
               'i.UTILCD', 'prop') := NULL]
  setnames(month.sp, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  month.sp[, AREA := 0]
  month.sp[, GEAR := factor('unknown', levels = levels(k.month[, GEAR]))]
  month.sp[, SIZE := factor('unknown', levels = c('large', 'small', 'unknown'))]
  
  setcolorder(month.sp,  names(month.solved))
  setcolorder(no.match, names(month.solved))
  month.solved <- rbindlist(list(month.solved, month.sp, no.match))
  rm(list = c(ls(pattern = 'month.sp'), ls(pattern = 'no.match')))
  
  #Merge back month.solved
  setcolorder(month.solved, names(comland.agg))
  comland.agg <- rbindlist(list(k.month, month.solved))
  setkey(comland.agg,
         YEAR,
         QY,
         HY,
         SIZE,
         GEAR,
         AREA,
         NESPP3,
         UTILCD)
  comland.agg <- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = key(comland.agg)]
  setnames(comland.agg, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))

#3.B SIZE------------------------------------------------------------------------------
  unk.size <- comland.agg[SIZE == 'unknown', ]
  k.size   <- comland.agg[SIZE != 'unknown', ]
  
  #3.B.1 - All match
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY', 'GEAR', 'AREA')
  
  unk.size.all <- unk.size[GEAR != 'unknown']
  unk.size.all <- unk.size.all[AREA != 0, ]
  
  k.size.all <- k.size[GEAR != 'unknown', ]
  k.size.all <- k.size.all[AREA != 0, ]
  
  setkeyv(unk.size.all, match.key)
  setkeyv(k.size.all,   match.key)
  
  size.all <- k.size.all[unk.size.all]
  
  #No match - need to match with larger aggregation
  no.match  <- size.all[is.na(SPPLIVMT), ]
  no.match[, c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.SIZE', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY, GEAR, AREA)
  setkeyv(k.size.all, key(no.match))
  size.all.2 <- k.size.all[no.match]
  no.match.2 <- size.all.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3, GEAR, AREA)
  setkeyv(k.size.all, key(no.match.2))
  size.all.3 <- k.size.all[no.match.2]
  no.match.3 <- size.all.3[is.na(SPPLIVMT), ]
  no.match.3[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match.3, YEAR, NESPP3, AREA)
  setkeyv(k.size.all, key(no.match.3))
  size.all.4 <- k.size.all[no.match.3]
  no.match.4 <- size.all.4[is.na(SPPLIVMT), ]
  no.match.4[, c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('GEAR',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match.4, YEAR, NESPP3)
  setkeyv(k.size.all, key(no.match.4))
  size.all.5 <- k.size.all[no.match.4]
  no.match.5 <- size.all.5[is.na(SPPLIVMT), ]
  no.match.5[, c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.5, c('i.AREA', 'i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA',   'GEAR',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to SIZE to small
  no.match.5[, SIZE := factor('small', levels = c('large', 'small', 'unknown'))]
  
  #Merge all together and proportion catch to known sizes
  size.all   <- size.all  [!is.na(SPPLIVMT), ]
  size.all.2 <- size.all.2[!is.na(SPPLIVMT), ]
  size.all.2[, QY   := i.QY]
  size.all.2[, i.QY := NULL]
  setcolorder(size.all.2, names(size.all))
  size.all.3 <- size.all.3[!is.na(SPPLIVMT), ]
  size.all.3[, QY   := i.QY]
  size.all.3[, HY   := i.HY]
  size.all.3[, i.QY := NULL]
  size.all.3[, i.HY := NULL]
  setcolorder(size.all.3, names(size.all))
  size.all.4 <- size.all.4[!is.na(SPPLIVMT), ]
  size.all.4[, QY     := i.QY]
  size.all.4[, HY     := i.HY]
  size.all.4[, GEAR   := i.GEAR]
  size.all.4[, i.QY   := NULL]
  size.all.4[, i.HY   := NULL]
  size.all.4[, i.GEAR := NULL]
  setcolorder(size.all.4, names(size.all))
  size.all.5 <- size.all.5[!is.na(SPPLIVMT), ]
  size.all.5[, QY     := i.QY]
  size.all.5[, HY     := i.HY]
  size.all.5[, GEAR   := i.GEAR]
  size.all.5[, AREA   := i.AREA]
  size.all.5[, i.QY   := NULL]
  size.all.5[, i.HY   := NULL]
  size.all.5[, i.GEAR := NULL]
  size.all.5[, i.AREA := NULL]
  setcolorder(size.all.5, names(size.all))
  
  size.all <- rbindlist(list(size.all, size.all.2, size.all.3, 
                             size.all.4, size.all.5))
  
  size.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  size.all[, unk  := i.SPPLIVMT * prop]
  size.all[, unk2 := i.SPPVALUE * prop]
  size.all[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.SIZE', 
               'i.UTILCD', 'prop') := NULL]
  setnames(size.all, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setcolorder(no.match.5, names(size.all))
  size.solved <- rbindlist(list(size.all, no.match.5))
  rm(list = c(ls(pattern = 'size.all'), ls(pattern = 'no.match')))
  
  #3.B.2 - GEAR
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY', 'GEAR')
  
  unk.size.g <- unk.size[GEAR != 'unknown']
  unk.size.g <- unk.size.g[AREA == 0, ]
  unk.size.g[, AREA := NULL]
  
  k.size.g <- k.size[GEAR != 'unknown', ]
  k.size.g <- k.size.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                       by = c(match.key, 'SIZE', 'UTILCD')]
  setnames(k.size.g, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.size.g, match.key)
  setkeyv(k.size.g,   match.key)
  
  size.g <- k.size.g[unk.size.g]
  
  #No match - need to match with larger aggregation
  no.match  <- size.g[is.na(SPPLIVMT), ]
  no.match[, c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.SIZE', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY, GEAR)
  setkeyv(k.size.g, key(no.match))
  size.g.2 <- k.size.g[no.match]
  no.match.2 <- size.g.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3, GEAR)
  setkeyv(k.size.g, key(no.match.2))
  size.g.3 <- k.size.g[no.match.2]
  no.match.3 <- size.g.3[is.na(SPPLIVMT), ]
  no.match.3[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match.3, YEAR, NESPP3)
  setkeyv(k.size.g, key(no.match.3))
  size.g.4 <- k.size.g[no.match.3]
  no.match.4 <- size.g.4[is.na(SPPLIVMT), ]
  no.match.4[, c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('GEAR',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to SIZE to small
  no.match.4[, SIZE := factor('small', levels = c('large', 'small', 'unknown'))]
  no.match.4[, AREA := 0]
  
  #Merge all together and proportion catch to known sizes
  size.g   <- size.g  [!is.na(SPPLIVMT), ]
  size.g.2 <- size.g.2[!is.na(SPPLIVMT), ]
  size.g.2[, QY   := i.QY]
  size.g.2[, i.QY := NULL]
  setcolorder(size.g.2, names(size.g))
  size.g.3 <- size.g.3[!is.na(SPPLIVMT), ]
  size.g.3[, QY   := i.QY]
  size.g.3[, HY   := i.HY]
  size.g.3[, i.QY := NULL]
  size.g.3[, i.HY := NULL]
  setcolorder(size.g.3, names(size.g))
  size.g.4 <- size.g.4[!is.na(SPPLIVMT), ]
  size.g.4[, QY     := i.QY]
  size.g.4[, HY     := i.HY]
  size.g.4[, GEAR   := i.GEAR]
  size.g.4[, i.QY   := NULL]
  size.g.4[, i.HY   := NULL]
  size.g.4[, i.GEAR := NULL]
  setcolorder(size.g.4, names(size.g))
  
  size.g <- rbindlist(list(size.g, size.g.2, size.g.3, size.g.4))
  
  size.g[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  size.g[, unk  := i.SPPLIVMT * prop]
  size.g[, unk2 := i.SPPVALUE * prop]
  size.g[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.SIZE', 
             'i.UTILCD', 'prop') := NULL]
  setnames(size.g, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  size.g[, AREA := 0]
  
  setcolorder(size.g,     names(size.solved))
  setcolorder(no.match.4, names(size.g))
  size.solved <- rbindlist(list(size.solved, size.g, no.match.4))
  rm(list = c(ls(pattern = 'size.g'), ls(pattern = 'no.match')))         

  #3.B.3 - AREA
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY', 'AREA')
  
  unk.size.a <- unk.size[GEAR == 'unknown']
  unk.size.a <- unk.size.a[AREA != 0, ]
  unk.size.a[, GEAR := NULL]
  
  k.size.a <- k.size[AREA != 0, ]
  k.size.a <- k.size.a[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                       by = c(match.key, 'SIZE', 'UTILCD')]
  setnames(k.size.a, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.size.a, match.key)
  setkeyv(k.size.a,   match.key)
  
  size.a <- k.size.a[unk.size.a]
  
  #No match - need to match with larger aggregation
  no.match  <- size.a[is.na(SPPLIVMT), ]
  no.match[, c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.SIZE', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY, AREA)
  setkeyv(k.size.a, key(no.match))
  size.a.2 <- k.size.a[no.match]
  no.match.2 <- size.a.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3, AREA)
  setkeyv(k.size.a, key(no.match.2))
  size.a.3 <- k.size.a[no.match.2]
  no.match.3 <- size.a.3[is.na(SPPLIVMT), ]
  no.match.3[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match.3, YEAR, NESPP3)
  setkeyv(k.size.a, key(no.match.3))
  size.a.4 <- k.size.a[no.match.3]
  no.match.4 <- size.a.4[is.na(SPPLIVMT), ]
  no.match.4[, c('AREA', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.AREA', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to SIZE to small
  no.match.4[, SIZE := factor('small',   levels = c('large', 'small', 'unknown'))]
  no.match.4[, GEAR := factor('unknown', levels = levels(k.size[, GEAR]))]
  
  #Merge all together and proportion catch to known sizes
  size.a   <- size.a  [!is.na(SPPLIVMT), ]
  size.a.2 <- size.a.2[!is.na(SPPLIVMT), ]
  size.a.2[, QY   := i.QY]
  size.a.2[, i.QY := NULL]
  setcolorder(size.a.2, names(size.a))
  size.a.3 <- size.a.3[!is.na(SPPLIVMT), ]
  size.a.3[, QY     := i.QY]
  size.a.3[, HY     := i.HY]
  size.a.3[, i.QY := NULL]
  size.a.3[, i.HY := NULL]
  setcolorder(size.a.3, names(size.a))
  size.a.4 <- size.a.4[!is.na(SPPLIVMT), ]
  size.a.4[, QY     := i.QY]
  size.a.4[, HY     := i.HY]
  size.a.4[, AREA   := i.AREA]
  size.a.4[, i.QY   := NULL]
  size.a.4[, i.HY   := NULL]
  size.a.4[, i.AREA := NULL]
  setcolorder(size.a.4, names(size.a))
  
  size.a <- rbindlist(list(size.a, size.a.2, size.a.3, size.a.4))
  
  size.a[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  size.a[, unk  := i.SPPLIVMT * prop]
  size.a[, unk2 := i.SPPVALUE * prop]
  size.a[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.SIZE', 
             'i.UTILCD', 'prop') := NULL]
  setnames(size.a, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  size.a[, GEAR := factor('unknown', levels = levels(k.size[, GEAR]))]
  
  setcolorder(size.a,     names(size.solved))
  setcolorder(no.match.4, names(size.a))
  size.solved <- rbindlist(list(size.solved, size.a, no.match.4))
  rm(list = c(ls(pattern = 'size.a'), ls(pattern = 'no.match')))         
  
  #3.B.4 - Species only - no other match
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY')
  
  unk.size.sp <- unk.size[GEAR == 'unknown']
  unk.size.sp <- unk.size.sp[SIZE == 'unknown', ]
  unk.size.sp <- unk.size.sp[AREA == 0, ]
  unk.size.sp[, c('GEAR', 'AREA') := NULL]
  
  k.size.sp <- k.size[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                      by = c(match.key, 'SIZE', 'UTILCD')]
  setnames(k.size.sp, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.size.sp, match.key)
  setkeyv(k.size.sp,   match.key)
  
  size.sp <- k.size.sp[unk.size.sp]
  
  #No match - need to match with larger aggregation
  no.match  <- size.sp[is.na(SPPLIVMT), ]
  no.match[, c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.SIZE', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY)
  setkeyv(k.size.sp, key(no.match))
  size.sp.2 <- k.size.sp[no.match]
  no.match.2 <- size.sp.2[is.na(SPPLIVMT), ]
  no.match.2[, c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.SIZE', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('SIZE', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3)
  setkeyv(k.size.sp, key(no.match.2))
  size.sp.3 <- k.size.sp[no.match.2]
  no.match.3 <- size.sp.3[is.na(SPPLIVMT), ]
  no.match.3[, c('SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to SIZE to small
  no.match.3[, SIZE := factor('small', levels = c('large', 'small', 'unknown'))]
  no.match.3[, GEAR := factor('unknown', levels = levels(k.size[, GEAR]))]
  no.match.3[, AREA := 0]
  
  #Merge together and proportion catch to known sizes
  size.sp   <- size.sp  [!is.na(SPPLIVMT), ]
  size.sp.2 <- size.sp.2[!is.na(SPPLIVMT), ]
  size.sp.2[, QY   := i.QY]
  size.sp.2[, i.QY := NULL]
  setcolorder(size.sp.2, names(size.sp))
  size.sp.3 <- size.sp.3[!is.na(SPPLIVMT), ]
  size.sp.3[, QY     := i.QY]
  size.sp.3[, HY     := i.HY]
  size.sp.3[, i.QY := NULL]
  size.sp.3[, i.HY := NULL]
  setcolorder(size.sp.3, names(size.sp))
  
  size.sp <- rbindlist(list(size.sp, size.sp.2, size.sp.3))
  
  size.sp[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  size.sp[, unk  := i.SPPLIVMT * prop]
  size.sp[, unk2 := i.SPPVALUE * prop]
  size.sp[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.SIZE', 
              'i.UTILCD', 'prop') := NULL]
  setnames(size.sp, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  size.sp[, AREA := 0]
  size.sp[, GEAR := factor('unknown', levels = levels(k.size[, GEAR]))]
  
  setcolorder(size.sp,    names(size.solved))
  setcolorder(no.match.3, names(size.solved))
  size.solved <- rbindlist(list(size.solved, size.sp, no.match.3))
  rm(list = c(ls(pattern = 'size.sp'), ls(pattern = 'no.match')))
  
  #Merge back size.solved
  setcolorder(size.solved, names(comland.agg))
  comland.agg <- rbindlist(list(k.size, size.solved))
  setkey(comland.agg,
         YEAR,
         QY,
         HY,
         SIZE,
         GEAR,
         AREA,
         NESPP3,
         UTILCD)
  comland.agg <- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = key(comland.agg)]
  setnames(comland.agg, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
#3.C GEAR------------------------------------------------------------------------------
  unk.gear <- comland.agg[GEAR == 'unknown', ]
  k.gear   <- comland.agg[GEAR != 'unknown', ]
  
  #3.C.1 - All match
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY', 'SIZE', 'AREA')
  
  unk.gear.all <- unk.gear[AREA != 0, ]
  
  k.gear.all <- k.gear[AREA != 0, ]
  
  setkeyv(unk.gear.all, match.key)
  setkeyv(k.gear.all,   match.key)
  
  gear.all <- k.gear.all[unk.gear.all]
  
  #No match - need to match with larger aggregation
  no.match  <- gear.all[is.na(SPPLIVMT), ]
  no.match[, c('GEAR', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.GEAR', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY, SIZE, AREA)
  setkeyv(k.gear.all, key(no.match))
  gear.all.2 <- k.gear.all[no.match]
  no.match.2 <- gear.all.2[is.na(SPPLIVMT), ]
  no.match.2[, c('GEAR', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.GEAR', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3, SIZE, AREA)
  setkeyv(k.gear.all, key(no.match.2))
  gear.all.3 <- k.gear.all[no.match.2]
  no.match.3 <- gear.all.3[is.na(SPPLIVMT), ]
  no.match.3[, c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.GEAR', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match.3, YEAR, NESPP3, AREA)
  setkeyv(k.gear.all, key(no.match.3))
  gear.all.4 <- k.gear.all[no.match.3]
  no.match.4 <- gear.all.4[is.na(SPPLIVMT), ]
  no.match.4[, c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('GEAR',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Drop AREA
  setkey(no.match.4, YEAR, NESPP3)
  setkeyv(k.gear.all, key(no.match.4))
  gear.all.5 <- k.gear.all[no.match.4]
  no.match.5 <- gear.all.5[is.na(SPPLIVMT), ]
  no.match.5[, c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.5, c('i.AREA', 'i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD',
                         'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to GEAR to other
  no.match.5[, GEAR := factor('other', levels = levels(k.gear[, GEAR]))]
  
  #Merge all together and proportion catch to known gears
  gear.all   <- gear.all  [!is.na(SPPLIVMT), ]
  gear.all.2 <- gear.all.2[!is.na(SPPLIVMT), ]
  gear.all.2[, QY   := i.QY]
  gear.all.2[, i.QY := NULL]
  setcolorder(gear.all.2, names(gear.all))
  gear.all.3 <- gear.all.3[!is.na(SPPLIVMT), ]
  gear.all.3[, QY   := i.QY]
  gear.all.3[, HY   := i.HY]
  gear.all.3[, i.QY := NULL]
  gear.all.3[, i.HY := NULL]
  setcolorder(gear.all.3, names(gear.all))
  gear.all.4 <- gear.all.4[!is.na(SPPLIVMT), ]
  gear.all.4[, QY     := i.QY]
  gear.all.4[, HY     := i.HY]
  gear.all.4[, SIZE   := i.SIZE]
  gear.all.4[, i.QY   := NULL]
  gear.all.4[, i.HY   := NULL]
  gear.all.4[, i.SIZE := NULL]
  setcolorder(gear.all.4, names(gear.all))
  gear.all.5 <- gear.all.5[!is.na(SPPLIVMT), ]
  gear.all.5[, QY     := i.QY]
  gear.all.5[, HY     := i.HY]
  gear.all.5[, SIZE   := i.SIZE]
  gear.all.5[, AREA   := i.AREA]
  gear.all.5[, i.QY   := NULL]
  gear.all.5[, i.HY   := NULL]
  gear.all.5[, i.SIZE := NULL]
  gear.all.5[, i.AREA := NULL]
  setcolorder(gear.all.5, names(gear.all))
  
  gear.all <- rbindlist(list(gear.all, gear.all.2, gear.all.3, 
                             gear.all.4, gear.all.5))
  
  gear.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  gear.all[, unk  := i.SPPLIVMT * prop]
  gear.all[, unk2 := i.SPPVALUE * prop]
  gear.all[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.GEAR', 
               'i.UTILCD', 'prop') := NULL]
  setnames(gear.all, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setcolorder(no.match.5, names(gear.all))
  gear.solved <- rbindlist(list(gear.all, no.match.5))
  rm(list = c(ls(pattern = 'gear.all'), ls(pattern = 'no.match')))
  
  #3.C.2 - Species only - no other match
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY', 'SIZE')
  
  unk.gear.sp <- unk.gear[AREA == 0, ]
  unk.gear.sp[, 'AREA' := NULL]
  
  k.gear.sp <- k.gear[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                      by = c(match.key, 'GEAR', 'UTILCD')]
  setnames(k.gear.sp, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setkeyv(unk.gear.sp, match.key)
  setkeyv(k.gear.sp,   match.key)
  
  gear.sp <- k.gear.sp[unk.gear.sp]
  
  #No match - need to match with larger aggregation
  no.match  <- gear.sp[is.na(SPPLIVMT), ]
  no.match[, c('GEAR', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.GEAR', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY, SIZE)
  setkeyv(k.gear.sp, key(no.match))
  gear.sp.2 <- k.gear.sp[no.match]
  no.match.2 <- gear.sp.2[is.na(SPPLIVMT), ]
  no.match.2[, c('GEAR', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.GEAR', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3, SIZE)
  setkeyv(k.gear.sp, key(no.match.2))
  gear.sp.3 <- k.gear.sp[no.match.2]
  no.match.3 <- gear.sp.3[is.na(SPPLIVMT), ]
  no.match.3[, c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.GEAR', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('GEAR', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match.3, YEAR, NESPP3)
  setkeyv(k.gear.sp, key(no.match.3))
  gear.sp.4 <- k.gear.sp[no.match.3]
  no.match.4 <- gear.sp.4[is.na(SPPLIVMT), ]
  no.match.4[, c('GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('GEAR',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Still no match - assign to GEAR to other
  no.match.4[, GEAR := factor('other', levels = levels(k.gear[, GEAR]))]
  no.match.4[, AREA := 0]
  
  #Merge all together and proportion catch to known gears
  gear.sp   <- gear.sp  [!is.na(SPPLIVMT), ]
  gear.sp.2 <- gear.sp.2[!is.na(SPPLIVMT), ]
  gear.sp.2[, QY   := i.QY]
  gear.sp.2[, i.QY := NULL]
  setcolorder(gear.sp.2, names(gear.sp))
  gear.sp.3 <- gear.sp.3[!is.na(SPPLIVMT), ]
  gear.sp.3[, QY     := i.QY]
  gear.sp.3[, HY     := i.HY]
  gear.sp.3[, i.QY := NULL]
  gear.sp.3[, i.HY := NULL]
  setcolorder(gear.sp.3, names(gear.sp))
  gear.sp.4 <- gear.sp.4[!is.na(SPPLIVMT), ]
  gear.sp.4[, QY     := i.QY]
  gear.sp.4[, HY     := i.HY]
  gear.sp.4[, SIZE   := i.SIZE]
  gear.sp.4[, i.QY   := NULL]
  gear.sp.4[, i.HY   := NULL]
  gear.sp.4[, i.SIZE := NULL]
  setcolorder(gear.sp.4, names(gear.sp))
  
  gear.sp <- rbindlist(list(gear.sp, gear.sp.2, gear.sp.3, gear.sp.4))
  
  gear.sp[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  gear.sp[, unk  := i.SPPLIVMT * prop]
  gear.sp[, unk2 := i.SPPVALUE * prop]
  gear.sp[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.GEAR', 
              'i.UTILCD', 'prop') := NULL]
  setnames(gear.sp, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  gear.sp[, AREA := 0]
  
  setcolorder(gear.sp,    names(gear.solved))
  setcolorder(no.match.4, names(gear.solved))
  gear.solved <- rbindlist(list(gear.solved, gear.sp, no.match.4))
  rm(list = c(ls(pattern = 'gear.sp'), ls(pattern = 'no.match')))
  
  #Merge back gear.solved
  setcolorder(gear.solved, names(comland.agg))
  comland.agg <- rbindlist(list(k.gear, gear.solved))
  setkey(comland.agg,
         YEAR,
         QY,
         HY,
         SIZE,
         GEAR,
         AREA,
         NESPP3,
         UTILCD)
  comland.agg <- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = key(comland.agg)]
  setnames(comland.agg, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
    
#3.D AREA------------------------------------------------------------------------------
  unk.area <- comland.agg[AREA == 0, ]
  k.area   <- comland.agg[AREA != 0, ]
  
  #3.C.1 - All match
  match.key <- c('YEAR', 'NESPP3', 'QY', 'HY', 'SIZE', 'GEAR')
  
  unk.area.all <- unk.area
  
  k.area.all <- k.area
  
  setkeyv(unk.area.all, match.key)
  setkeyv(k.area.all,   match.key)
  
  area.all <- k.area.all[unk.area.all]
  
  #No match - need to match with larger aggregation
  no.match  <- area.all[is.na(SPPLIVMT), ]
  no.match[, c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match, c('i.AREA', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop QY
  setkey(no.match, YEAR, NESPP3, HY, SIZE, GEAR)
  setkeyv(k.area.all, key(no.match))
  area.all.2 <- k.area.all[no.match]
  no.match.2 <- area.all.2[is.na(SPPLIVMT), ]
  no.match.2[, c('AREA', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.2, c('i.AREA', 'i.QY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('AREA', 'QY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop HY
  setkey(no.match.2, YEAR, NESPP3, SIZE, GEAR)
  setkeyv(k.area.all, key(no.match.2))
  area.all.3 <- k.area.all[no.match.2]
  no.match.3 <- area.all.3[is.na(SPPLIVMT), ]
  no.match.3[, c('AREA', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.3, c('i.AREA', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('AREA', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Drop SIZE
  setkey(no.match.3, YEAR, NESPP3, GEAR)
  setkeyv(k.area.all, key(no.match.3))
  area.all.4 <- k.area.all[no.match.3]
  no.match.4 <- area.all.4[is.na(SPPLIVMT), ]
  no.match.4[, c('AREA', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.4, c('i.AREA', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA',   'SIZE',   'QY',   'HY',  'UTILCD',  'SPPLIVMT', 'SPPVALUE'))
  #Drop GEAR
  setkey(no.match.4, YEAR, NESPP3)
  setkeyv(k.area.all, key(no.match.4))
  area.all.5 <- k.area.all[no.match.4]
  no.match.5 <- area.all.5[is.na(SPPLIVMT), ]
  no.match.5[, c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.5, c('i.AREA', 'i.GEAR', 'i.SIZE', 'i.QY', 'i.HY', 'i.UTILCD', 
                         'i.SPPLIVMT', 'i.SPPVALUE'), 
                       c('AREA', 'GEAR', 'SIZE', 'QY', 'HY', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #Still no match - use 3 or 5 year window then drop year
  years <- unique(no.match.5[, YEAR], by = key(no.match.5))
  no.match.6 <- c()
  area.all.6 <- c()
  for(i in 1:length(years)){
    #3 year window
    k.area.3y <- comland.agg[AREA != 0 & YEAR %in% (years[i] - 1):(years[i] + 1), ]
    setkey(k.area.3y, NESPP3, AREA)
    k.area.3y <- k.area.3y[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                           by = c(key(k.area.3y), 'UTILCD')]
    setnames(k.area.3y, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
    
    unk.area.3y <- no.match.5[YEAR == years[i], ]
    
    setkey(unk.area.3y, NESPP3)
    setkey(k.area.3y,   NESPP3)
    area.3y <- k.area.3y[unk.area.3y]
    
    no.match.3y <- area.3y[is.na(SPPLIVMT), ]
    no.match.3y[, c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
    setnames(no.match.3y, c('i.AREA', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
             c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
    no.match.6 <- rbindlist(list(no.match.6, no.match.3y))
    area.all.6 <- rbindlist(list(area.all.6, area.3y))
    }

  years <- unique(no.match.6[, YEAR], by = key(no.match.6))  
  no.match.7 <- c()
  area.all.7 <- c()  
  for(i in 1:length(years)){    
    #5 year window
    k.area.5y <- comland.agg[AREA != 0 & YEAR %in% (years[i] - 2):(years[i] + 2), ]
    setkey(k.area.5y, NESPP3, AREA)
    k.area.5y <- k.area.5y[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                           by = c(key(k.area.5y), 'UTILCD')]
    setnames(k.area.5y, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
    
    unk.area.5y <- no.match.6[YEAR == years[i], ]
    
    setkey(unk.area.5y, NESPP3)
    setkey(k.area.5y,   NESPP3)
    area.5y <- k.area.5y[unk.area.5y] 
    
    no.match.5y <- area.5y[is.na(SPPLIVMT), ]
    no.match.5y[, c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
    setnames(no.match.5y, c('i.AREA', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
             c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
    
    no.match.7 <- rbindlist(list(no.match.7, no.match.5y))
    area.all.7 <- rbindlist(list(area.all.7, area.5y))
    }   
  #Drop year
  setkey(no.match.7, NESPP3)
  k.area.all <- k.area.all[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                           by = c('NESPP3', 'AREA', 'UTILCD')]
  setnames(k.area.all, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  setkey(k.area.all, NESPP3)
  
  area.all.8 <- k.area.all[no.match.7]
  no.match.8 <- area.all.8[is.na(SPPLIVMT), ]
  no.match.8[, c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE') := NULL]
  setnames(no.match.8, c('i.AREA', 'i.UTILCD', 'i.SPPLIVMT', 'i.SPPVALUE'), 
           c('AREA', 'UTILCD', 'SPPLIVMT', 'SPPVALUE'))
  #If still no match - leave as unknown

  
  #Merge all together and proportion catch to known areas
  area.all   <- area.all  [!is.na(SPPLIVMT), ]
  area.all.2 <- area.all.2[!is.na(SPPLIVMT), ]
  area.all.2[, QY   := i.QY]
  area.all.2[, i.QY := NULL]
  setcolorder(area.all.2, names(area.all))
  area.all.3 <- area.all.3[!is.na(SPPLIVMT), ]
  area.all.3[, QY   := i.QY]
  area.all.3[, HY   := i.HY]
  area.all.3[, i.QY := NULL]
  area.all.3[, i.HY := NULL]
  setcolorder(area.all.3, names(area.all))
  area.all.4 <- area.all.4[!is.na(SPPLIVMT), ]
  area.all.4[, QY     := i.QY]
  area.all.4[, HY     := i.HY]
  area.all.4[, SIZE   := i.SIZE]
  area.all.4[, i.QY   := NULL]
  area.all.4[, i.HY   := NULL]
  area.all.4[, i.SIZE := NULL]
  setcolorder(area.all.4, names(area.all))
  area.all.5 <- area.all.5[!is.na(SPPLIVMT), ]
  area.all.5[, QY     := i.QY]
  area.all.5[, HY     := i.HY]
  area.all.5[, SIZE   := i.SIZE]
  area.all.5[, GEAR   := i.GEAR]
  area.all.5[, i.QY   := NULL]
  area.all.5[, i.HY   := NULL]
  area.all.5[, i.SIZE := NULL]
  area.all.5[, i.GEAR := NULL]
  setcolorder(area.all.5, names(area.all))
  area.all.6 <- area.all.6[!is.na(SPPLIVMT), ]
  setcolorder(area.all.6, names(area.all))
  area.all.7 <- area.all.7[!is.na(SPPLIVMT), ]
  setcolorder(area.all.7, names(area.all))
  area.all.8 <- area.all.8[!is.na(SPPLIVMT), ]
  setcolorder(area.all.8, names(area.all))
  
  area.all <- rbindlist(list(area.all,   area.all.2, area.all.3, area.all.4, 
                             area.all.5, area.all.6, area.all.7, area.all.8))
  
  area.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key]
  area.all[, unk  := i.SPPLIVMT * prop]
  area.all[, unk2 := i.SPPVALUE * prop]
  area.all[, c('SPPLIVMT', 'SPPVALUE', 'i.SPPLIVMT', 'i.SPPVALUE', 'i.AREA', 
               'i.UTILCD', 'prop') := NULL]
  setnames(area.all, c('unk','unk2'), c('SPPLIVMT', 'SPPVALUE'))
  
  setcolorder(no.match.8, names(area.all))
  area.solved <- rbindlist(list(area.all, no.match.8))
  rm(list = c(ls(pattern = 'area.all'), ls(pattern = 'no.match')))
  
  #Merge back area.solved
  setcolorder(area.solved, names(comland.agg))
  comland.agg <- rbindlist(list(k.area, area.solved))
  setkey(comland.agg,
         YEAR,
         QY,
         HY,
         SIZE,
         GEAR,
         AREA,
         NESPP3,
         UTILCD)
  comland.agg <- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), 
                             by = key(comland.agg)]
  setnames(comland.agg, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))  

#-------------------------------------------------------------------------------
if(sum.by == 'EPU'){
  #Assign EPU based on statarea
  gom<-c(500, 510, 512:515)
  gb<-c(521:526, 551, 552, 561, 562)
  mab<-c(537, 539, 600, 612:616, 621, 622, 625, 626, 631, 632)
  ss<-c(463:467, 511)

  comland.agg[AREA %in% gom, EPU := 'GOM']
  comland.agg[AREA %in% gb,  EPU := 'GB']
  comland.agg[AREA %in% mab, EPU := 'MAB']
  comland.agg[AREA %in% ss,  EPU := 'SS']
  comland.agg[is.na(EPU),    EPU := 'OTHER']
  comland.agg[, EPU := factor(EPU, levels = c('GOM', 'GB', 'MAB', 'SS', 'OTHER'))]
 
  setkey(comland.agg,
         YEAR,
         NESPP3,
         QY,
         GEAR,
         SIZE,
         EPU,
         UTILCD)

  comland.agg <- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland.agg)]
            
  setnames(comland.agg, c('V1', 'V2'), c('SPPLIVMT', 'SPPVALUE'))
  
  #Note - NAFO landings by division only so not available in sum.by = "stat.area"
  #Add NAFO foreign landings - Data from http://www.nafo.int/data/frames/data.html
  temp <- tempfile()
  download.file("https://www.nafo.int/Portals/0/Stats/nafo-21b-60-69.zip?ver=2016-08-03-063915-850",temp)
  nafo.60 <- as.data.table(read.csv(unz(temp, "NAFO21B-60-69.txt")))
  unlink(temp)
  download.file("https://www.nafo.int/Portals/0/Stats/nafo-21b-70-79.zip?ver=2016-08-03-063915-850",temp)
  nafo.70 <- as.data.table(read.csv(unz(temp, "NAFO21B-70-79.txt")))
  unlink(temp)
  download.file("https://www.nafo.int/Portals/0/Stats/nafo-21b-80-89.zip?ver=2016-08-03-063915-850",temp)
  nafo.80 <- as.data.table(read.csv(unz(temp, "NAFO21B-80-89.txt")))
  unlink(temp)
  download.file("https://www.nafo.int/Portals/0/Stats/nafo-21b-90-99.zip?ver=2016-08-03-063915-850",temp)
  nafo.90 <- as.data.table(read.csv(unz(temp, "NAFO21B-90-99.txt")))
  unlink(temp)
  download.file("https://www.nafo.int/Portals/0/Stats/nafo-21b-2000-09.zip?ver=2016-08-03-063915-850",temp)
  nafo.00 <- as.data.table(read.csv(unz(temp, "NAFO21B-2000-09.txt")))
  unlink(temp)
  download.file("https://www.nafo.int/Portals/0/Stats/nafo-21b-2010-15.zip?ver=2017-06-01-164323-460",temp)
  nafo.10 <- as.data.table(read.csv(unz(temp, "nafo-21b-2010-15/NAFO21B-2010-15.csv")))
  unlink(temp)
  
  #2010 + data have different column headers
  setnames(nafo.10,
          c('Gear', 'AreaCode', 'SpeciesEffort'),
          c('GearCode', 'Divcode', 'Code'))
          
  nafo <- rbindlist(list(nafo.60, nafo.70, nafo.80, nafo.90, nafo.00, nafo.10))
  
  #Remove US landings (Country code 22), extra divisions (use only 47, 51:56, 61:63),
  #and effort codes (1:3)
  nafo <- nafo[Country != 22 & Divcode %in% c(47, 51:56, 61:63) & Code > 3, ]
  
  #Deal with unknown monthly catch?????
  
  #Get nafo code in a similar format to comland
  nafoland <- nafo[, list(Year, GearCode, Tonnage, Divcode, Code, Catches)]
  nafoland[, MONTH := 0]
  setnames(nafoland, 'Catches', 'SPPLIVMT')
  
  month <- c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')
  for(i in 1:12){
    nafoland.month <- nafo[, list(Year, GearCode, Tonnage, Divcode, Code, get(month[i]))]
    nafoland.month[, MONTH := i]
    setnames(nafoland.month,
            names(nafoland.month)[6],
            'SPPLIVMT')
    nafoland <- rbindlist(list(nafoland, nafoland.month))
    }
  
  nafoland <- nafoland[SPPLIVMT != 0,]
  
  nafoland[, EPU := factor(NA, levels = c('GOM', 'GB', 'MAB', 'SS', 'OTHER'))]
  nafoland[Divcode == 47,             EPU := 'SS']
  nafoland[Divcode == 51,             EPU := 'GOM']
  nafoland[Divcode %in% c(52, 54:56), EPU := 'GB']
  nafoland[Divcode %in% c(53, 61:63), EPU := 'MAB']
  nafoland[is.na(EPU),                EPU := 'OTHER']
  
  nafoland[, Divcode := NULL]
  
  ##Fix missing Scotian Shelf data from 21B
  SS.nafo <- as.data.table(read.csv(file.path(data.dir.3, "SS_NAFO_21A.csv"), skip = 8))
  
  #Add NAFOSPP code to SS.nafo
  nafo.spp <- as.data.table(read.csv(file.path(data.dir.3, 'species.txt')))
  setnames(nafo.spp, "Abbreviation", "Species_ASFIS")
  nafo.spp <- nafo.spp[, list(Code, Species_ASFIS)]
  
  SS.nafo <- merge(SS.nafo, nafo.spp, by = 'Species_ASFIS', all.x = T)
  
  #Only grab missing data
  SS.nafo <- SS.nafo[Year %in% c(2003, 2008, 2009), ]
  
  setkey(SS.nafo,
         Year,
         Code)
  
  SS.land <- SS.nafo[, sum(Catch...000.Kg.), by = key(SS.nafo)]
  
  setnames(SS.land, "V1", "SPPLIVMT")
  
  #Add GearCode, Tonnage, Month, and EPU 
  SS.land[, GearCode := 99]
  SS.land[, Tonnage  := 0]
  SS.land[, MONTH    := 0]
  SS.land[, EPU      := 'SS']
  
  setcolorder(SS.land, names(nafoland))
  
  nafoland <- rbindlist(list(nafoland, SS.land))
  
  #Rectify NAFO codes with US codes
  #Species
  setnames(nafoland,
          c('Year', 'GearCode', 'Tonnage', 'Code'),
          c('YEAR', 'NAFOGEAR', 'TONCL1', 'NAFOSPP'))
  
  spp <- as.data.table(sqlQuery(channel, "select NAFOSPP, NESPP3 from CFSPP"))
  
  #Fix missing NAFO codes
  missing.spp <- data.table(NAFOSPP = c(110, 141, 189, 480, 484, 487, 488, 489),
                            NESPP3  = c(240, 509, 512, 366, 368, 367, 370, 369))
  spp <- rbindlist(list(spp, missing.spp))
  
  setkey(spp, NAFOSPP)
  spp <- unique(spp, by = key(spp))
  
  #Fix many to one relationships
  spp[NAFOSPP == 199, NESPP3 := 524]
  spp[NAFOSPP == 299, NESPP3 := 525]
  spp[NAFOSPP == 469, NESPP3 := 359]
  spp[NAFOSPP == 499, NESPP3 := 526]
  spp[NAFOSPP == 529, NESPP3 := 764]
  spp[NAFOSPP == 699, NESPP3 := 899]
  
  nafoland <- merge(nafoland, spp, by = 'NAFOSPP', all.x = T)
  
  #fix codes
  nafoland[NAFOSPP == 309, NESPP3 := 150L]
  nafoland[NAFOSPP == 462, NESPP3 := 481L]
  nafoland[NAFOSPP == 464, NESPP3 := 355L]
  nafoland[NAFOSPP == 468, NESPP3 := 493L]
  nafoland[NAFOSPP == 704, NESPP3 := 817L]
  
  #remove species without a match
  nafoland <- nafoland[!is.na(NESPP3), ]
  
  #Remove herring catch - already included from Maine Data earlier
  nafoland <- nafoland[NESPP3 != 168, ]
  
  #Gearcodes
  gear <- as.data.table(sqlQuery(channel, "select NEGEAR, NAFOGEAR from Gear"))
  gear <- unique(gear, by = 'NAFOGEAR')
  
  nafoland <- merge(nafoland, gear, by = 'NAFOGEAR', all.x = T)
  
  #fix codes
  nafoland[NAFOGEAR == 8,  NEGEAR := 50L]
  nafoland[NAFOGEAR == 9,  NEGEAR := 370L]
  nafoland[NAFOGEAR == 19, NEGEAR := 58L]
  nafoland[NAFOGEAR == 49, NEGEAR := 60L]
  nafoland[NAFOGEAR == 56, NEGEAR := 21L]
  
  #Tonnage
  nafoland[TONCL1 == 7, TONCL1 := 6L]
  
  #Drop NAFO codes
  nafoland[, c('NAFOGEAR', 'NAFOSPP') := NULL]
  
  #Fix skates
  #get little skates and winter skates from skates(ns) - use survey in half years
  #Generate Half year variable in comland
  nafoland.skates <- nafoland[NESPP3 == 365, ]
  nafoland.skates[MONTH %in% 1:6, Half := 1]
  nafoland.skates[MONTH %in% 7:12, Half := 2]
  
  setkey(skate.hake.nafo,
         YEAR,
         Half,
         EPU)
  
  nafoland.skates <- merge(nafoland.skates, skate.hake.nafo, by = key(skate.hake.nafo), all.x = T)
  
  nafoland.skates[NESPP3 == 365, little := little.per * SPPLIVMT]
  nafoland.skates[is.na(little), little := 0]
  
  nafoland.skates[NESPP3 == 365, winter := winter.per * SPPLIVMT]
  nafoland.skates[is.na(winter), winter := 0]
  
  nafoland.skates[NESPP3 == 365, other.skate := SPPLIVMT - (little + winter)]
  
  #Little (366), winter (367), skates(ns) (365)
  #put skates in nafoland format to merge back
  little <- nafoland.skates[, list(YEAR, Half, EPU, TONCL1, MONTH, 
                                   NESPP3, NEGEAR, little)]
  little[, NESPP3 := 366L]
  setnames(little, "little", "SPPLIVMT")
  little <- little[SPPLIVMT > 0, ]
  
  winter <- nafoland.skates[, list(YEAR, Half, EPU, TONCL1, MONTH, 
                                   NESPP3, NEGEAR, winter)]
  winter[, NESPP3 := 367L]
  setnames(winter, "winter", "SPPLIVMT")
  winter <- winter[SPPLIVMT > 0, ]
  
  other <- nafoland.skates[, list(YEAR, Half, EPU, TONCL1, MONTH, 
                                  NESPP3, NEGEAR, other.skate)]
  other[, NESPP3 := 365L]
  setnames(other, "other.skate", "SPPLIVMT")
  other <- other[SPPLIVMT > 0, ]
  
  #merge all three and reformat for nafoland
  skates.add.back <- rbindlist(list(little, winter, other))
  
  skates.add.back[, Half := NULL]
  setcolorder(skates.add.back, names(nafoland))
  
  nafoland <- rbindlist(list(nafoland[NESPP3 != 365, ], skates.add.back))  
  
  #aggregate nafo landings
  #2 - aggregate by quarter year, half year, major gear, and small/large TC
  nafoland[MONTH %in% 1:3,   QY := 1]
  nafoland[MONTH %in% 4:6,   QY := 2]
  nafoland[MONTH %in% 7:9,   QY := 3]
  nafoland[MONTH %in% 10:12, QY := 4]
  nafoland[MONTH == 0,       QY := 1]
  
  nafoland[NEGEAR %in% otter,     GEAR := 'otter']
  nafoland[NEGEAR %in% dredge.sc, GEAR := 'dredge.sc']
  nafoland[NEGEAR %in% pot,       GEAR := 'pot']
  nafoland[NEGEAR %in% longline,  GEAR := 'longline']
  nafoland[NEGEAR %in% seine,     GEAR := 'seine']
  nafoland[NEGEAR %in% gillnet,   GEAR := 'gillnet']
  nafoland[NEGEAR %in% midwater,  GEAR := 'midwater']
  nafoland[NEGEAR %in% dredge.o,  GEAR := 'dredge.o']
  nafoland[NEGEAR == 99,          GEAR := 'unknown']
  nafoland[is.na(GEAR),           GEAR := 'other']
  nafoland[, GEAR := as.factor(GEAR)]
  
  nafoland[TONCL1 %in% 1:3, SIZE := 'small']
  nafoland[TONCL1 > 3,      SIZE := 'large']
  nafoland[TONCL1 == 0,     SIZE := 'unknown']
  nafoland[, SIZE := as.factor(SIZE)]
  
  setkey(nafoland,
         YEAR,
         QY,
         GEAR,
         SIZE,
         EPU,
         NESPP3)
  
  nafoland.agg <- nafoland[, sum(SPPLIVMT), by = key(nafoland)]
  
  setnames(nafoland.agg, "V1", "SPPLIVMT")
  
  #Create dummy variable for value
  nafoland.agg[, SPPVALUE := 0]
  nafoland.agg[, UTILCD := 0]
  
  #Merge comland and nafoland
  setcolorder(nafoland.agg, names(comland.agg))       
  
  if(foreign == 'y'){
    comland.agg[,  US := T]
    nafoland.agg[, US := F]
  }
  
  comland.nafo <- rbindlist(list(comland.agg, nafoland.agg))
  
  #Remove Menhaden data
  #save(comland.nafo, file = paste(out.dir, "comland_Menhaden.RData", sep = ''))
  comland <- comland.nafo[NESPP3 != 221, ]
}

if(sum.by == 'stat.area') comland <- comland.agg
  
#Output file
if(landed     == 'n') file.landed <- '' else file.landed <- '_meatwt'
if(adjust.ppi == 'n') file.adjust <- '' else file.adjust <- '_deflated'
if(sum.by == 'EPU') file.by <- '' else file.by <- '_stat_areas'
file.name <- paste0('comland', file.landed, file.adjust, file.by, '.RData')

save(comland, file = file.path(out.dir, file.name))
```

### Data analysis

Fisheries dependent data from Comlands is used in several indicators for the State of the Ecosystem report; the more complicated analyses are detailed in their own sections.  The most straightforward use of this data are the aggregate landings indicators.  These are calculated by first assigning the various species into [aggregate groups](#aggroups).  Species are also marked by which management body manages them.  Landings are then summed by year, [EPU](#epu), aggregate group, and whether they are managed or not.  Both managed and unmanaged totals are added together to get the final amount of total landings for that aggregate group within its respective region.  Both the total and those landings managed by the management body receiving the report are reported.  Proportions of managed landings to total landings are also reported in tabular form.

### Plotting

```{r seafood-landings, fig.cap="NEFMC seafood specific landings (red) and total commericial landings (black) in Georges Bank (A: Apex predators, B: Piscivore, C: Planktivore, D: Benthivore, E: Benthos).",echo = T, warning = F,fig.pos='H', message = F, fig.align="center", fig.height=7}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))

## Ecosystem-wide and managed species total landings
opar <- par(mfrow = c(5, 1), mar = c(0, 0, 0, 0), oma = c(4, 6.5, 2, 6))

soe.plot(SOE.data, "Time","Apex Predator Landings GB", stacked = "A",status = F,
         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986,
         ymin = FALSE, y.lower = 0,cex.stacked = 1.5 )

soe.plot(SOE.data, "Time", "Piscivore Landings GB", stacked = "B",status = F,
         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986,
         ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = "Time", 
         y.var2 = "Piscivore NEFMC managed species sea food GB",cex.stacked = 1.5)

soe.plot(SOE.data, "Time", "Planktivore Landings GB", stacked = "C",status = F,
         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986,
         ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = "Time", 
         y.var2 = "Planktivore NEFMC managed species sea food GB",cex.stacked = 1.5)

soe.plot(SOE.data, "Time", "Benthivore Landings GB", stacked = "D",status = F,
         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986,
         ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = "Time", 
         y.var2 = "Benthivore NEFMC managed species sea food GB",cex.stacked = 1.5)

soe.plot(SOE.data, "Time", "Benthos Landings GB", stacked = "E",status = F,
         endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986,
         ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = "Time", 
         y.var2 = "Benthos NEFMC managed species sea food GB",cex.stacked = 1.5)

soe.stacked.axis("Year", expression("Landings, 10"^3*"metric tons"), x.line = 2.7)

```




<!--chapter:end:chapters/landings_data.Rmd-->

# Bennet Indicator

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Bennet Indicator

**Indicator category**: Database pull with analysis

**Contributor(s)**: John Walden
  
**Data steward**: NA
  
**Point of contact**: John Walden <john.walden@noaa.gov>
  
**Public availability statement**: Derived CFDBS data are available for this analysis (see [Comland](#comdat)).
  
## Methods

### Data sources
Data used in the Bennet Indicator were derived from the Comland data set; a processed subset of the Commercial Fisheries Database Biological Sample (CFDBS). The derived Comland data set is available for download [here](https://comet.nefsc.noaa.gov/erddap/tabledap/group_landings_soe_v1.html).

### Data extraction 
For information regarding processing of CFDBS, please see [Comland](#comdat) methods. The Comland dataset containing seafood landings data was subsetted to US landings after 1964 where revenue was &geq;0 for each EPU (i.e. Mid-Atlantic Bight, Georges Bank, and Gulf of Maine). Each EPU was run in an individual R script, and the code specific to Georges Bank is shown below.

```r, echo = T, eval = F, warning=F, message=F}
#This code is used to load and process comland data. See comland methods for source data (CFDBS) processing methods. 

#Packages
PKG <- c("data.table","plyr","RColorBrewer", "ggplot2","cowplot","gridExtra","grid")
for (p in PKG) {
  if(!require(p,character.only = TRUE)) {  
    install.packages(p)
    require(p,character.only = TRUE)}
}
# #Setting Save path


#load "comland" data - These data are unavailble due to PII concerns. See aggregated data load below
ecosys2<-subset(comland, US=='TRUE' & YEAR>=1964 & SPPVALUE >=0)

#Load species and PDT codes
load(file.path(data.dir, "Species_codes.RData"))

#Set EPU
epu <- "GB"

#processing
spp<-subset(spp, NESPP3>0)
spp2<-unique(spp[,c(3,12)], by='NESPP3')
spp2<-spp2[which(!duplicated(spp2$NESPP3)),]
sp_combine<-merge(ecosys2, spp2, by="NESPP3", all.x=TRUE)
add.apex <- data.table(NESPP3 = 000, YEAR = 1971, QY = 1, GEAR = 'other',
                       SIZE = 'small', EPU = epu, UTILCD = 0, SPPLIVMT = 0,
                       SPPVALUE = 0, US = TRUE, Feeding.guild = 'Apex Predator')
sp_combine <- rbindlist(list(sp_combine, add.apex))

#Subset data into Georges Bank group
LANDINGS<-subset(sp_combine)
LANDINGS<-LANDINGS[which(!is.na(LANDINGS$Feeding.guild)),]

#Set Up data Table
landsum<-data.table(LANDINGS)
# setkey(landsum,  "EPU", "YEAR","Feeding.guild")
setkey(landsum,"EPU","YEAR","Feeding.guild")


#Sum by feeding guild
landsum[,lapply(.SD, sum, na.rm=TRUE), by=key(landsum), .SDcols=c("SPPLIVMT","SPPVALUE")]

```


### Data analysis

Revenue earned by harvesting resources from an LME at time *t* is a function of both the quantity landed of each species and the prices paid for landings. Changes in revenue between any two years depends on both prices and quantities in each year, and both may be changing simultaneously. For example, an increase in the harvest of higher priced species, such as scallops can lead to an overall increase in total revenue from an LME between time periods even if quantities landed of other species decline. Although measurement of revenue change is useful, the ability to see what drives revenue change, whether it is changing harvest levels, the mix of species landed, or price changes provides additional valuable information. Therefore, it is useful to decompose revenue change into two parts, one which is due to changing quantities (or volumes), and a second which is due to changing prices. In an LME, the quantity component will yield useful information about how the species mix of harvests are changing through time.

A Bennet indicator (BI) is used to examine revenue change between 1964 and 2015 for two major LME regions. It is composed of a volume indicator (VI), which measures changes in quantities, and a price indicator (PI) which measures changes in prices. The Bennet (1920) indicator (BI) was first used to show how a change in social welfare could be decomposed into a sum of a price and quantity change indicator [@Cross2009]. It is called an indicator because it is based on differences in value between time periods, rather than ratios, which are referred to as indices. The BI is the indicator equivalent of the more popular Fisher index [@Balk2010], and has been used to examine revenue changes in Swedish pharmacies, productivity change in U.S. railroads [@lim2009], and dividend changes in banking operations [@Grifell-Tatje2004].  An attractive feature of the BI is that the overall indicator is equal to the sum of its subcomponents [@Balk2010]. This allows one to examine what component of overall revenue is responsible for change between time periods. This allows us to examine whether changing quantities or prices of separate species groups are driving revenue change in each EPU between 1964 and 2015.

Revenue in a given year for any species group is the product of quantity landed times price, and the sum of revenue from all groups is total revenue from the LME. In any year, both prices and quantities can change from prior years, leading to total revenue change. At time t, revenue (R) is defined as $$R^{t} = \sum_{j=1}^{J}p_{j}^{t}y_{j}^{t},$$
where $p_{j}$ is the price for species group $j$, and $y_{j}$ is the quantity landed of species group $j$. Revenue change between any two time periods, say $t+1$ and $t$, is then $R^{t+1}-R^{t}$, which can also be expressed as:
$$\Delta R = \sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t+1}-\sum_{j=1}^{J}p_{j}^{t}y_{j}^{t}.$$
This change can be decomposed further, yielding a VI and PI. The VI is calculated using the following formula [@Moosberg2007]:

$$VI = \frac{1}{2}(\sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t+1} - \sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t} + \sum_{j=1}^{J}p_{j}^{t}y_{j}^{t+1} - \sum_{j=1}^{J}p_{j}^{t}y_{j}^{t})$$
The price indicator (PI) is calculated as follows:
$$PI = \frac{1}{2}(\sum_{j=1}^{J}y_{j}^{t+1}p_{j}^{t+1} - \sum_{j=1}^{J}y_{j}^{t+1}p_{j}^{t} + \sum_{j=1}^{J}y_{j}^{t}p_{j}^{t+1} - \sum_{j=1}^{J}y_{j}^{t}p_{j}^{t})$$
Total revenue change between time $t$ and $t+1$ is the sum of the VI and PI. Since revenue change is being driven by changes in the individual prices and quantities landed of each species group, changes at the species group level can be examined separately by taking advantage of the additive property of the indicator. For example, if there are five different species groups, the sum of the VI for each group will equal the overall VI, and the sum of the PI for each group will equal the overall PI. 

```{r, echo = T, eval = F, fig.align='center',fig.asp=0.75}
#R code to construct Bennet Indicator for Ecosystem Project
#Author: John Walden
#Date: October 4, 2017
#
#Revised January 18, 2018 to calculate the indicator relative to average conditions
#during each time period. Set EPU in extraction/processing code chunk above.


#filter by specific EPU
epu = "GB"
value <- subset(landsum, EPU == epu)

#Calculate price
value$PRICE=value$SPPVALUE/value$SPPLIVMT
value[is.na(value)]<-0


#Next two lines are to calculate mean values for landings
#and value for the time series by feeding guild

meanval<-as.data.frame(value[,j=list(mean(SPPVALUE,na.rm=TRUE), mean(SPPLIVMT,na.rm=TRUE)), by=Feeding.guild])
meanval<-rename(meanval, c("V1"="BASEV", "V2"="BASEQ"))
meanval$BASEP=meanval$BASEV/meanval$BASEQ;

#order by feeding guild

value<-value[order(value$Feeding.guild),]
meanval<-meanval[order(meanval$Feeding.guild),]

#Merge Value data frame with Base Year Value Data Frame
value<-merge(value, meanval, by="Feeding.guild")

#Construct price and Volume Indicators
#NOTE: ALL values are normalized to $1,000,000

value$VI=((0.5*(value$BASEP+value$PRICE))*(value$SPPLIVMT-value$BASEQ))/1000000
value$PI=((0.5*(value$BASEQ+value$SPPLIVMT))*(value$PRICE-value$BASEP))/1000000

value<-value[order(value$YEAR),]

#The next Data table sets up the yearly aggregate Bennet PI and VI

biyear<-data.table(value)
setkey(biyear, "YEAR")
biyear<-biyear[,lapply(.SD, sum), by=key(biyear), .SDcols=c("VI","PI","BASEV","SPPVALUE")]
biyear$revchange<-(biyear$VI+biyear$PI)
biyear$BI<-(biyear$VI + biyear$PI)

#The Next Steps restructure the year data frame so the yearly
#Bennet Indicator can be plotted. Negative values are difficult in GGPLOT.
#Since the Bennet indicator can have a negative value, separate data frames
#need to be created. First, the data needs to be restructured to use the 
#stacked bar function in ggplot. GGPLOT is used because it can graph differen#t data layers on the same graph.

y1<-biyear[,c(1,2)]
y1$indicator='VI'
y2<-biyear[,c(1,3)]
y2$indicator='PI'

colnames(y1)[2]<-"value"
colnames(y2)[2]<-"value"
ytotal<-rbind(y1,y2)
```

### Plotting

```{r bennet-plot1, message = F, warning=F}

#This section loads the aggregated comland data set to the point where the last chunk left off
#to avoid privacy issues. As such, there are some repeated steps from above. Apologies for mixed #syntax.

#Libraries
library(data.table)
library(plyr)
library(RColorBrewer)
library(ggplot2)
library(cowplot)
library(gridExtra)
library(grid)
library(dplyr)

epu <- "GB"

data.dir <- here::here("data")

load(file.path(data.dir,"SOE_data_erddap.Rdata"))

#Load data for Georges Bank
ytotal <- SOE.data %>% filter(Units == "million USD ($2015)",
                    stringr::str_detect(Var, "by EPU"),
                    EPU == epu)
ytotal$Var <- stringr::str_remove(ytotal$Var, "by EPU")

#Calculate Bennet indicator as the sum of price and volume indicators
#This step was accomplished above when the data.frame "biyear" was created
BI <- ytotal %>%
  group_by(EPU, Time) %>%
  dplyr::summarise(revchange = sum(Value)) 

#Subset by sign
viy1<-subset(ytotal, (stringr::str_detect(ytotal$Var, "VI") & Value>=0))
viy2<-subset(ytotal, (stringr::str_detect(ytotal$Var, "VI") & Value<0))
piy1<-subset(ytotal, (stringr::str_detect(ytotal$Var, "PI") & Value>=0))
piy2<-subset(ytotal, (stringr::str_detect(ytotal$Var, "PI") & Value<0))

#plot
BI<-ggplot()+geom_bar(data=viy1, aes(x=Time, y=Value, fill=Var), stat="identity")+
  geom_bar(data=viy2, aes(x=Time, y=Value, fill=Var), stat="identity")+
  geom_bar(data=piy1, aes(x=Time, y=Value, fill=Var), stat="identity")+
  geom_bar(data=piy2, aes(x=Time, y=Value, fill=Var), stat="identity")+
  ggtitle("Revenue Change ($2015), Price (PI) and Volume Indicator (VI)", subtitle=paste0(epu," EPU 1964-2016 Compared to Average Year")) +
  labs(x="YEAR", y="Value $1,000,000 ($2015)") +
  scale_fill_brewer(name="Indicator",palette = "Set2")+
  geom_line(data=BI, aes(x=Time, y=revchange, colour="$"))+
  scale_colour_grey(name="Revenue Change")+
  theme(plot.title = element_text(hjust=0.3),plot.subtitle=element_text(hjust=0.5))

#The next step creates the separate volume indicators
#The aggregate volume indicator can be broken into smaller
#components. In this case, by Feeding.guild group.  

VI <- SOE.data %>% filter(Units == "million USD ($2015)",
                    !stringr::str_detect(Var, "by EPU"),
                    stringr::str_detect(Var, "VI"),
                    EPU == epu)
VI$Var <- stringr::str_remove(VI$Var,"VI")

#Subset by sign
vi1<-subset(VI, Value >= 0)
vi2<-subset(VI, Value < 0)

#Volume indicator
volind <- VI %>% group_by(Time) %>% dplyr::summarise(revchange = sum(Value))

VI<-ggplot()+geom_bar(data=vi1, aes(x=Time, y=Value, fill=Var), stat="identity")+
  geom_bar(data=vi2, aes(x=Time, y=Value, fill=Var), stat="identity")+
  ggtitle(paste0("Volume Indicator ($2015) Year 2000 Base\n",epu," EPU 1964-2016")) +
  labs(x="YEAR", y="Value $1,000,000 ($2015)") +
  scale_fill_brewer(name= "Feeding Guild", palette="Set1")+
    geom_line(data=volind , aes(x=Time, y=revchange, colour="$"))+
  scale_colour_grey(name="VI/PI")+
  theme(plot.title = element_text(hjust=0.5, size = 10),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 10)) 


#Next, use the same algorithm to create Price Indicator graphs
PI <- SOE.data %>% filter(Units == "million USD ($2015)",
                    !stringr::str_detect(Var, "by EPU"),
                    stringr::str_detect(Var, "PI"),
                    EPU == epu)
PI$Var <- stringr::str_remove(PI$Var,"PI")
pi1<-subset(PI, Value >= 0)
pi2<-subset(PI, Value < 0)
prind <- PI %>% group_by(Time) %>% dplyr::summarise(revchange = sum(Value))

PI<-ggplot()+geom_bar(data=pi1, aes(x=Time, y=Value, fill=Var), stat="identity") + 
  geom_bar(data=pi2, aes(x=Time, y=Value, fill=Var), stat="identity")+
  geom_bar(stat="identity") + 
  ggtitle(paste0("Price Indicator ($2015) Year 2000 Base\n",epu," EPU 1964-2016")) +
  labs(x="YEAR", y="Value $1,000,000 ($2015)") +
  scale_fill_brewer(name= "Feeding Guild", palette="Set1")+
  geom_line(data=prind, aes(x=Time, y=revchange, colour="$"))+
  scale_colour_grey(name="VI/PI")+  
  theme(plot.title = element_text(hjust=0.5, size = 10),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 10)) 

#grid.arrange(VI, PI,BI, nrow = 2,width = 1:2)
r1 <-plot_grid(BI, ncol = 1)
r1
```

```{r plot2, echo = F, eval = T, fig.width=8}
grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1, position = c("bottom", "right")) {
  
  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)
  
  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))
  
  grid.newpage()
  grid.draw(combined)
  
  # return gtable invisibly
  invisible(combined)
  
}
grid_arrange_shared_legend(VI, PI, ncol = 2, nrow = 1)
```





<!--chapter:end:chapters/Bennet_indicator.Rmd-->

# Catch and Fleet Diversity

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Permit-level species diversity and Council-level fleet diversity.

**Indicator category**: Database pull with analysis
Published methods

**Contributor(s)**: Geret DePiper, Min-Yang Lee
  
**Data steward**: Geret DePiper, <geret.depiper@noaa.gov>
  
**Point of contact**: Geret DePiper, <geret.depiper@noaa.gov>
  
**Public availability statement**: Source data is not publicly availabe due to PII restrictions. Derived time series are available for download [here](https://comet.nefsc.noaa.gov/erddap/tabledap/comm_data_soe_v1.html).
  

## Methods
Diversity estimates have been developed to understand whether specialization, or alternatively stovepiping, is occurring in fisheries of the Northeastern Large Marine Ecosystem. We use the average effective Shannon indices for species revenue at the permit level, for all permits landing any amount of [NEFMC](https://www.nefmc.org/) or [MAFMC](http://www.mafmc.org/) Fishery Management Plan (FMP) species within a year (including both Monkfish and Spiny Dogfish). We also use the effective Shannon index of fleet revenue diversity and count of active fleets to assess the extent to which the distribution of fishing changes across fleet segments.

### Data sources
Data for these diversity estimates comes from a variety of sources, including the
Commercial Fishery Dealer Database, Vessel Trip Reports, Clam logbooks, vessel characteristics from Permit database, WPU series producer price index. These data are typically not available to the public.

### Data extraction 
The following describes both the permit-level species and fleet diversity data generation. Price data was extracted from the Commercial Fishery Dealer database (CFDERS) and linked to Vessel Trip Reports by a heirarchical matching algorithm that matched date and port of landing at its highest resolution. Code used in these analyses is available upon request.

<!-- For NOAA personnel: Code currently archived in the \\\\net\\home2\\mlee\\diversity\\code folder, while data is currently archived in \\\\net\\home2\\mlee\\diversity folder. -->

Output data was then matched to vessel characteristics from the VPS VESSEL data set. For the permit-level estimate, species groups are based off of a slightly refined NESPP3 code, defined in the data as "myspp", which is further developed in the script to rectify inconsistencies in the data.

Species groups used include Highly Migratory Species, Monkfish, Atlantic Sea Scallops, Shrimp, Skates, Atlantic Herring, Ocean Quahog, Surf Clam, Tilefish, Black Sea Bass and Fluke, Butterfish and Red Hake and Unknown Whiting, Bluefish, Spiny Dogfish, Illex, American Lobster, Loligo, Menhaden, Offshore hake, Scup, Sand Dabs, Pout, Wolffish, Winter Flounder, Yellowtail Flounder, Unspecified hakes, White hake, Halibut, Bluefish & Scup (NE only), New England Groundfish (cod, pollock, hadddock, Monkfish, Winter flounder, Witch flounder, White hake, Plaice, redfish), Mid-Atlantic Groundfish (cod, wolffish, plaice, Witch flounder, haddock,  pollock, redfish, and halibut), pout and windowpane flounder (MA only), and an "Other" category for all other species. 

For the fleet diversity metric, gears include scallop dredge (gearcodes DRS, DSC, DTC, and DTS), other dredges (gearcodes DRM, DRO, and DRU), gillnet (gearcodes GND, GNT, GNO, GNR, and GNS), hand (gearcode HND), longline (gearcodes LLB and LLP), bottom trawl (gearcodes OTB, OTF, OTO, OTC. OTS, OHS, OTR, OTT, and PTB), midwater trawls (gearcode OTM and PTM), pot (gearcodes PTL, PTW, PTC, PTE, PTF, PTH, PTL, PTO, PTS, and PTX), purse seine (gearcode PUR), and hydraulic clam dredge (gearcode DRC).Vessels were further grouped by length categories of less than 30 feet, 30 to 50 feet, 50 to 75 feet, and 75 feet and above. All revenue was deflated to real dollars using the "WPU0223" Producer Price Index with a base of January 2015. Stata code for data processing is available [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/data/Human_Dimensions_code).

### Data analysis
This permit-level species effective Shannon index is calculated as 
$$exp(-\sum_{i=1}^{N}p_{ijt}ln(p_{ijt}))$$
for all $j$, with $p_{ijt}$ representing the proportion of revenue generated by species or species group $i$ for permit $j$ in year $t$, and is a composite of richness (the number of species landed) and abundance (the revenue generated from each species). The annual arithmetic mean value of the effective Shannon index across permits is used as the indicator of permit-level species diversity. 

In a similar manner, the fleet diversity metric is estimated as 
$$exp(-\sum_{i=1}^{N}p_{kt}ln(p_{kt})) $$
for all $k$, where $p_{kt}$ represents the proportion of total revenue generated by fleet segment $k$ (gear and length combination) per year $t$. The indices each run from 1996 to 2017. A count of the number of fleets active in every year is also provided to assess whether changes in fleet diversity are caused by shifts in abundance (number of fleets), or evenness (concentration of revenue). The work is based off of analysis conducted in @eric_m_thunberg_measures_2015 and published in @gaichas_framework_2016.

```{r fleet-diversity, fig.cap="Fleet diversity (A) and fleet count (B) in the Mid Atlantic Bight.", echo=T, message=FALSE, warning=FALSE, fig.align='center'}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))

opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(4, 6, 2, 6))

soe.plot(SOE.data, "Time", "Mid-Atlantic average fleet diversity", stacked = "A",
         rel.y.num = 0.9, end.start = 2008, tol = 0.15, full.trend = F, cex.stacked = 1.5)
soe.stacked.axis('Year', 'Fleet diversity', y.line = 2.5, outer = F,
                 rel.x.text = 1, rel.y.text = 1)
soe.plot(SOE.data,"Time", "Mid-Atlantic fleet count", stacked = "B",
         rel.y.num = 0.9, end.start = 2008, full.trend = F, cex.stacked = 1.5)

soe.stacked.axis('Year', 'Fleet count', y.line = 2.5, outer = F,
                 rel.x.text = 1, rel.y.text = 0.95)

```

<!--chapter:end:chapters/Catch_and_Fleet_Diversity_indicators.Rmd-->

# Aquaculture

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Aquaculture indicators

**Indicator category**: Synthesis of published information

**Contributor(s)**: Sean Hardison, Lisa Calvo, Karl Roscher
  
**Data steward**: Sean Hardison <sean.hardison@noaa.gov>
  
**Point of contact**: Sean Hardison <sean.hardison@noaa.gov> 
  
**Public availability statement**: Source data are publicly available in referenced reports, and are also available for download [here](https://comet.nefsc.noaa.gov/erddap/tabledap/aquaculture_soe_v1.html). 


## Methods
Aquaculture data included in the SOE report were time series of number of oysters sold in Virginia, Maryland, and New Jersey. 


### Data sources
Virginia oyster harvest data are collected from mail and internet-based surveys of active oyster aquaculture operations on both sides of the Chesapeake Bay, which are then synthesized in an annual report [@Hudson2017a]. In Maryland, shellfish aquaculturists are required to report their monthly harvests to the Maryland Department of Natural Resources (MD-DNR). The MD-DNR then aggregates the harvest data for release in the Maryland Aquaculture Coordinating Council Annual Report [@ACC2017], from which data were collected. Similar to Virginia, New Jersey releases annual reports synthesizing electronic survey results from lease-holding shellfish growers. Data from New Jersey reflects cage reared oysters grown from hatchery seed [@Calvo2017]. 


### Data extraction 
Data were collected directly from state aquaculture reports. Oyster harvest data in MD was reported in bushels which were then converted to individual oysters by an estimate of 300 oysters bushel$^{-1}$.

### Data analysis
No data analyses occurred for this indicator.

### Plotting

```{r, mariculture, fig.cap="Oyster aquaculture production in terms of number of oysters sold from Virginia, Maryland, and New Jersey.",echo = T, warning = F, message=F, fig.align='center', fig.height=4}

library(dplyr)

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

#colors
topo <- c('black', "indianred", "darkorange")

#filter
oyst_vars <- SOE.data %>%
  dplyr::filter(Var %in%
          c("aquaculture NJ gear raised oysters sold",
            "aquaculture MD oyster harvest N",
            "aquaculture VA oyster harvest N")) %>% 
  tidyr::spread(.,Var,Value)

#plotting
opar <- par(mar = c(4, 6, 2, 6))
y.min <- 1
y.max <- 40e6
time <- oyst_vars$Time
plot(NULL, xlim = c(2005, 2017),
     ylim = c(y.min,y.max), xlab = '', ylab = '', yaxt = "n",ty = 'n',
     cex.lab=1, las = 1, cex.axis = 1)
points(time, oyst_vars$`aquaculture NJ gear raised oysters sold`, type = "o", lwd = 2.5, col = topo[1], pch = 20)
points(time, oyst_vars$`aquaculture MD oyster harvest N`, type = "o", lwd = 2.5, col = topo[2], pch = 20)
points(time, oyst_vars$`aquaculture VA oyster harvest N`, type = "o", lwd = 2.5, col = topo[3], pch = 20)

mtext(1, text = 'Year', line = 2)
mtext(2, text = expression("Oysters Harvested, 10"^6*'n'), line = 2.5)
axis(2, at = c(1*10^6,5*10^6,10*10^6,15*10^6,
                 20*10^6,25*10^6,30*10^6,35*10^6,40*10^6), 
       labels = c("1","5","10","15","20",
                  "25","30","35","40"),
       las = 1, cex.axis = 1)
legend(2006, 40e6, legend = c("New Jersey",
                            "Maryland",
                            "Virginia"), col = topo, bty = "n",lwd = 2)

```





<!--chapter:end:chapters/Aquaculture_indicators.Rmd-->

# New England Harmful Algal Bloom Indicator

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```

**Description**: Regional incidence of shellfish bed closures due to presence of toxins associated with harmful algae. 

**Indicator Category**: Synthesis of published information

**Contributor(s)**: Dave Kulis, Donald M Anderson, Sean Hardison

**Data steward**: Sean Hardison, sean.hardison@noaa.gov

**Point of contact**: Sean Hardison, sean.hardison@noaa.gov

**Public availability statement**: Data are publicly available (see Data Sources below).

## Methods
The New England Harmful Algal Bloom (HAB) indicator is a synthesis of shellfish bed closures related to the presence of HAB-associated toxins above threshold levels from 2007-2016 (Figure \@ref(fig:NE-HAB-image)). Standard detection methods were used to identify the presence of toxins associated with Amnesic Shellfish Poisoning (ASP), Paralytic Shellfish Poisoning (PSP), and Diarrhetic Shellfish Poisoning (DSP) by state and federal laboratories. 

#### Paralytic Shellfish Poisoning
The most common cause of shellfish bed closures in New England is the presence of paralytic shellfish toxins (PSTs) produced by the dinoflagellate *Alexandrium catenella*. All New England states except Maine relied on the AOAC-approved mouse bioassay method to detect PSTs in shellfish during the 2007-2016 period reported here [@Anonymous2005].

In Maine, PST detection methods were updated in May 2014 when the state adopted the hydrophilic interaction liquid chromatography (HILIC) UPLC-MS/MS protocol [@Boundy2015] in concordance with National Shellfish Sanitation Program (NSSP) requirements.  Prior to this, the primary method used to detect PST in Maine was with the mouse bioassay. 

#### Amnesic Shellfish Poisoning
Amnesic shellfish poisoning (ASP) is caused by the toxin domoic acid (DA), which is produced by several phytoplankton species belonging to the genus *Pseudo-nitzchia*. In New England, a UV-HPLC method [@Quilliam1995], which specifies a HPLC-UV protocol, is used for ASP detection. 

#### Diarrhetic Shellfish Poisoning
Diarrhetic Shellfish Poisoning (DSP) is rare in New England waters, but the presence of the DSP-associated okadaic acid (OA) in mussels was confirmed in Massachusetts in 2015 (J. Deeds, personal communication, July 7, 2018). Preliminary testing for OA in Massachusetts utilized the commercially available Protein Phosphatase Inhibition Assay (PPIA) and these results are confirmed through LC-MS/MS when necessary [@Smienk2012; @Stutts2017]. 


### Data sources
<!--Please provide a text description of data sources, inlcuding primary collection methods. What equipment was used to turn signal to data? From which vessel were data collected and how? What quality control procedures were employed, if any?--> 


Data used in this indicator were drawn from the 2017 Report on the ICES-IOC Working Group on Harmful Algal Bloom Dynamics (WGHABD). The report and data are available [here](http://www.ices.dk/sites/pub/Publication%20Reports/Expert%20Group%20Report/SSGEPD/2017/01%20WGHABD%20-%20Report%20of%20the%20ICES%20-%20IOC%20Working%20Group%20on%20Harmful%20Algal%20Bloom%20Dynamics.pdf).


Closure information was collated from information provided by the following organizations:
```{r closuresrc, echo = F, include = T, results='asis', message=FALSE, warning=F}
tabl <- data.frame(State = c("Maine","New Hampshire","Massachusetts","Rhode Island","Connecticut"),
                   `Source Organization` = c("Maine Department of Marine Resources",
                                           "New Hampshire Department of Environmental Services",
                                           "Massachusetts Division of Marine Fisheries",
                                           "Rhode Island Department of Environmental Management",
                                           "Connecticut Department of Agriculture"))
names(tabl)[2] <- "Source Organization"
knitr::kable(
  tabl, booktabs = TRUE,
  caption = 'Shellfish closure information providers.'
)

```


### Data extraction
<!--Text overview description of extraction methods. What information was extracted and how was it aggregated? Can point to other indicator extraction methods if the same.-->

Data were extracted from the original report visually and accuracy confirmed with report authors.

```{r, echo = F, eval = F}

 #No code used for extraction

```


### Data analysis
<!--Text description of analysis methods, similar in structure and detail to a peer-reviewed paper methods section-->

No data analysis steps took place for this indicator.

```{r, echo=F, message=FALSE, warning=FALSE, include=T, eval = F}

#SOE R packages

library(Kendall);library(data.table);library(zoo)
library(dplyr);library(nlme);library(AICcmodavg)
library(colorRamps);library(Hmisc);library(rgdal)
library(maps);library(mapdata);library(raster)
library(grid);library(stringr);library(png)
library(ncdf4);library(marmap); library(magick)

```

The script used to develop the figure in the SOE report is below. 

```{r NE-HAB, fig.cap="Regional HAB related shellfish bed closures in New England between 2007 and 2016.", fig.asp=1.1, fig.align='center', message=F, warning=F}
#get map data and set constants
 
# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')
gis.dir <- here::here('gis')

#Source GIS script
source(file.path(r.dir, "GIS_source.R"))

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))


#projection
map.crs <- CRS("+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0
               +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")

#coastline
coast <- readOGR(gis.dir, 'NES_LME_coast', verbose = F)
coast <- spTransform(coast,map.crs)

#define extents for cropping
e1  <- extent(-78.5, -64, 41, 45)

#crop
coast1 <- crop(coast, e1)

#Get data from SOE dataset
events <- SOE.data[grepl("SP occurrence",SOE.data$Var),]
lon <- SOE.data %>%
  dplyr::filter(Var == "NE HAB Regional center Lon") %>% pull(Value)
lat <- SOE.data %>%
  dplyr::filter(Var == "NE HAB Regional center Lat") %>% pull(Value)

events_df <- data.frame(lon = lon,
                        lat = lat,
                        val = events$Value,
                        var = events$Var)


g1 <- events_df %>% filter(val == 8)
#g2 <- events_df %>% filter(val == 3.5)
g3 <- events_df %>% filter(val == 1)

#data.frame to sp object
coordinates(g1) <- ~lon+lat
g1@proj4string <- map.crs
g1 <- spTransform(g1, map.crs)

coordinates(g3) <- ~lon+lat
g3@proj4string <- map.crs
g3 <- spTransform(g3, map.crs)


#plot map and dots of different size based on category
par(mar = c(0,2.8,0,0.2))
plot(coast1, xlim = c(-71.5,-64.5),ylim = c(41,45),col = "grey",yaxs="i")
plot(g1,  add = T,cex = 6, pch = 16, col = "darkorange")
#plot(g2,  add = T,cex = 4, pch = 16, col = "purple")
plot(g3,  add = T,cex = 2, pch = 16, col = c("purple","#56B4E9"))

axis(1, at = c(-71,-69,-67,-65), labels = paste( c(-71,-69,-67,-65) * -1, 'W')
     ,col = NA, col.ticks = 1, pos = 41)
axis(2, at = c(45, 44, 43, 42, 41.05), labels = paste(c(45, 44, 43, 42, 41), 'N')
     , las = T, pos = -71.78,col = NA, col.ticks = 1)
legend(-66.2,42.5, c("PSP", "ASP", "DSP"), col = c("darkorange","purple","#56B4E9"), pch = 16,
       cex = 1.1, bty = "n", pt.cex = 2)
text(-65.65,42.495, "Category")
arrows(-71.765,41,-71.765,45,angle = 90,lwd = 2)
arrows(-64.22,41,-64.22,45,angle = 90,lwd = 2)
abline(h = 41, lwd = 2)
abline(h = 45, lwd = 2)
abline(v = -64, lwd = 2)
legend(-67.9,42.5, c("6-10", "2-5", "1"), col = c("black"), pch = 16, cex = 1.1,
       pt.cex = c(6,4,2), x.intersp = 1.45, y.intersp = 1.75, bty = "n")
text(-67.7,42.525, "2007-2016 Detections")

```



<!--chapter:end:chapters/NE_HABs_indicator.Rmd-->

# Mid-Atlantic Harmful Algal Bloom Indicator

```{r, echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```

**Description**: An aggregation of reported algal bloom data in Chesapeake Bay between 2007-2017.

**Indicator category**: Database pull

**Contributor(s)**: Sean Hardison, Virginia Department of Health

**Data steward**: Sean Hardison, sean.hardison@noaa.gov

**Point of contact**: Sean Hardison, sean.hardison@noaa.gov

**Public availability statement**: Source data for this indicator are available [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/data/CB_HAB). Processed time series can be found [here](http://comet.nefsc.noaa.gov/erddap/tabledap/CBhabs_ann_soe_v2.html).


## Methods
We presented two indicator time series for reports of algal blooms in the southern portion of Chesapeake Bay between 2007-2017. The first indicator was observations of algal blooms above 5000 cell ml^-1^. This threshold was developed by the Virginia Department of Health (VDH) for *Microcystis* spp. algal blooms based on World Health Organization guidelines [@WHO2003; @VDH2011].  VDH also uses this same threshold for other algal species blooms in Virginia waters. When cell concentrations are above 5000 cell ml^-1^, VDH recommends initiation of biweekly water sampling and that relevant local agencies be notified of the elevated cell concentrations.

The second indicator we reported, blooms of *Cochlodinium polykrikoides* at cell concentrations >300 cell ml^-1^, was chosen due to reports of high ichthyotoxicity seen at these levels. @Tang2009 showed that fish exposed to cultured *C. polykrikoides* at densities as low 330 cells ml^-1^ saw 100% mortality within 1 hour, which if often far less than *C. polykrikoides* cell concentrations seen in the field. Algal bloom data were not available for 2015 nor 2010. The algal bloom information presented here are a synthesis of reported events, and has been updated to include data not presented in the 2018 State of the Ecosystem Report. 

### Data source(s)
Source data were obtained from VDH. Sampling, identification, and bloom characterization was completed by the VDH, Phytoplankton Analysis Laboratory at Old Dominion University, Reece Lab at the Virginia Institute of Marine Science, and Virginia Department of Environmental Quality. Problem algal species were targeted for identification via light microscopy followed by standard or quantitative PCR assays and/or enzyme-linked immunosorbent assay (ELISA). Reports specifying full methodologies from ODU, VIMS, and VDH source data are available upon request.

### Data extraction
Data were extracted from a series of spreadsheets provided by the VDH. We quantified the number of algal blooms in each year reaching target cell density thresholds in the southern Chesapeake Bay.

```{r, eval = T, echo=T, message=FALSE, warning=FALSE, include=T}
#SOE R packages

library(readxl)
library(dplyr)
library(tidyr)
library(stringr)
```

```{r r-extract,fig.align = "center", eval = T,fig.cap="All reported algal blooms >5000 cells ml <sup>-1</sup> (black), and reports of <i>C. polykrikoides</i> blooms >300 cells ml <sup>-1</sup> (red) between 2007-2017.", echo = T, eval = T, message=F, warning=F}
data.dir <- "data/CB_HAB"

#Function to process data - cpm specifies cells per ml filter
fixer <- function(cpm){
  hab_2007_2012 <- read_excel(file.path(data.dir,"Query_2007-2012.xlsx"))
  hab_2013_odu <- read_excel(file.path(data.dir,"2013 ODU Data.xlsx"),skip = 4)
  hab_2013_vims <- read_excel(file.path(data.dir,"vims_2013.xlsx"),skip = 6)
  hab_2014_odu <- read_excel(file.path(data.dir,"2014 ODU data.xlsx"))
  hab_2014_vims <- read_excel(file.path(data.dir,"FINALforVDH_22Dec14final.xlsx"))
  hab_2016 <- read_excel(file.path(data.dir,"HAB_MAP_Data_2016.xlsx"))
  hab_2017 <- read_excel(file.path(data.dir,"HAB_MAP_Data_2017.xlsx"),sheet=2)
  
  #2012---------------------------------------------------------
  HAB_2007_2012 <- hab_2007_2012 %>% filter(!is.na(cells_per_ml)) %>%
    filter(!is.na(date)) %>%
    mutate(year = format(as.POSIXct(date), "%Y")) %>% 
    filter(cells_per_ml >= cpm) %>%
    group_by(year, species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  
  #2013---------------------------------------------------------
  #ODU
  odu_2013 <- gather(hab_2013_odu, species, cells_per_ml, `Pfiesteria like dinoflagellate`:`A. monilatum`) %>%
    filter(!is.na(cells_per_ml)) %>%
    filter(cells_per_ml >= cpm) %>%
    mutate(year = 2013) %>%
    group_by(year, species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  
  #VIMS
  vims_2013 <- hab_2013_vims %>% filter(!is.na(cells_per_ml)) %>%
    mutate(year = "2013") %>%
    filter(cells_per_ml >= cpm) %>%
    group_by(year, species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  
  HAB_2013 <- rbind(vims_2013, odu_2013)
  
  #2014--------------------------------------------------------
  #ODU
  long <- gather(hab_2014_odu, species, cells_per_ml, `Karlodinium veneficum`:`Cyanobacteria bloom`, factor_key = TRUE)
  hab_2014_odu <- long %>% filter(cells_per_ml != 0)
  hab_2014_odu$species <- sub("[.]"," ", hab_2014_odu$species)
  hab_2014_odu$cells_per_ml <- gsub("[A-Za-z+//]",'',hab_2014_odu$cells_per_ml)
  hab_2014_odu$cells_per_ml <- as.numeric(hab_2014_odu$cells_per_ml)
  
  hab_2014_odu <- hab_2014_odu %>% mutate(year = "2014") %>%
    filter(cells_per_ml >= cpm) %>%
    group_by(year,species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  
  #VIMS
  long <- gather(hab_2014_vims, species, cells_per_ml, `A. monilatum`:`C. subsalsa`)
  hab_2014_vims <- long %>% mutate(year = "2014") %>% filter(!is.na(cells_per_ml)) %>%
    mutate(cells_per_ml = as.numeric(cells_per_ml)) %>% 
    filter(cells_per_ml >= cpm) %>%
    group_by(year,species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  HAB_2014 <- rbind(hab_2014_odu, hab_2014_vims)
  
  #2015----------------------------------------------------------
  #No data
  
  #2016---------------------------------------------------------
  HAB_2016 <- hab_2016 %>% mutate(species= 
                                    plyr::mapvalues(species, 
                                                    from = c("Eugelna sanguinea",
                                                             "Microcystin aeruginosa",
                                                             "Microcystis aeruginosa",
                                                             "Alexandrium monilatum-likely",
                                                             "Alexandrium monilatum"),
                                                    to = c("Eugelena spp.", 
                                                           "Microcystis spp.",
                                                           "Microcystis spp.",
                                                           "Alexandrium spp.",
                                                           "Alexandrium spp.")))
  HAB_2016$cells_per_ml <- gsub('[a-zA-Z+<>]','',HAB_2016$cells_per_ml)
  HAB_2016 <- HAB_2016 %>%
    filter(!is.na(cells_per_ml)) %>%
    mutate(year = 2016, cells_per_ml = as.numeric(cells_per_ml)) %>%
    filter(cells_per_ml >= cpm) %>%
    group_by(year, species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  
  #2017------------------------------------------------------------
  hab_2017$species = str_trim(hab_2017$species)
  HAB_2017 <- hab_2017 %>% mutate(species = plyr::mapvalues(species, c("A. monilatum","Anabaena sp",
                                                                       "Anabaena sp.","Anabaena spp",
                                                                       "none","NO HABs","C. polykrikoides",
                                                                       "Microcystis aeurignosa","Cylindrospermopsis sp"),
                                                            c("Alexandrium monilatum", "Anabaena spp.",
                                                              "Anabaena spp.","Anabaena spp.",
                                                              "NA","NA","Cochlodinium polykrikoides",
                                                              "Microcystis aeruginosa","Cylindrospermopsis sp.")))
  HAB_2017$cells_per_ml <- gsub("[a-zA-Z+/]",'',HAB_2017$cells_per_ml)
  HAB_2017$cells_per_ml <- str_trim(HAB_2017$cells_per_ml)
  HAB_2017$cells_per_ml <- as.numeric(HAB_2017$cells_per_ml)
  HAB_2017 <- HAB_2017 %>% 
    filter(!is.na(cells_per_ml)) %>%
    mutate(year = "2017") %>%
    filter(cells_per_ml >= cpm) %>%
    group_by(year, species) %>%
    dplyr::summarise(Events = n()) %>%
    as.data.frame()
  
  #Aggregate--------------------------------------------------------
  ts <- rbind(HAB_2007_2012, HAB_2013, HAB_2014, HAB_2016, HAB_2017)
  
  return(ts)
}

#All blooms > 5000 cells ml^-1
full <- fixer(cpm = 5000)
full <- full %>% group_by(year) %>% dplyr::summarise(total = sum(Events))
plot(full$year, full$total, type = "o", ylim = c(0,90),
     pch = 20, ylab = "Bloom Events", las = 1, xlab = "Time", lwd = 2)

#cochlodinum > 300 cells ml^-1
cochlo <- fixer(cpm = 300)
cochlo[cochlo$species == "C. polykrikoides" |
     cochlo$species == "C.polykrikoides" ,]$species <- "Cochlodinium polykrikoides"
cochlo <- cochlo[cochlo$species == "Cochlodinium polykrikoides",]
cochlo <- cochlo %>% group_by(year) %>% dplyr::summarise(total = sum(Events))
points(cochlo$year, cochlo$total, type = "o", pch = 20, col = "indianred", lwd = 2)
legend(x = 2007, y = 80, legend = c(expression(paste("All reports >5000 cells ml"^"-1")),
                                    expression(paste(italic("C. polykrikoides "),"reports >300 cells ml"^"-1"))),
       col = c("black","indianred"),
       lwd = 2,
       bty = "n")
```


### Data analysis
<!--Text description of analysis methods, similar in structure and detail to a peer-reviewed paper methods section-->
No data analysis steps took place for this indicator.

<!--chapter:end:chapters/MAB_HABs_indicator.Rmd-->

# Fishery Reliance and Social Vulnerability

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Fishing community commercial and recreational fishing reliance and social vulnerability

**Indicator category**: Database pull with analysis

**Contributor(s)**: Lisa L. Colburn
  
**Data steward**: Lisa L. Colburn
  
**Point of contact**: Lisa L. Colburn 
  
**Public availability statement**: The source data used to construct the commercial fishing engagement and reliance indices include confidential information and are not available publicly. However, the commercial fishing engagement and reliance indices are not confidential so are available to the public. All calculated indices can be found [here](https://www.st.nmfs.noaa.gov/humandimensions/social-indicators/map).
  

## Methods


### Data sources
NOAA Fisheries' Community Social Vulnerability Indicators (CSVIs) were developed using secondary data including social, demographic and fisheries variables. The social and demographic data were downloaded from the 2014 American Community Survey (ACS) 5-yr estimates Dataset at the [U.S. Census American FactFinder site](https://www.census.gov/programs-surveys/acs/) for coastal communities at the Census Designated Place (CDP) level, and in some cases the County Subdivision (MCD) level. Commercial fisheries data were pulled from the SOLE server located at Northeast Fisheries Science Center in Woods Hole, MA. The recreational fishing information is publicly accessible through the [Marine Recreational Information Program (MRIP)](https://www.st.nmfs.noaa.gov/recreational-fisheries/MRIP/), and for this analysis was custom requested from NOAA Fisheries headquarters.


### Data extraction 
Commercial fisheries data was pulled from the NEFSC SOLE server in Woods Hole, MA.

SQL and SAS code for data extraction and processing steps:
```{sql, eval = F, echo = T}
create table cfders2011 as
select *
from connection to oracle
(select port, state, year, dealnum, permit, nespp3, spplndlb, sppvalue from cfders2011 where permit > 0 order by permit);
create table cfvess11 as
select *
from connection to oracle
(select permit, homeport, homest from CFDBS.cfvess11 where permit > 0 order by permit);
create table port_name as
select *
from connection to oracle
(select port, portnm from port order by port);
create table st_name as
select *
from connection to oracle
(select state, stateabb from statenm order by state);


Truncated SAS code:

/*CREATE VARIABLES FOR TOTAL LANDINGS WEIGTH AND VALUE (SUM) BY PORT OF LANDING AND BY HOMEPORT*/
data landings_ports1; set landings_ports;
run;
proc sort;
by port state;
run;
proc means noprint data = landings_ports1; by port state; 
var spplndlb sppvalue;
id port state;
output out = landport_totspp sum = L_Totlb L_Totval;
run;
proc sort;
by port;
run;
data landings_ports2; set landings_ports;
run;
proc sort;
by homeport homest;
run;
proc means noprint data = landings_ports2; by homeport homest; 
var spplndlb sppvalue;
id homeport homest;
output out = homeport_totspp sum = H_Totlb H_Totval;
run;
proc sort;
by homeport;
run;

/*CREATE SPECIES VARIABLES*/
data landings_ports_NE_spp; set landings_ports;
monklb = 0; monkval = 0; /*monkfish*/
bluelb = 0; blueval = 0; /*bluefish*/
.omitted.
otherlb = 0; otherval = 0; /*other - everything else*/
run;
data landings_ports_NE_spp2; set landings_ports_NE_spp;
if nespp3 = 012 then do; monklb = spplndlb; monkval = sppvalue; end;
...ommitted.
if nespp3 = 406 then do; spotlb = spplndlb; spotval = sppvalue; end;
if nespp3 not in (012, 023, 033, 051, 081, 105, 112, 115, 116, 120, 121, 122, 123, 124, 125, 132, 147, 152, 153, 155, 159, 168, 194, 197, 212, 
221, 240, 250, 269, 305, 329, 330, 335, 344, 345, 351, 352, 365, 366, 367, 368, 369, 370, 372, 373, 384, 415, 418, 432, 438, 443, 444, 445, 
446, 447, 464, 466, 467, 468, 469, 470, 471, 472, 507, 508, 509, 512, 517, 700, 710, 711, 724, 727, 748, 754, 769, 774, 775, 781, 786, 789, 
798, 799, 800, 801, 802, 805, 806, 899, 001, 090, 069, 107, 150, 173, 196, 334, 347, 349, 364, 371, 420, 422, 481, 484, 714, 776, 777, 823, 763, 736) 
then do; otherlb = spplndlb; otherval = sppvalue; end;
run; 

/*SUM SPECIES LANDINGS BY PORT OF LANDING*/
proc sort; by port; proc means noprint data = landings_ports_NE_spp2; by port state; 
. omitted ...
id port state;
output out = spp_porlnd_NE sum = ;
run;
proc sort;
by port;
run;

/*SUM SPECIES LANDINGS BY HOMEPORT*/
data spp_home; set landings_ports_NE_spp2;
run;
proc sort; by homeport homest; proc means noprint data = spp_home; by homeport homest; 
. species are counted..
id homeport homest;
output out = spp_homep_NE sum = ;
run
proc sort;
by homeport; run;

/*MERGE TOTAL PERMITS AND TOTAL DEALERS BY PORT OF LANDING*/
data land_port_totperm2; set land_port_totperm;
run;
proc sort;
by port; run;
data lnd_port_permit; merge spp_porlnd_NE (IN=X) land_port_totperm2 (IN=Y);
by port; if X=1; run;data land_port_totdeal2; set land_port_totdeal;
run;
proc sort;
by port;
run;
data lnd_port_permit_deal; merge lnd_port_permit (IN=x) land_port_totdeal2 (IN=Y);
by port; if X=1; run;

/*MERGE WITH PORT NAME AND STATE ABBREVIATION*/
data lnd_port_permit_deal_nm; merge lnd_port_permit_deal (IN=X) port_name (IN=Y);
by port; if X=1; run;
data lnd_port_permit_deal_nm_st; merge lnd_port_permit_deal_nm (IN=x) st_name (IN=Y); proc sort;
by port; if X=1; run;

/*MERGE TOTAL PERMITS AND TOTAL DEALERS BY HOMEPORT*/
data home_port_totperm2; set home_port_totperm;
run;
proc sort;
by homeport;
run;
data home_port_permit; merge spp_homep_NE (IN=X) home_port_totperm2 (IN=Y);
by homeport; if X=1; run; data home_port_totdeal2; set home_port_totdeal;
run;
proc sort;
by homeport;
run;
data home_port_permit_deal; merge home_port_permit (IN=x) home_port_totdeal2 (IN=Y);
by homeport; if X=1; run; proc sort;
by homeport;
run;

/*MERGE TOTAL LANDINGS BY PORT OF LANDING*/
data lnd_port_per_deal_nm_st_tspp; merge lnd_port_permit_deal_nm_st (IN=X) landport_totspp (IN=Y);
by port; if X=1; run;

/*MERGE TOTAL LANDINGS BY HOMEPORT*/
data home_port_per_deal_tspp; merge home_port_permit_deal (IN=X) homeport_totspp (IN=Y);
by homeport; if X=1; run;
data netana.port_landing11; set lnd_port_per_deal_nm_st_tspp;
if state in (22, 32, 24, 42, 7, 35, 33, 8, 23, 49, 36);
run;
proc sort;
by port state;
run;
data netana.homeport11; set home_port_per_deal_tspp;
if homest in ('ME', 'NH', 'MA', 'RI', 'CT', 'NY', 'NJ', 'DE', 'MD', 'VA', 'NC');
run;
proc sort;
by homeport homest;
run;


```


### Data analysis
The indicators were developed using the methodology described in @Jacob2010, @Jacob2013, @colburn_social_2012 and @jepson_development_2013. Indicators were constructed through principal component analysis with a single factor solution, and the following criteria had to have been met: a minimum variance explained of 45%; Kasier-Meyer Olkin measure of sampling adequacy above.500; factor loadings above.350; Bartlett's test of sphericity significance above .05; and an Armor's Theta reliability coefficient above .500. Factor scores for each community were ranked based on standard deviations into the following categories: High(>=1.00SD), MedHigh .500-.999 SD), Moderate (.000-.499 SD) and Low (<.000 SD).

### Plotting

```{r map1, echo = T, eval=T, fig.cap="Commercial (A) and recreational (B) reliance and social vulnerability in the Mid-Atlantic Bight",  fig.width = 8, fig.height = 5.4, fig.align='center', fig.show='hold', message=F, warning=F}
  
#Libraries
library(rgdal)
library(maps);library(raster);
library(mapdata);library(grid);
library(stringr);library(png)
library(ncdf4);library(dplyr)

#Get data and set projection
data.dir <- here::here("data")
gis.dir <- here::here("gis")


load(file.path(data.dir, "SOE_data_erddap.Rdata"))

#coastline
coast <- readOGR(gis.dir, 'NES_LME_coast', verbose = F)


#projection
map.crs <- CRS("+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0
               +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
coast <- spTransform(coast,map.crs)



#Make figures
par(mfrow=c(1,2), mar = c(2,2.85,1,1))

cat1 <- SOE.data[SOE.data$Var == "social vulnerability MAB",]$Value
cat2 <- SOE.data[SOE.data$Var == "commercial reliance MAB",]$Value

#group data into categories
ncat1 <- NULL
for (i in 1:length(cat1)){
  if (cat1[i] > 2){
    ncat1[i] <- "C"
  } else if ((cat1[i] <= 2) & (cat1[i] > 1)){
    ncat1[i] <- "B"
  } else {
    ncat1[i] <- "A"
  }
}

ncat2 <- NULL
for (i in 1:length(cat1)){
  if (cat2[i] > 2){
    ncat2[i] <- 3
  } else if ((cat2[i] <= 2) & (cat2[i] > 1)){
    ncat2[i] <- 2
  } else {
    ncat2[i] <- 1
  }
}


cat3 <- paste0(ncat1, ncat2)
cat3 <- factor(cat3, levels = c("A1","A2","A3",
                                "B1","B2","B3",
                                "C1","C2","C3"),
               ordered = TRUE)
lon <- SOE.data[SOE.data$Var == "choropleth longitude MAB",]$Value
lat <- SOE.data[SOE.data$Var == "choropleth latitude MAB",]$Value

#new dataframe to turn into sp object
mab.dat <- data.frame(lon = lon,
                      lat = lat,
                      cat3 = cat3)

#break up into groups for plotting - allows for effective layering
g1 <- mab.dat %>% filter(cat3 == "A1"|cat3 == "A2"|cat3 == "A3") %>% arrange(cat3)
g2 <- mab.dat %>% filter(cat3 == "B1"|cat3 == "B2") %>% arrange(cat3)
g3 <- mab.dat %>% filter(cat3 == "C1") %>% arrange(cat3)


g2_big <- mab.dat %>% filter(cat3 == "B3")
g3_big <- mab.dat %>% filter(cat3 == "C2"|cat3 == "C3") %>% arrange(cat3)


#turn grouped data into sp objects for overlaying on plot of Mid-Atlantic

coordinates(g1) <- ~lon+lat
g1@proj4string <- map.crs
g1 <- spTransform(g1, map.crs)
colo1 <- c(rep("#FFFFFF",table(g1$cat3)[1]),
           rep("#B8B8FF",table(g1$cat3)[2]),
           rep("#7F7FFF",table(g1$cat3)[3]))


coordinates(g2) <- ~lon+lat
g2@proj4string <- map.crs
g2 <- spTransform(g2, map.crs)
colo2 <- c(rep("#FFB8B8",table(g2$cat3)[4]),rep("#B871B8",table(g2$cat3)[5]))


coordinates(g3) <- ~lon+lat
g3@proj4string <- map.crs
g3 <- spTransform(g3, map.crs)
colo3 <- c("#FF7F7F")


coordinates(g2_big) <- ~lon+lat
g2_big@proj4string <- map.crs
g2_big <- spTransform(g2_big, map.crs)

coordinates(g3_big) <- ~lon+lat
g3_big@proj4string <- map.crs
g3_big <- spTransform(g3_big, map.crs)


#plot map and dots of different size based on category

plot(coast, xlim = c(-77.5,-71), ylim = c(36.5,41),col = "grey")
plot(g1, pch = 16, col = colo1,cex = 1, add = T)
plot(g2, pch = 16, col = colo2,cex = 1, add = T)
plot(g3, pch = 16, col = colo3,cex = 1,add = T)


plot(g2_big, pch = 16, col = "blue",cex = 2, add = T)
plot(g3_big, pch = 16, col = c(rep("red",table(g3_big$cat3)[8]),rep("black",table(g3_big$cat3)[9])),cex = 2,add = T)

#colors of bivariate color scheme
base_col <- c("#7F7FFF","blue","black",
              "#B8B8FF","#B871B8","red",
              "#FFFFFF","#FFB8B8","#FF7F7F")

#raster image to display on plot as bivariate legend
r <- raster(xmn = -73.5, xmx = -71.4, ymn = 36, ymx = 37.8, nrows = 3, ncols = 3)
r[] <- 1:9
plot(r, col = base_col, legend = F, bty = "n", axes = F, yaxt = "n", frame.plot = F,
     xaxt='n', add = T)

#text
text(-73.9, 36.9, "Commercial Reliance",srt=90, cex = .75)
text(-72.4,35.6, "Social Vulnerability", cex = .75)
text(-77.1,42, "A", cex = 1.5)

axis(1, at = c(-76,-74,-72), labels = paste( c(-76,-74,-72) * -1, 'W'),pos = 34.7,col = NA, col.ticks = 1)
axis(2, at = axTicks(2), labels = paste(axTicks(2), 'N'), las = T, pos = -77.8,col = NA, col.ticks = 1)
box(lty = 1, lwd = 2)
#text

#####################################################################################################

cat1 <- SOE.data[SOE.data$Var == "social vulnerability MAB",]$Value
cat2 <- SOE.data[SOE.data$Var == "recreational reliance MAB",]$Value

#group data into categories
ncat1 <- NULL
for (i in 1:length(cat1)){
  if (cat1[i] > 2){
    ncat1[i] <- "C"
  } else if ((cat1[i] <= 2) & (cat1[i] > 1)){
    ncat1[i] <- "B"
  } else {
    ncat1[i] <- "A"
  }
}

ncat2 <- NULL
for (i in 1:length(cat1)){
  if (cat2[i] > 2){
    ncat2[i] <- 3
  } else if ((cat2[i] <= 2) & (cat2[i] > 1)){
    ncat2[i] <- 2
  } else {
    ncat2[i] <- 1
  }
}


cat3 <- paste0(ncat1, ncat2)
cat3 <- factor(cat3, levels = c("A1","A2","A3",
                                "B1","B2","B3",
                                "C1","C2","C3"),
               ordered = TRUE)
lon <- SOE.data[SOE.data$Var == "choropleth longitude MAB",]$Value
lat <- SOE.data[SOE.data$Var == "choropleth latitude MAB",]$Value

#new dataframe to turn into sp object
mab.dat <- data.frame(lon = lon,
                      lat = lat,
                      cat3 = cat3)

#break up into groups for plotting - allows for effective layering
g1 <- mab.dat %>% filter(cat3 == "A1"|cat3 == "A2"|cat3 == "A3") %>% arrange(cat3)
g2 <- mab.dat %>% filter(cat3 == "B1"|cat3 == "B2") %>% arrange(cat3)
g3 <- mab.dat %>% filter(cat3 == "C1") %>% arrange(cat3)


g2_big <- mab.dat %>% filter(cat3 == "B3")
g3_big <- mab.dat %>% filter(cat3 == "C2"|cat3 == "C3") %>% arrange(cat3)


#turn grouped data into sp objects for overlaying on plot of Mid-Atlantic

coordinates(g1) <- ~lon+lat
g1@proj4string <- map.crs
g1 <- spTransform(g1, map.crs)
colo1 <- c(rep("#FFFFFF",table(g1$cat3)[1]),
           rep("#B8B8FF",table(g1$cat3)[2]),
           rep("#7F7FFF",table(g1$cat3)[3]))


coordinates(g2) <- ~lon+lat
g2@proj4string <- map.crs
g2 <- spTransform(g2, map.crs)
colo2 <- c(rep("#FFB8B8",table(g2$cat3)[4]),rep("#B871B8",table(g2$cat3)[5]))


coordinates(g3) <- ~lon+lat
g3@proj4string <- map.crs
g3 <- spTransform(g3, map.crs)
colo3 <- c("#FF7F7F")


coordinates(g2_big) <- ~lon+lat
g2_big@proj4string <- map.crs
g2_big <- spTransform(g2_big, map.crs)

coordinates(g3_big) <- ~lon+lat
g3_big@proj4string <- map.crs
g3_big <- spTransform(g3_big, map.crs)


#plot map and dots of different size based on category

plot(coast, xlim = c(-77.5,-71), ylim = c(36.5,41),col = "grey")
plot(g1, pch = 16, col = colo1,cex = 1, add = T)
plot(g2, pch = 16, col = colo2,cex = 1, add = T)
plot(g3, pch = 16, col = colo3,cex = 1,add = T)


plot(g2_big, pch = 16, col = "blue",cex = 2, add = T)
plot(g3_big, pch = 16, col = c(rep("red",table(g3_big$cat3)[8]),rep("black",table(g3_big$cat3)[9])),cex = 2,add = T)

#colors of bivariate color scheme
base_col <- c("#7F7FFF","blue","black",
              "#B8B8FF","#B871B8","red",
              "#FFFFFF","#FFB8B8","#FF7F7F")

#raster image to display on plot as bivariate legend
r <- raster(xmn = -73.5, xmx = -71.4, ymn = 36, ymx = 37.8, nrows = 3, ncols = 3)
r[] <- 1:9
plot(r, col = base_col, legend = F, bty = "n", axes = F, yaxt = "n", frame.plot = F,
     xaxt='n', add = T)
axis(1, at = c(-76,-74,-72), labels = paste( c(-76,-74,-72) * -1, 'W'),pos = 34.7,col = NA, col.ticks = 1)
axis(2, at = axTicks(2), labels = paste(axTicks(2), 'N'), las = T, pos = -77.8,col = NA, col.ticks = 1)
box(lty = 1, lwd = 2)
#text
text(-73.9, 36.9, "Recreational Reliance",srt=90, cex = .75)
text(-72.4,35.6, "Social Vulnerability", cex = .75)
text(-77.1,42, "B", cex = 1.5)
```








<!--chapter:end:chapters/Comm_rel_vuln_indicator.Rmd-->

# Fishing Community Climate Vulnerability

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)
image.dir <- here::here("images")

```

**Description**: Community climate vulnerability

**Indicator category**: Database pull with analysis

**Contributor(s)**: Lisa L. Colburn
  
**Data steward**: Lisa L. Colburn
  
**Point of contact**: Lisa L. Colburn
  
**Public availability statement**: The fisheries data used for this analysis includes confidential information and is not available to the public. 

  

## Methods
### Data sources
The data used in community climate vulnerability analyses were derived from the following sources in partnership with the Atlantic Coastal Cooperative Statistics Program's (ACCSP) Standard Atlantic Fisheries Information System (SAFIS).

```{r source table, echo = F, include = T, results='asis'}
tabl <- '
|Database Name     | Description                                  |
|:-------------------------|:-------------------------------------|
|Cfdersyyyy|The dealer data are transaction-level pricing at the level of the "market-category." These data are primarily generated through mandatory reporting by federally-permitted fish dealers. The federal reporting is supplemented with data from non-federally-permitted (state-only) fish dealers. Data are currently reported electronically in partnership with ACCSP through SAFIS.           |
|Cfvessyyy|A related database that contains permit information.   |

'
cat(tabl)
```

In these databases, the variable "port" contains the post associated with the vessel. The variable "Statenm" refers to the state of the mailing address of the owner.

### Data extraction 
```{sql, eval = F, echo = T}
create table cfders2011 as
select *
from connection to oracle
(select port, state, year, dealnum, permit, nespp3, spplndlb, sppvalue from cfders2011 where permit > 0 order by permit);
create table cfvess11 as
select *
from connection to oracle
(select permit, homeport, homest from CFDBS.cfvess11 where permit > 0 order by permit);
create table port_name as
select *
from connection to oracle
(select port, portnm from port order by port);
create table st_name as
select *
from connection to oracle
(select state, stateabb from statenm order by state);

Truncated SAS code:
/*CREATE VARIABLES FOR TOTAL LANDINGS WEIGTH AND VALUE (SUM) BY PORT OF LANDING AND BY HOMEPORT*/
data landings_ports1; set landings_ports;
run;
proc sort;
by port state;
run;
proc means noprint data = landings_ports1; by port state; 
var spplndlb sppvalue;
id port state;
output out = landport_totspp sum = L_Totlb L_Totval;
run;
proc sort;
by port;
run;
data landings_ports2; set landings_ports;
run;
proc sort;
by homeport homest;
run;
proc means noprint data = landings_ports2; by homeport homest; 
var spplndlb sppvalue;
id homeport homest;
output out = homeport_totspp sum = H_Totlb H_Totval;
run;
proc sort;
by homeport;
run;
/*CREATE SPECIES VARIABLES*/
data landings_ports_NE_spp; set landings_ports;
monklb = 0; monkval = 0; /*monkfish*/
bluelb = 0; blueval = 0; /*bluefish*/
.omitted.
otherlb = 0; otherval = 0; /*other - everything else*/
run;
data landings_ports_NE_spp2; set landings_ports_NE_spp;
if nespp3 = 012 then do; monklb = spplndlb; monkval = sppvalue; end;
...ommitted.
if nespp3 = 406 then do; spotlb = spplndlb; spotval = sppvalue; end;
if nespp3 not in (012, 023, 033, 051, 081, 105, 112, 115, 116, 120, 121, 122, 123, 124, 125, 132, 147, 152, 153, 155, 159, 168, 194, 197, 212, 
221, 240, 250, 269, 305, 329, 330, 335, 344, 345, 351, 352, 365, 366, 367, 368, 369, 370, 372, 373, 384, 415, 418, 432, 438, 443, 444, 445, 
446, 447, 464, 466, 467, 468, 469, 470, 471, 472, 507, 508, 509, 512, 517, 700, 710, 711, 724, 727, 748, 754, 769, 774, 775, 781, 786, 789, 
798, 799, 800, 801, 802, 805, 806, 899, 001, 090, 069, 107, 150, 173, 196, 334, 347, 349, 364, 371, 420, 422, 481, 484, 714, 776, 777, 823, 763, 736) 
then do; otherlb = spplndlb; otherval = sppvalue; end;
run; 
/*SUM SPECIES LANDINGS BY PORT OF LANDING*/
proc sort; by port; proc means noprint data = landings_ports_NE_spp2; by port state; 
. omitted ...
id port state;
output out = spp_porlnd_NE sum = ;
run;
proc sort;
by port;
run;
/*SUM SPECIES LANDINGS BY HOMEPORT*/
data spp_home; set landings_ports_NE_spp2;
run;
proc sort; by homeport homest; proc means noprint data = spp_home; by homeport homest; 
. species are counted..
id homeport homest;
output out = spp_homep_NE sum = ;
run
proc sort;
by homeport; run;
/*MERGE TOTAL PERMITS AND TOTAL DEALERS BY PORT OF LANDING*/
data land_port_totperm2; set land_port_totperm;
run;
proc sort;
by port; run;
data lnd_port_permit; merge spp_porlnd_NE (IN=X) land_port_totperm2 (IN=Y);
by port; if X=1; run;data land_port_totdeal2; set land_port_totdeal;
run;
proc sort;
by port;
run;
data lnd_port_permit_deal; merge lnd_port_permit (IN=x) land_port_totdeal2 (IN=Y);
by port; if X=1; run;
/*MERGE WITH PORT NAME AND STATE ABBREVIATION*/
data lnd_port_permit_deal_nm; merge lnd_port_permit_deal (IN=X) port_name (IN=Y);
by port; if X=1; run;
data lnd_port_permit_deal_nm_st; merge lnd_port_permit_deal_nm (IN=x) st_name (IN=Y); proc sort;
by port; if X=1; run;
/*MERGE TOTAL PERMITS AND TOTAL DEALERS BY HOMEPORT*/
data home_port_totperm2; set home_port_totperm;
run;
proc sort;
by homeport;
run;
data home_port_permit; merge spp_homep_NE (IN=X) home_port_totperm2 (IN=Y);
by homeport; if X=1; run; data home_port_totdeal2; set home_port_totdeal;
run;
proc sort;
by homeport;
run;
data home_port_permit_deal; merge home_port_permit (IN=x) home_port_totdeal2 (IN=Y);
by homeport; if X=1; run; proc sort;
by homeport;
run;
/*MERGE TOTAL LANDINGS BY PORT OF LANDING*/
data lnd_port_per_deal_nm_st_tspp; merge lnd_port_permit_deal_nm_st (IN=X) landport_totspp (IN=Y);
by port; if X=1; run;
/*MERGE TOTAL LANDINGS BY HOMEPORT*/
data home_port_per_deal_tspp; merge home_port_permit_deal (IN=X) homeport_totspp (IN=Y);
by homeport; if X=1; run;
data netana.port_landing11; set lnd_port_per_deal_nm_st_tspp;
if state in (22, 32, 24, 42, 7, 35, 33, 8, 23, 49, 36);
run;
proc sort;
by port state;
run;
data netana.homeport11; set home_port_per_deal_tspp;
if homest in ('ME', 'NH', 'MA', 'RI', 'CT', 'NY', 'NJ', 'DE', 'MD', 'VA', 'NC');
run;
proc sort;
by homeport homest;
run;


```

### Data analysis and plotting

The results described below were developed using the methodology described in @colburn_indicators_2016.

1.  *Mapping community climate vulnerability* - The map was produced using two variables: total value landed in a community and community species vulnerability, defined below:
    a.  Communities were grouped based on total value of landings into the following categories: 1 (<\$ 200,000), 2 (\$200,000-\$9,999,999), 3 (\$10,000,000-\$49, 999,999), and 4 (\$50,000,000 and above). Only communities with a total value landed of \$200,000 or more were selected for the mapping process.
    b.  Community climate vulnerability is determined by the percent contribution of species classified as very high, high, moderate, or low climate vulnerability in a community. The percent contribution of species is calculated as following: 


        * **% VH & H** = value of landing contributed by species classified as having very high or high climate change vulnerability/total value of landings \* 100

        * **% M** = value of landing contributed by species classified as having moderate climate 
change vulnerability/total value of landings \* 100

        * **% L** = value of landing contributed by species classified as having low climate change 
vulnerability/ total value of landings \* 100

If a community received a dominant score (50% or more) for any of the above categories, % VH &, %M, or %L, then the community received a respective community species vulnerability ranking of High, Moderate, or Low.  For example, if 90% of the total value landed a community is contributed by species classified as having very high or high climate change vulnerability, then this community gets "Very High/High" community species vulnerability. In case of no dominant percentage identified, the community gets a "Mixed" community species vulnerability ranking. 

2)  *Pie charts* - The pie charts were created using the NMFS landings data pulled from NEFSC databases in Woods Hole, MA. The percent contribution of each species was calculated by dividing the total value of landings in each port by each species' landed value.  Data was calculated and graphed in a pie chart in Excel and given the colors that represent High (red), Moderate (blue), Low (yellow) climate vulnerability. The "other" category consists of species with low landings and/or those that do not have a vulnerability ranking based on @Hare2016.  These species were aggregated and given the color gray.


```{r species-vulnerability, fig.cap="Commercial species vulnerability to climate change in in New England fishing communities.",fig.align = 'center', echo = F, eval = T}
knitr::include_graphics(file.path(image.dir, 'Species_vulnerability_NE.jpg'))
```




<!--chapter:end:chapters/Comm_climate_vuln_indicator.Rmd-->

# Harbor Porpoise Bycatch

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```

**Description**: Harbor Porpoise Indicator

**Indicator category**: Synthesis of published information; Published methods

**Contributor(s)**: Christopher D. Orphandies
  
**Data steward**: Chris Orphanides, <chris.orphanides@noaa.gov>

**Point of contact**: Chris Orphanides, <chris.orphanides@noaa.gov>

**Public availability statement**: Source data are available in public [stock assessment reports](https://www.fisheries.noaa.gov/national/marine-mammal-protection/marine-mammal-stock-assessment-reports-region) (2018 report in-press). Derived data as shown in the 2018 SOE reports are available [here](http://comet.nefsc.noaa.gov/erddap/tabledap/protected_species_soe_v1.html)
  

## Methods


### Data sources
Reported harbor porpoise bycatch estimates and potential biological removal levels can be found in publicly available documents; detailed [here](https://www.fisheries.noaa.gov/national/marine-mammal-protection/marine-mammal-stock-assessment-reports-region). The most recent bycatch estimates for 2016 were taken from the 2018 stock assessment (in-press). More detailed documentation as to the methods employed can be found in NOAA Fisheries Northeast Fisheries Science Center (NEFSC) Center Reference Documents (CRDs) found on the NEFSC [publications page](https://www.nefsc.noaa.gov/publications/crd/). 

The document for the 2016 estimates (CRD 19-04) is available [here](https://www.nefsc.noaa.gov/publications/crd/crd1904/crd1904.pdf). Additional methodological details are available for previous year's estimates and are documented in numerous published CRDs: [CRD 17-18](https://www.nefsc.noaa.gov/publications/crd/crd1718/crd1718.pdf), [CRD-16-05](https://www.nefsc.noaa.gov/publications/crd/crd1605/crd1605.pdf), [CRD 15-15](https://www.nefsc.noaa.gov/publications/crd/crd1515/crd1515.pdf), [CRD 14-02](https://repository.library.noaa.gov/view/noaa/4718), [CRD 13-13](https://www.nefsc.noaa.gov/publications/crd/crd1313/crd1313_2nd_ed.pdf), [CRD 11-08](https://www.nefsc.noaa.gov/publications/crd/crd1108/1108.pdf), [CRD 10-10](https://www.nefsc.noaa.gov/publications/crd/crd1010/crd1010.pdf), [CRD 07-20](https://www.nefsc.noaa.gov/publications/crd/crd0720/crd0720.pdf), [CRD 06-13](https://www.nefsc.noaa.gov/publications/crd/crd0613/crd0613.pdf), [CRD 03-18](https://www.nefsc.noaa.gov/publications/crd/crd0318/crd0318.pdf), [CRD 01-15](https://www.nefsc.noaa.gov/publications/crd/crd0115/0115.pdf), and [CRD 99-17](https://www.nefsc.noaa.gov/publications/crd/pdfs/crd9917.pdf).

### Data extraction 
Annual gillnet bycatch estimates are documented in a CRD (see sources above). These feed into the Stock Assessment Reports which report both the annual bycatch estimate and the mean 5-year estimate. The 5-year estimate is the one used for management purposes, so that is the one provided for the SOE plot.

### Data analysis
Bycatch estimates as found in stock assessment reports were plotted along with confidence intervals. The confidence intervals were calculated from published CVs assuming a normal distribution ($\sigma = \mu CV$; $CI = \bar{x} \pm \sigma * 1.96$).

### Plotting

```{r harbor-porpoise, fig.cap="Harbor porpoise bycatch estimated shown with Potential Biological Removal (red) and confidence intervals (orange).", echo = T, message=F, warning=F, fig.pos='H', fig.height=4}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mar = c(4, 6, 2, 6))

soe.plot(SOE.data, 'Time', "Harbor porpoise bycatch estimates",            
         rel.y.num = 1.2, end.start = 2007, full.trend = F, point.cex = 1,
         ymax = F, y.upper = 2500, mean_line = F, x.label = 'Year',
         y.label = 'Bycatch, n', rel.y.text = 1)

legend(2000, 2250, legend = "Potential Biological Removal",
       col = adjustcolor("red", .5), lwd = 3,
       bty = "n", cex = 0.9)

#credible intervals and PBI
lw_CI <- SOE.data[Var == 'Harbor porpoise bycatch 2.5 CI',
                       list(Time, Value)]
up_CI <- SOE.data[Var == 'Harbor porpoise bycatch 97.5 CI',
                       list(Time, Value)]
pbi   <- SOE.data[Var == 'Harbor porpoise potential biological removal',
                       list(Time, Value)]

points(pbi,   type  = "l", lty = 1, col = adjustcolor("red", .5), lwd = 3)
points(lw_CI, type  = "l", lty = 2, col = adjustcolor("darkorange", .9), lwd = 2.5)
points(up_CI, type  = "l", lty = 2, col = adjustcolor("darkorange", .9), lwd = 2.5)
```


<!--chapter:end:chapters/HP_indicator.Rmd-->

# Right Whale Abundance 


```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Right Whale

**Indicator category**: Synthesis of published information; Published methods

**Contributor(s)**: Christopher D. Orphanides
  
**Data steward**: Chris Orphanides, chris.orphanides@noaa.gov
  
**Point of contact**: Richard Pace, richard.pace@noaa.gov
  
**Public availability statement**: Source data are available from the New England Aquarium upon request. Derived data are available [here](http://comet.nefsc.noaa.gov/erddap/tabledap/protected_species_soe_v1.html)

## Methods

### Data sources
The North Atlantic right whale abundance estimates were taken from a published document [see @Pace2017], except for the most recent 2016 and 2017 estimates. Abundance estimates from 2016 and 2017 were taken from the 2016 NOAA marine mammal stock assessment [@Hayes2017] and an unpublished 2017 stock assessment.


### Data extraction 
Data were collected from existing reports and validated by report authors. 

### Data analysis
Analysis for right whale abundance estimates is provided by @Pace2017, and code can be found in the [supplemental materials](https://onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2Fece3.3406&file=ece33406-sup-0001-SupInfo.docx). 

### Plotting

```{r rw-abundance, fig.cap="North Atlantic right whale population estimates shown with 95% credible intervals.",echo = T, message=F, warning=F,  fig.pos='H', fig.height = 4.5, fig.width=7}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))

par(mar = c(5,5,4,1))
soe.plot(SOE.data, 'Time', "right_whale_median", rel.y.num = 1.2, 
         full.trend = F, lwd = 1.5, point.cex = 1, end.start = 2007,
         x.label = 'Year', y.label = 'Abundance, n', 
         rel.y.text = 1)

lw_CI <- SOE.data[Var == 'right_whale_lower_95', list(Time, Value)]
up_CI <- SOE.data[Var == 'right_whale_upper_95', list(Time, Value)]
points(lw_CI, type  = "l", lty = 2, col = adjustcolor("darkorange", .6), lwd = 2.5)
points(up_CI, type  = "l", lty = 2, col = adjustcolor("darkorange", .6), lwd = 2.5)

```

<!--chapter:end:chapters/RW_indicator.rmd-->

# Ichthyoplankton Diversity

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: NOAA NEFSC Oceans and Climate branch public ichthyoplankton dataset

**Indicator category**: Database pull with analysis 

**Contributor(s)**: Harvey J. Walsh
  
**Data steward**: Harvey Walsh, harvey.walsh@noaa.gov
  
**Point of contact**: Harvey Walsh, harvey.walsh@noaa.gov
  
**Public availability statement**: Source data are available to the public [here](ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/). Derived data for this indicator are available [here](https://comet.nefsc.noaa.gov/erddap/tabledap/ichthyo_div_soe_v1.html).


## Methods
Data from the NOAA NEFSC Oceans and Climate branch public dataset were used to examine changes in diversity of abundance among 45 ichthyoplankton taxa.  The 45 taxa were established [@RN126], and include the most abundant taxa from the 1970s to present that represent consistency in the identification of larvae. 

### Data sources
Multi-species plankton surveys cover the entire Northeast US shelf from Cape Hatteras, North Carolina, to Cape Sable, Nova Scotia, four to six times per year.  A random-stratified design based on the NEFSC bottom trawl survey design [@Azarovitz1981] is used to collect samples from 47 strata. The number of strata is lower than the trawl survey as many of the narrow inshore and shelf-break strata are combined in the EcoMon design. 
The area encompassed by each stratum determined the number of samples in each stratum. Samples were collected both day and night using a 61 cm bongo net. Net tow speed was 1.5 knots and maximum sample depth was 200 m. Double oblique tows were a minimum of 5 mintues in duration, and fished from the surface to within 5 m of the seabed or to a maximum depth of 200 m. The volume filtered of all collections was measured with mechanical flowmeters mounted across the mouth of each net. 

Processing of most samples was conducted at the Morski Instytut Rybacki (MIR) in Szczecin, Poland; the remaining samples were processed at the NEFSC or the Atlantic Reference Center, St Andrews, Canada.  Larvae were identified to the lowest possible taxa and enumerated for each sample.  Taxon abundance for each station was standardized to number under 10 m^-2^ sea surface.

### Data extraction
Data retrieved from NOAA NEFSC Oceans and Climate branch [public dataset](ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/) (Filename: "EcoMon_Plankton_Data_v3_0.xlsx", File Date: 10/20/2016).

### Data analysis
All detailed data processing steps are not currently included in this document, but general steps are outlined. Data were grouped into seasons: spring = February, March, April and fall = September, October, November. Stratified weighted mean abundance was calculated for each taxon for each year and season across all plankton strata (n = 47) for 17 years (1999 to 2015). Shannon Diversity Index and count of positive taxon was calculated for each season and year.

MATLAB code used to calculate diversity indices:
```{octave, echo = T, eval = F}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Calculates Shannon Diversity Index of Ichthyoplankton data
%   
%   Input: excel file of ichthyoplankton data
%
%   USES: index_SaW.m
%
%   last modified: 03August2018, HJW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Data retrieved from NOAA NEFSC Oceans and Climate branch public dataset
%   ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/
%   Filename: EcoMon_Plankton_Data_v3_0.xlsx
%   File Date: 10/20/2016
 
% Data processing - not included in this file
%   Data grouped into seasons: spring = Feb to Apr, fall = Sept to Nov
%   Stratified weighted mean abundance was calculated for each taxon for each year and season
%       Abundance across all plankton strata (n = 47) for 17 years (1999 to 2015)
 
%% Import aggregated data from spreadsheet
% Script for importing data from the following spreadsheet:
%
%    Workbook: /Users/hwalsh/NEFSC larval samples/CombinedData/SOE_Diversity/NEFSCIchthyoplanktonAbundance.xlsx
%    Worksheet: Sheet1
%% Output Data
% SprSW = Spring Shannon Diversity Index
% SprCount = Spring count of positive ichthyoplankton taxa (Max = 45)
% FallSW = Fall Shannon Diversity Index
% FallCount = Fall count of positive ichthyoplankton taxa (Max = 45)
 
%% Import the data
[~, ~, raw] = xlsread('/Users/hwalsh/NEFSC larval samples/CombinedData/SOE_Diversity/NEFSCIchthyoplanktonAbundance.xlsx','Sheet1');
raw = raw(2:end,7:end);
 
%% Create output variable
data = reshape([raw{:}],size(raw));
 
%% Create table
NEFSCIchthyoplanktonAbundance = table;
 
%% Allocate imported array to column variable names
NEFSCIchthyoplanktonAbundance.Brevoortiatyrannus = data(:,1);
NEFSCIchthyoplanktonAbundance.Clupeaharengus = data(:,2);
NEFSCIchthyoplanktonAbundance.Cyclothonespp = data(:,3);
NEFSCIchthyoplanktonAbundance.Diaphusspp = data(:,4);
NEFSCIchthyoplanktonAbundance.Ceratoscopelusmaderensis = data(:,5);
NEFSCIchthyoplanktonAbundance.Benthosemaspp = data(:,6);
NEFSCIchthyoplanktonAbundance.Urophycisspp = data(:,7);
NEFSCIchthyoplanktonAbundance.Enchelyopuscimbrius = data(:,8);
NEFSCIchthyoplanktonAbundance.Gadusmorhua = data(:,9);
NEFSCIchthyoplanktonAbundance.Melanogrammusaeglefinus = data(:,10);
NEFSCIchthyoplanktonAbundance.Pollachiusvirens = data(:,11);
NEFSCIchthyoplanktonAbundance.Merlucciusalbidus = data(:,12);
NEFSCIchthyoplanktonAbundance.Merlucciusbilinearis = data(:,13);
NEFSCIchthyoplanktonAbundance.Centropristisstriata = data(:,14);
NEFSCIchthyoplanktonAbundance.Pomatomussaltatrix = data(:,15);
NEFSCIchthyoplanktonAbundance.Cynoscionregalis = data(:,16);
NEFSCIchthyoplanktonAbundance.Leiostomusxanthurus = data(:,17);
NEFSCIchthyoplanktonAbundance.Menticirrhusspp = data(:,18);
NEFSCIchthyoplanktonAbundance.Micropogoniasundulatus = data(:,19);
NEFSCIchthyoplanktonAbundance.Tautogolabrusadspersus = data(:,20);
NEFSCIchthyoplanktonAbundance.Tautogaonitis = data(:,21);
NEFSCIchthyoplanktonAbundance.Auxisspp = data(:,22);
NEFSCIchthyoplanktonAbundance.Scomberscombrus = data(:,23);
NEFSCIchthyoplanktonAbundance.Peprilusspp = data(:,24);
NEFSCIchthyoplanktonAbundance.Sebastesspp = data(:,25);
NEFSCIchthyoplanktonAbundance.Prionotusspp = data(:,26);
NEFSCIchthyoplanktonAbundance.Myoxocephalusaenaeus = data(:,27);
NEFSCIchthyoplanktonAbundance.Myoxocephalusoctodecemspinosus = data(:,28);
NEFSCIchthyoplanktonAbundance.Ammodytesspp = data(:,29);
NEFSCIchthyoplanktonAbundance.Pholisgunnellus = data(:,30);
NEFSCIchthyoplanktonAbundance.Ulvariasubbifurcata = data(:,31);
NEFSCIchthyoplanktonAbundance.Anarhichasspp = data(:,32);
NEFSCIchthyoplanktonAbundance.Citharichthysarctifrons = data(:,33);
NEFSCIchthyoplanktonAbundance.Etropusspp = data(:,34);
NEFSCIchthyoplanktonAbundance.Syaciumspp = data(:,35);
NEFSCIchthyoplanktonAbundance.Bothusspp = data(:,36);
NEFSCIchthyoplanktonAbundance.Hippoglossinaoblonga = data(:,37);
NEFSCIchthyoplanktonAbundance.Paralichthysdentatus = data(:,38);
NEFSCIchthyoplanktonAbundance.Pseudopleuronectesamericanus = data(:,39);
NEFSCIchthyoplanktonAbundance.Hippoglossoidesplatessoides = data(:,40);
NEFSCIchthyoplanktonAbundance.Limandaferruginea = data(:,41);
NEFSCIchthyoplanktonAbundance.Glyptocephaluscynoglossus = data(:,42);
NEFSCIchthyoplanktonAbundance.Scophthalmusaquosus = data(:,43);
NEFSCIchthyoplanktonAbundance.Symphurusspp = data(:,44);
NEFSCIchthyoplanktonAbundance.Lophiusamericanus = data(:,45);
 
%% Clear temporary variables
clearvars data raw;
%% Spearate Spring (Spr) and Fall data
Spr=table2array(NEFSCIchthyoplanktonAbundance(1:17,:))';
Fall=table2array(NEFSCIchthyoplanktonAbundance(18:34,:))';
%% Shannon-Wiener index
[SprSW]=index_SaW(Spr,exp(1));
[FallSW]=index_SaW(Fall,exp(1));
%% Count of number taxa per year 
SprCount=zeros(1,length(SprSW));
for ii=1:length(Spr)
    for yy=1:length(SprCount)
        if Spr(ii,yy)>0
            SprCount(1,yy)=SprCount(1,yy)+1;
        end
    end
end
FallCount=zeros(1,length(FallSW));
for ii=1:length(Fall)
    for yy=1:length(FallCount)
        if Fall(ii,yy)>0
            FallCount(1,yy)=FallCount(1,yy)+1;
        end
    end
end
clear ii yy

```

### Plotting

```{r larval-diversity, fig.cap="Ichthyoplankton Shannon diversity in the spring (A) and fall (B) in the Northeast Large Marine Ecosystem.",echo = T, fig.show='hold', fig.align='default', warning = F, message = F,fig.pos='H'}
# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4))

soe.plot(SOE.data, "Time", "Spring_Ich_Shannon Diversity Index", stacked = "A",
         rel.y.num = 1.1, end.start = 2007, full.trend = F,
         cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "Fall_Ich_Shannon Diversity Index", stacked = "B",
         rel.y.num = 1.1, end.start = 2007, full.trend = F,
         cex.stacked = 1.5)

soe.stacked.axis("Year", "Shannon Index", y.line = 2.5)

```

<!--chapter:end:chapters/Ich_div_indicator.Rmd-->

# Species Distribution Indicators


```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Species mean depth, along-shelf distance, and distance to coastline

**Indicator category**: Extensive analysis; not yet published

**Contributor(s)**: Kevin Friedland
  
**Data steward**: Kevin Friedland, <kevin.friedland@noaa.gov>
  
**Point of contact**: Kevin Friedland, <kevin.friedland@noaa.gov>
  
**Public availability statement**: Source data are available upon request (read more [here](https://inport.nmfs.noaa.gov/inport/item/22560)). Derived data may be downloaded [here](https://comet.nefsc.noaa.gov/erddap/tabledap/SOE_habitat_soe_v1.html).


## Methods
Three metrics quantifying spatial-temporal distribution shifts within fish populations were developed by @Friedland2018, including mean depth, along-shelf distance, and distance to coastline. Along-shelf distance is a metric for quantifying the distribution of a species through time along the axis of the US Northeast Continental Shelf, which extends northeastward from the Outer Banks of North Carolina. Values in the derived time series correspond to mean distance in km from the southwest origin of the along-shelf axis at 0 km. The along-shelf axis begins at 76.53&deg;W 34.60&deg;N and terminates at 65.71&deg;W 43.49&deg;N. 

Once mean distance is found, depth of occurrence and distance to coastline can be calculated for each species' positional center. Analyses present in the State of the Ecosystem (SOE) reports include mean depth and along-shelf distance for Atlantic cod, sea scallop, summer flounder, and black sea bass. 


### Data sources
Data for these indicators were derived from fishery-independent bottom trawl survey data collected by the Northeast Fisheries Science Center (NEFSC). 


<!-- ### Data extraction  -->


### Data analysis

R code:
```{r, echo = T, eval = F}

library(raster)
library(ncdf4)
library(stats)
library(geosphere)
library(plyr)

# set wd C:\1_analyses_ne_shelf\along shelf pos
#setwd(choose.dir(default=getwd()))
setwd("C:/1_analyses_ne_shelf/along shelf pos")
wd=getwd()


# name of survey data file select season <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
surfilename="Survdat_8_2017.Rdata"

# select season SPRING <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
selseaon="SPRING"
outfile="dhdc_8_2017_fallASS_sprDATA.csv"
outfile="dhdc_8_2017_fallASS_sprDATA jc.csv"
outfile="dhdc_8_2017_fallASS_sprDATA joe.csv"
outfile="dhdc_8_2017_fallASS_sprDATA all.csv"

# select season FALL <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
selseaon="FALL"
#outfile="dhdc_1_2017_sprASS_fallDATA.csv"
outfile="dhdc_1_2017_sprASS_fallDATA jc.csv"
outfile="dhdc_1_2017_sprASS_fallDATA joe.csv"
outfile="dhdc_1_2017_sprASS_fallDATA all.csv"


# read species list  sps.csv
#sps=read.csv(file.choose(), header = TRUE)
sps=read.csv(file="sps.csv", header = TRUE)
sps=read.csv(file="sps 312.csv", header = TRUE)
sps=read.csv(file="sps_joe.csv", header = TRUE)
sps=read.csv(file="sps_spring.csv", header = TRUE)
sps=read.csv(file="sps_fall.csv", header = TRUE)
numsps=nrow(sps)


# select species to analyze <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
spptokeep=sps$SVSPP
#spptokeep=c(73,74,75,76,77)
#spptokeep=c(312)  #Jonah Crab	312


# read in depth grid
gdepth=raster("nes_bath_data.nc", band=1)

# read in coordinates for along shelf diagnal  diag.csv
#diag=read.csv(file.choose(), header = TRUE)
diag=read.csv(file="diag.csv", header = TRUE)

# read in coordinate for coast     nes_coastline.csv
#nescoast=read.csv(file.choose(), header = TRUE)
nescoast=read.csv(file="nes_coastline.csv", header = TRUE)

# read in coordinate for coast     nes_coast_2.csv
#nescoast2=read.csv(file.choose(), header = TRUE)
nescoast2=read.csv(file="nes_coast_2.csv", header = TRUE)

# read in coordinate for coast V2     hersey_high_2.csv
#nesc2=read.csv(file.choose(), header = TRUE)
nesc2=read.csv(file="hersey_high_2.csv", header = TRUE)

# constants
radt=pi/180
R <- 6371 # Earth mean radius [km]




# CODE TO READ IN STRATA COMPUTE AREAS, NOW JUST READ IN STRATAREAS dataframe
# readin in strata.shp and compute areas of strata
#TrawlStrata<-shapefile(file.choose())
#plot(TrawlStrata)
#AREA<-areaPolygon(TrawlStrata, r=6371000)/10^6

# for array of strata and area and make into dataframe
#stratareas=cbind(TrawlStrata@data$STRATA, AREA)
#colnames(stratareas) <- c("STRATA","AREA")
#stratareas=data.frame(stratareas)
#save(stratareas, file="stratareas.rdata")

# load stratareas
load("stratareas.rdata")


# get durvey datafile  Survdat.RData
#load(file.choose())
#load(file="Survdat.RData")
load(file=surfilename)

# trin the data.... need to choose season
retvars <- c("CRUISE6","STATION","STRATUM","SVSPP","YEAR","SEASON","LAT","LON","DEPTH","ABUNDANCE","BIOMASS")
survdat <- survdat[retvars]  		
survdat <- survdat[(survdat$SEASON==selseaon),]

# stata to use
# offshore strata to use
CoreOffshoreStrata<-c(seq(1010,1300,10),1340, seq(1360,1400,10),seq(1610,1760,10))
# inshore strata to use, still sampled by Bigelow
CoreInshore73to12=c(3020, 3050, 3080 ,3110 ,3140 ,3170, 3200, 3230, 3260, 3290, 3320, 3350 ,3380, 3410 ,3440)
# combine
strata_used=c(CoreOffshoreStrata,CoreInshore73to12)

# find records to keep based on core strata
rectokeep=survdat$STRATUM %in% strata_used

#table(rectokeep)
# add rec to keep to survdat
survdat=cbind(survdat,rectokeep)

# delete record form non-core strata
survdat=survdat[!survdat$rectokeep=="FALSE",]

# get rid of species
survdat$rectokeep=survdat$SVSPP %in% spptokeep
survdat=survdat[!survdat$rectokeep=="FALSE",]


# find unique tow records only, since length and weight removed
# unique deletes to one record per species
survdat <- unique(survdat)

# add field with rounded BIOMASS scaler used to adjust distributions
survdat$LOGBIO <- round(log10(survdat$BIOMASS * 10+10))
# take a look go from 1 to 5?
table(survdat$LOGBIO)

# trim the data.... to prepare to find stations only
retvars <- c("CRUISE6","STATION","STRATUM","YEAR")
survdat_stations <- survdat[retvars]  		
# unique reduces to a record per tow
survdat_stations <- unique(survdat_stations)


# make table of strata by year
numtowsstratyr=table(survdat_stations$STRATUM,survdat_stations$YEAR)

# find records to keep based on core strata
rectokeep=stratareas$STRATA %in% strata_used

# add rec to keep to survdat
stratareas=cbind(stratareas,rectokeep)

# delete record form non-core strata
stratareas_usedonly=stratareas[!stratareas$rectokeep=="FALSE",]

# creat areapertow
areapertow=numtowsstratyr

#compute area covered per tow per strata per year
for(i in 1:47){
  areapertow[,i]=stratareas_usedonly$AREA/numtowsstratyr[,i]
}

# change inf to NA and round and out in DF
areapertow[][is.infinite(areapertow[])]=NA
areapertow=round(areapertow)
areapertow=data.frame(areapertow)
colnames(areapertow) <- c("STRATA","YEAR","AREAWT")

# add col to survdat for strata weight
survdat$AREAPERTOW=NA

#fill AREAPERTOW
dimsurvdat=dim(survdat)
for (i in 1:dimsurvdat[1]){
  survdat$AREAPERTOW[i]=areapertow$AREAWT[which(survdat$STRATUM[i]==areapertow$STRATA & survdat$YEAR[i]==areapertow$YEAR)]
}

table(ceiling(survdat$AREAPERTOW/1000))
table(survdat$LOGBIO)
table(ceiling(survdat$AREAPERTOW/1000*survdat$LOGBIO/9))

# add col to survdat for PLOTWT
survdat$PLOTWT=NA
survdat$PLOTWT= ceiling(survdat$AREAPERTOW/1000*survdat$LOGBIO/9)

table(survdat$PLOTWT)

# Plot stations
plot(survdat$LON[survdat$YEAR==1974],survdat$LAT[survdat$YEAR==1974])


# put in shorter name
sdat=survdat

# clear some space
remove(survdat)

# number of records to evaluate
numrecs=nrow(sdat)


#======================================================================================

# TAKEN OUT SINCE THE SAME AS GASDIST

#blank array for ASDIST
#d = array(data = NA, dim =  nrow(diag))

# block to calculate diag distance  ASDIST
#for (j in 1:numrecs) {
#   print(numrecs-j)
# 
#  lat1=sdat$LAT[j]* radt
#  long1=sdat$LON[j]* radt
#  for (i in 1:150){
#    lat2=diag$LAT[i]* radt
#    long2=diag$LON[i]* radt
#    d[i] <- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R
#  }
#  dindex=which(d==min(d))
##  
#  lat1=34.60* radt
#  long1=-76.53* radt
#  
#  lat2=diag$LAT[dindex]* radt
#  long2=diag$LON[dindex]* radt
#  sdat$ASDIST[j] = acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R
#}


#======================================================================================

# TAKEN OUT SINCE THE SAME AS GDTOC

#blank array for DTOC
#d = array(data = NA, dim = nrow(nescoast))

# block to calculate diag distance  DTOC
#for (j in 1:numrecs) {
##  print(numrecs-j)
  
#  lat1=sdat$LAT[j]* radt
#  long1=sdat$LON[j]* radt
#  for (i in 1:nrow(nescoast)){
#    lat2=nescoast$LAT[i]* radt
#    long2=nescoast$LON[i]* radt
 #   d[i] <- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R
#  }
#  sdat$DTOC[j] = d[which(d==min(d))]
  
#}



#======================================================================================
print("distance to coast using geosphere")

####  Geosphere package to calc distance to coastline from pts (lon,lat), returns meters
dd = array(data = NA, dim = nrow(sdat))
pts = data.frame(sdat$LON, sdat$LAT)
#line = t(rbind(nescoast$Longitude, nescoast$Latitude))
#line = t(rbind(nesc2$LON, nesc2$LAT))
line_nescoast2 = t(rbind(nescoast2$LON, nescoast2$LAT))

#dd=dist2Line(pts[,], line)
dd=dist2Line(pts[,], line_nescoast2)
sdat$GDTOC=dd/1000 # convert meters to KM

# TESTING (look at sdat to compare dtc (geosphere) to DTOC (from loop))
# plot(line)
# lines(diag)
# lines(line)
# points(nescoast)
#ddtest=data.frame(dd)
#ddtest$distance=NULL
#plot(nescoast2)
#lines(nescoast2)
#ptt=105
#points(sdat$LON[ptt], sdat$LAT[ptt], col="red"); sdat$DTOC[ptt]; sdat$dtc[ptt]; points(ddtest[ptt,], col="green")

#======================================================================================

print("diag distance using geosphere")
# Find distance to diagonal line (diag), use coordinates of nearest point to find distance to NC outerbanks (min(diag))
dd2 = array(data = NA, dim = nrow(sdat))
dd2 = dist2Line(pts[,], diag, distfun=distHaversine)
#Distance of closest point to data along diag line to NC coast
p1 = diag[1,] #start of line
p3 = diag[150,] #end of line
p2 = data.frame(dd2[,2], dd2[,3])
distNC = distCosine(p1, p2, r=6378137) /1000 # convert to KM (Great circle distance)
sdat$GASDIST = distNC


#======================================================================================


# create column for missing depth data intially with depth data
sdat$MISDEPTH=sdat$DEPTH

# find cases with missing depth data
missingdepth=which(is.na(sdat$DEPTH))

# fill only those records in misdepth with depth from grid
for(k in missingdepth){
  sdat$MISDEPTH[k] = extract(gdepth,cbind(sdat$LON[k],sdat$LAT[k])) * -1
}

#=========================================================================================




#outline=paste("YR",",","SVSPP",",","mASDIST",",","mDTOC",",","mMISDEPTH",",","mLAT",",","mLON",",","mGASDIST",",","mGDTOC")
#write.table(outline,file=outfile,row.name=F,col.names=F,append=TRUE)



out_data=array(NA,c((max(sdat$YEAR)-min(sdat$YEAR)+1)*numsps,7))

row_c=0

for (i in 1:numsps){
  print (i)
  for(j in min(sdat$YEAR):max(sdat$YEAR)){
 
    row_c=row_c+1   
#sumdist=sum(sdat$ASDIST[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
#lendist=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
#mASDIST =sumdist / lendist

#sumdist=sum(sdat$DTOC[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
#lendist=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
#mDTOC =sumdist / lendist

sumdepth=sum(sdat$MISDEPTH[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
lendepth=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
mMISDEPTH =sumdepth / lendepth

sumdepth=sum(sdat$LAT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
lendepth=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
mLAT =sumdepth / lendepth

sumdepth=sum(sdat$LON[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
lendepth=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
mLON =sumdepth / lendepth


sumdepth=sum(sdat$GASDIST[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
lendepth=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
mGASDIST =sumdepth / lendepth

sumdepth=sum(sdat$GDTOC[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
lendepth=sum(sdat$PLOTWT[sdat$YEAR==j & sdat$SVSPP==sps$SVSPP[i]])
mGDTOC =sumdepth / lendepth


out_data[row_c,1]=j
out_data[row_c,2]=sps$SVSPP[i]
out_data[row_c,3]=mMISDEPTH
out_data[row_c,4]=mLAT
out_data[row_c,5]=mLON
out_data[row_c,6]=mGASDIST
out_data[row_c,7]=mGDTOC



  }  
}


#outline=paste(j,",",sps$SVSPP[i],",",mMISDEPTH,",",mLAT,",",mLON,",",mGASDIST,",",mGDTOC)
#write.table(outline,file=outfile,row.name=F,col.names=F,append=TRUE)

out_data=data.frame(out_data)



names(out_data)[names(out_data)=="X1"] <- "YR"
names(out_data)[names(out_data)=="X2"] <- "SP"
names(out_data)[names(out_data)=="X3"] <- "DEPTH"
names(out_data)[names(out_data)=="X4"] <- "LAT"
names(out_data)[names(out_data)=="X5"] <- "LON"
names(out_data)[names(out_data)=="X6"] <- "ASDIST"
names(out_data)[names(out_data)=="X7"] <- "DTEOC"


write.csv(out_data,file=outfile )

```

### Plotting

```{r asd-bsb,fig.cap="Black sea bass along-shelf distance trends in spring (A) and fall (B).", echo = T, fig.show='hold', warning = F, message = F,fig.pos='H'}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4))

soe.plot(SOE.data, "Time", "black sea bass spring mean along-shelf dist", 
         stacked = "A", rel.y.num = 1.1, suppressAxis = T, 
         line.forward = T, tol = 0.15, x.start = 1963, end.start = 2007,
         cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "black sea bass fall mean along-shelf dist", 
         stacked = "B", rel.y.num = 1.1, tol = 0.15, end.start = 2007,
         cex.stacked = 1.5)
soe.stacked.axis("Year", "Mean along-shelf distance, km", y.line = 3.0)

```


```{r mean-depth-bsb,fig.cap="Mean depth of black sea bass in spring (A) and fall (B) on the Northeast Continental Shelf.", echo = T, fig.show='hold', warning = F, message = F,fig.pos='H'}


opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4))

soe.plot(SOE.data, "Time", "black sea bass spring mean depth", 
         stacked = "A", rel.y.num = 1.1, suppressAxis = T, 
          tol = 0.15, x.start = 1963, end.start = 2007,
         cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "black sea bass fall mean depth", 
         stacked = "B", rel.y.num = 1.1, tol = 0.15, end.start = 2007,line.forward = T,
         cex.stacked = 1.5)
soe.stacked.axis("Year", "Mean depth, m", y.line = 3.0)

```

<!--chapter:end:chapters/Species_dist_indicators.Rmd-->

# Species Density Estimates

**Description**: Current and Historical Species Distributions

**Indicator category**: Database pull; Database pull with analysis

**Contributor**: Kevin Friedland

**Data steward**: Kevin Friedland

**Point of contact**: Kevin Friedland, kevin.friedland@noaa.gov

**Public availability statement**: Source data are publicly available.


## Methods
We used kernel density plots to depict shifts in species' distributions over time. These figures characterize the probability of a species occurring in a given area based on NEFSC Bottom Trawl Survey data. Kernel density estimates (KDEs) of distributions are shown for the period of 1970-1979 (shaded blue) and most recent three years of survey data (shaded red) (e.g. Figure \@ref(fig:kde-fig)). Results are typically visualized for spring and fall bottom trawl surveys seperately. 

Three probability levels (25%, 50%, 75%) are shown for each time period, where the 25% region depicts the core area of the distribution and the 75% region shows the area occupied more broadly by the species. A wide array of KDEs for many ecologically and economically important species on the Northeast US Continental Shelf are available [here](https://www.nefsc.noaa.gov/ecosys/current-conditions/kernel-density.html).

### Data sources
Current and historical species distributions are based on the NEFSC Bottom Trawl Survey data (aka ["Survdat"](#survdat)) and depth strata. Strata are available as shapefiles that can be downloaded  [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis) (listed as 
"strata.shp"). 

### Data analysis

<!--Include accompanying R code, pseudocode, flow of scripts, and/or link to location of code used in analyses.-->
```{r, echo = T, message= F, warning=F, include=T, eval = T}
library(ecodata)
library(maps)
library(mapdata)
library(ks)
library(marmap)
library(raster)
library(geosphere)
library(dplyr)

#  plot ks plots

#----STUFF TO SET-----------------
data.dir <- here::here("data")
gis.dir <- here::here("gis")

# Gives most recent period. 2015-2019 input is 2016-2018 data
rminyr <- 2015
rmaxyr <- 2019

# tlevel is density of color for KD contours areas
tlevel=75  # move later
color_b="blue"
color_r="orange3"
color_r="tomato3"

# Code to read in strata and compute areas. Or read from cache.

# readin in strata.shp and compute areas of strata
# TrawlStrata <- raster::shapefile(file.path(gis.dir,"BTS_Strata.shp"))
# AREA <- geosphere::areaPolygon(TrawlStrata, r=6371000)/10^6

# for array of strata and area and make into dataframe
# stratareas <- cbind(TrawlStrata@data$STRATA, AREA)
# colnames(stratareas) <- c("STRATA","AREA")
# stratareas <- data.frame(stratareas)
load(file.path(data.dir, "StratAreas.Rdata"))

# Query bathymetry or load from cache
# getNOAA.bathy(lon1 = -77, lon2 = -65, lat1 = 35, lat2 = 45,
#               resolution = 10) -> nesbath
load(file.path(data.dir, "nesbath.Rdata"))

#Load raw survey data
load(file.path(data.dir, "Survdat.RData"))

# MUST run addTrans function
# color transparency
addTrans <- function(color,trans){
  # This function adds transparancy to a color.
  # Define transparancy with an integer between 0 and 255
  # 0 being fully transparant and 255 being fully visable
  # Works with either color and trans a vector of equal length,
  # or one of the two of length 1.
  
  if (length(color)!=length(trans)&!any(c(length(color),length(trans))==1)) stop("Vector lengths not correct")
  if (length(color)==1 & length(trans)>1) color <- rep(color,length(trans))
  if (length(trans)==1 & length(color)>1) trans <- rep(trans,length(color))
  
  num2hex <- function(x)
  {
    hex <- unlist(strsplit("0123456789ABCDEF",split=""))
    return(paste(hex[(x-x%%16)/16+1],hex[x%%16+1],sep=""))
  }
  rgb <- rbind(col2rgb(color),trans)
  res <- paste("#",apply(apply(rgb,2,num2hex),2,paste,collapse=""),sep="")
  return(res)
}

plot_kd <- function(species, season, exclude_years){

  # stata to use
  # offshore strata to use
  CoreOffshoreStrata <- c(seq(1010,1300,10),1340, seq(1360,1400,10),seq(1610,1760,10))
  
  # inshore strata to use, still sampled by Bigelow
  CoreInshore73to12 <- c(3020, 3050, 3080 ,3110 ,3140 ,3170, 3200, 3230,
                         3260, 3290, 3320, 3350 ,3380, 3410 ,3440)
  # combine
  strata_used <- c(CoreOffshoreStrata,CoreInshore73to12)
  
  survdat <- survdat %>%
    dplyr::select(c(CRUISE6,STATION,STRATUM,SVSPP,YEAR,
                    SEASON,LAT,LON,ABUNDANCE,BIOMASS)) %>% 
    filter(SEASON == season,
           STRATUM %in% strata_used) %>% # delete record form non-core strata and get unique records,
    # should be one per species
    distinct() %>% 
    # add field with rounded BIOMASS scaler used to adjust distributions
    mutate(LOGBIO = round(log10(BIOMASS * 10+10)))

  # trim the data....to prepare to find stations only
  survdat_stations <- survdat %>% 
    dplyr::select(CRUISE6, STATION, STRATUM, YEAR) %>% 
    distinct()
  
  # make table of strata by year
  numtowsstratyr <- table(survdat_stations$STRATUM,survdat_stations$YEAR)
  
  # find records to keep based on core strata
  rectokeep <- stratareas$STRATA %in% strata_used
  
  # add rec to keep to survdat
  stratareas <- cbind(stratareas,rectokeep)
  
  # delete record form non-core strata
  stratareas_usedonly <- stratareas[!stratareas$rectokeep=="FALSE",]
    
  areapertow=numtowsstratyr
  
  #compute area covered per tow per strata per year
  for(i in 1:50){
  areapertow[,i]=stratareas_usedonly$AREA/numtowsstratyr[,i]
  }
  
  # change inf to NA and round and out in DF
  areapertow[][is.infinite(areapertow[])]=NA
  areapertow=round(areapertow)
  areapertow=data.frame(areapertow)
  colnames(areapertow) <- c("STRATUM","YEAR","AREAWT")
  areapertow$STRATUM <- as.numeric(as.character(areapertow$STRATUM))
  areapertow$YEAR <- as.numeric(as.character(areapertow$YEAR))
    
  survdat <- survdat %>% 
    inner_join(.,areapertow, by= c("STRATUM","YEAR")) %>% 
    dplyr::rename(AREAPERTOW = AREAWT)
  
  # add col to survdat for PLOTWT
  survdat$PLOTWT <- NA
  survdat$PLOTWT <- ceiling(survdat$AREAPERTOW/1000*survdat$LOGBIO/9)
  
  if (!is.null(exclude_years)){
    sdat <- survdat %>% filter(!YEAR %in% exclude_years)
  } else {
    sdat <- survdat
  }
  
  # read species list
  sps <- ecodata::species_groupings %>% filter(!is.na(SVSPP)) %>% dplyr::select(COMNAME, SVSPP)
  sps <- sps[!duplicated(sps),]
  numsps <- nrow(sps)
  
  # graph par
  par(mar = c(0,0,0,0))
  par(oma = c(0,0,0,0))
  
  # index 1:numsps, or by species record number for one species, i.e.25:25
  tspe <- sps %>% filter(COMNAME == species)

  # start map
  map("worldHires", xlim=c(-77,-65),ylim=c(35,45), fill=T,border=0,col="gray")
  map.axes()

  plot(nesbath,deep=-200, shallow=-200, step=1,add=T,lwd=1,col="gray50",lty=2)


  # for base period, 1970 to 1979, find call lons for species and by biomass weighting
  minyr=1969;maxyr=1980
  clons1 = sdat$LON[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==1)]
  clons2 = sdat$LON[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==2)]
  clons3 = sdat$LON[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==3)]
  clons4 = sdat$LON[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==4)]
  clons5 = sdat$LON[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==4)]
  
  # get rid of missings, KS does not like
  clons1 <- na.omit(clons1)
  clons2 <- na.omit(clons2)
  clons3 <- na.omit(clons3)
  clons4 <- na.omit(clons4)
  clons5 <- na.omit(clons5)
  
  # accumulate all lons, repeating for weighting 
  clons=c(clons1,clons2,clons2,clons3,clons3,clons3,clons4,clons4,clons4,clons4,
          clons5,clons5,clons5,clons5,clons5)

  # same for lats
  clats1 = sdat$LAT[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==1)]
  clats2 = sdat$LAT[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==2)]
  clats3 = sdat$LAT[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==3)]
  clats4 = sdat$LAT[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==4)]
  clats5 = sdat$LAT[(sdat$YEAR>minyr & sdat$YEAR<maxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==5)]
  clats1 <- na.omit(clats1)
  clats2 <- na.omit(clats2)
  clats3 <- na.omit(clats3)
  clats4 <- na.omit(clats4)
  clats5 <- na.omit(clats5)
  clats=c(clats1,clats2,clats2,clats3,clats3,clats3,clats4,clats4,clats4,clats4,
          clats5,clats5,clats5,clats5,clats5)

  # combine lons and lats
  x=cbind(clons,clats)
  # compute KD using KS routine
  Hscv1 <- Hscv.diag(x=x)
  #fhat.pi1 <- kde(x=x, H=Hscv1)
  
  fhat.pi1 <- kde(x, compute.cont=T, binned=F, xmin=c(-77, 35), xmax=c(-65, 45)) # specify grid to match raster stack of OISST... etc.

  # add to plot each probability separately
  contour.25 <- with(fhat.pi1, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont["25%"]))
  contour.50 <- with(fhat.pi1, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont["50%"]))
  contour.75 <- with(fhat.pi1, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont["75%"]))
  
  for (j in 1:length(contour.75)){
    polygon(unlist(contour.75[[j]][2]), unlist(contour.75[[j]][3]),col=addTrans(color_b,tlevel), border=F)
  }
  for (j in 1:length(contour.50)){
    polygon(unlist(contour.50[[j]][2]), unlist(contour.50[[j]][3]),col=addTrans(color_b,tlevel), border=F)
  }
  for (j in 1:length(contour.25)){
    polygon(unlist(contour.25[[j]][2]), unlist(contour.25[[j]][3]),col=addTrans(color_b,tlevel), border=F)
  }
  
  clons1 = sdat$LON[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==1)]
  clons2 = sdat$LON[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==2)]
  clons3 = sdat$LON[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==3)]
  clons4 = sdat$LON[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==4)]
  clons5 = sdat$LON[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==4)]
  # get rid of missings, KS does not like
  clons1 <- na.omit(clons1)
  clons2 <- na.omit(clons2)
  clons3 <- na.omit(clons3)
  clons4 <- na.omit(clons4)
  clons5 <- na.omit(clons5)
  # accumulate all lons, repeating for weighting 
  clons=c(clons1,clons2,clons2,clons3,clons3,clons3,clons4,clons4,clons4,clons4,
          clons5,clons5,clons5,clons5,clons5)
  
  # same for lats
  clats1 = sdat$LAT[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==1)]
  clats2 = sdat$LAT[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==2)]
  clats3 = sdat$LAT[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==3)]
  clats4 = sdat$LAT[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==4)]
  clats5 = sdat$LAT[(sdat$YEAR>rminyr & sdat$YEAR<rmaxyr & sdat$SVSPP==tspe$SVSPP & sdat$PLOTWT ==5)]
  clats1 <- na.omit(clats1)
  clats2 <- na.omit(clats2)
  clats3 <- na.omit(clats3)
  clats4 <- na.omit(clats4)
  clats5 <- na.omit(clats5)
  clats=c(clats1,clats2,clats2,clats3,clats3,clats3,clats4,clats4,clats4,clats4,
          clats5,clats5,clats5,clats5,clats5)
  
  x=cbind(clons,clats)
  Hscv2 <- Hscv.diag(x=x)


  #fhat.pi2 <- kde(x=x, H=Hscv2)
  fhat.pi2 <- kde(x, compute.cont=T, binned=F, xmin=c(-77, 35), xmax=c(-65, 45)) # specify grid to match raster stack of OISST... etc.
  
  # add to plot each probability separately
  contour.25 <- with(fhat.pi2, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont["25%"]))
  contour.50 <- with(fhat.pi2,contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont["50%"]))
  contour.75 <- with(fhat.pi2, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont["75%"]))
  
  for (j in 1:length(contour.75)){
    polygon(unlist(contour.75[[j]][2]), unlist(contour.75[[j]][3]),col=addTrans(color_r,tlevel), border=F)
  }
  for (j in 1:length(contour.50)){
    polygon(unlist(contour.50[[j]][2]), unlist(contour.50[[j]][3]),col=addTrans(color_r,tlevel), border=F)
  }
  for (j in 1:length(contour.25)){
    polygon(unlist(contour.25[[j]][2]), unlist(contour.25[[j]][3]),col=addTrans(color_r,tlevel), border=F)
  }


  text(-70,37.5, pos=4,labels = species)
  segments(-69.5, 37,-68.5, 37,lwd=1,col="gray50",lty=2)
  text(-68.5,37, pos=4,labels = "200m")

  stline=36.5
  text(-70.4,stline, pos=4,labels = "25%    50%    75%")
  incline=-.3
  segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_b,tlevel))
  segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_b,tlevel))
  segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_b,tlevel))
  text(-66.8,stline+incline, pos=4,labels = "Base")

  incline=-.7
  segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_r,tlevel))
  segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_r,tlevel))
  segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_r,tlevel))
  text(-66.8,stline+incline, pos=4,labels = "Recent")

  incline=-1.1 
  segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_b,tlevel))
  segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_b,tlevel))
  segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_b,tlevel))
  segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_r,tlevel))
  segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_r,tlevel))
  segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_r,tlevel))
  text(-66.8,stline+incline, pos=4,labels = "Overlap")

} 


```

### Plotting

```{r kde-fig, fig.cap="Current and historical sea scallop kernel density estimates derived from spring survey data. Current estimates derived from 2016-2018 data.", echo = T, eval = T}
plot_kd(species = "SEA SCALLOP", season = "SPRING", exclude_years = NULL)
```



<!--chapter:end:chapters/Species_density_estimates.Rmd-->

# Thermal Habitat Projections


```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Species Thermal Habitat Projections

**Indicator category**: Published methods

**Contributor(s)**: Vincent Saba
  
**Data steward**: Vincent Saba, <vincent.saba@noaa.gov>
  
**Point of contact**: Vincent Saba, <vincent.saba@noaa.gov>
  
**Public availability statement**: Source data are available to the public. Model outputs for thermal habitat projections are available [here](https://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000).


## Methods
This indicator is based on work reported in @Kleisner2017.

### Data sources

#### Global Climate Model Projection
We used [NOAA GFDL's CM2.6 simulation](https://www.gfdl.noaa.gov/high-resolution-climate-modeling/) consisting of (1) a 1860 pre-industrial control, which brings the climate system into near-equilibrium with 1860 greenhouse gas concentrations, and (2) a transient climate response (2xCO2) simulation where atmospheric CO2 is increased by 1% per year, which results in a doubling of CO2 after 70 years. The climate change response from CM2.6 was based on the difference between these two experimental runs. Refer to @Saba2016 for further details. 

#### Modeling Changes in Suitable Thermal Habitat
The NOAA NEFSC U.S. Northeast Shelf (NES) bottom trawl survey, which has been conducted for almost 50-years in the spring and fall, provides a rich source of data on historical and current marine species distribution, abundance, and habitat, as well as oceanographic conditions [@Azarovitz1981]. The survey was implemented to meet several objectives: (1) monitor trends in abundance, biomass, and recruitment, (2) monitor the geographic distribution of species, (3) monitor ecosystem changes, (4) monitor changes in life history traits (e.g., trends in growth, longevity, mortality, and maturation, and food habits), and (5) collect baseline oceanographic and environmental data. These data can be leveraged for exploring future changes in the patterns of abundance and distribution of species in the region. 

<!-- ### Data extraction -->


### Data analysis
#### Global Climate Model Projection
The CM2.6 80-year projections can be roughly assigned to a time period by using the IPCC Representative Concentration Pathways (RCPs), which describe four different 21st century pathways of anthropogenic greenhouse gas emissions, air pollutant emissions, and land use [@IPCC2014]. There are four RCPs, ranging from a stringent mitigation scenario (RCP2.6), two intermediate scenarios (RCP4.5 and RCP6.0), and one scenario with very high greenhouse gas emissions (RCP8.5). For RCP8.5, the global average temperature at the surface warms by 2C by approximately 2060-2070 relative to the 1986-2005 climatology (see Figure SPM.7a in [IPCC, 2013](https://www.ipcc.ch/pdf/assessment-report/ar5/wg1/WG1AR5_SPM_FINAL.pdf)). For CM2.6, the global average temperature warms by 2C by approximately years 60-80 (see Fig. 1 in @winton_has_2014). Therefore, the last 20 years of the transient climate response simulation roughly corresponds to 2060-2080 of the RCP8.5 scenario. 

Here, the monthly differences in surface and bottom temperatures ('deltas') for spring (February-April) and fall (September- November) are added to an average annual temperature climatology for spring and fall, respectively, derived from observed surface and bottom temperatures to produce an 80-year time series of future bottom and surface temperatures in both seasons. The observed temperatures come from the NEFSC spring and fall bottom trawl surveys conducted from 1968 to 2013 and represent approximately 30,000 observations over the time series. 


#### Modeling Changes in Suitable Thermal Habitat
We modeled individual species thermal habitat across the whole U.S. NES and not by sub-region because we did not want to assume that species would necessarily maintain these assemblages in the future. Indeed, the goal here is to determine future patterns of thermal habitat availability for species on the U.S. NES in more broad terms. We fit one GAM based on both spring and fall data (i.e., an annual model as opposed to separate spring and fall models) and use it to project potential changes in distribution and magnitude of biomass separately for each season for each species. By creating a single annual model based on temperature data from both spring and fall, we ensure that the full thermal envelope of each species is represented. For example, if a species with a wide thermal tolerance has historically been found in cooler waters in the spring, and in warmer waters in the fall, an annual model will ensure that if there are warmer waters in the spring in the future, that species will have the potential to inhabit those areas. Additionally, because the trawl survey data are subject to many zero observations, we use delta-lognormal GAMs [@Wood2011a], which model presence-absence separately from logged positive observations. The response variables in each of the GAMs are presence/absence and logged positive biomass of each assemblage or individual species, respectively. A binomial link function is used in the presence/absence models and a Gaussian link function is used in the models with logged positive biomass. 
The predictor variables are surface and bottom temperature and depth (all measured by the survey at each station), fit with penalized regression splines, and survey stratum, which accounts for differences in regional habitat quality across the survey region. Stratum may be considered to account for additional information not explicitly measured by the survey (e.g., bottom rugosity). Predictions of species abundance are calculated as the product of the predictions from the presence-absence model, the exponentiated predictions from the logged positive biomass model, and a correction factor to account for the retransformation bias associated with the log transformation [@Duan1983; and see @Pinsky2013]. 

We calculated the suitable thermal habitat both in terms of changes in 'suitable thermal abundance', defined as the species density possible given appropriate temperature, depth and bathymetric conditions, and changes in 'suitable thermal area', defined as the size of the physical area potentially occupied by a species given appropriate temperature, depth and bathymetric conditions. Suitable thermal abundance is determined from the predictions from the GAMs (i.e., a prediction of biomass). However, this quantity should not be interpreted directly as a change in future abundance or biomass, but instead as the potential abundance of a species in the future given changes in temperature and holding all else (e.g., fishing effort, species interactions, productivity, etc.) constant. Suitable thermal area is determined as a change in the suitable area that a species distribution occupies in the future and is derived from the area of the kernel density of the distribution. To ensure that the estimates are conservative, we select all points with values greater than one standard deviation above the mean. We then compute the area of these kernels using the gArea function from the 'rgeos' package in R [@Bivand2011]. 

### Plotting

```{r th-maps,fig.cap="Current thermal habitat estimate (A), and 20-40 year thermal habitat projection (B) for summer flounder on the Northeast Continental Shelf." , fig.width = 8,fig.height = 5.5, fig.show='hold',fig.pos='H', warning=F, message=F}

#Source GIS script
r.dir <- here::here("R")
source(file.path(r.dir,"GIS_source.R"))


library(dplyr)
library(colorRamps)
library(rerddap)


#thermal habitat map function
th_plot <- function(data, letter, z.max, y.axis = T,
                    legend = T){
  
  data <- nc_open(file.path(gis.dir,data))
  
  #longitude
  lon <- ncvar_get(data, "xi", verbose = F)
  
  #latitude
  lat <- ncvar_get(data, "yi", verbose = F)
  
  #thermal habitat projection
  z <- ncvar_get(data, "zi")
  
  #combine in data.frame
  proj <- data.frame(lon = lon,
                     lat = lat,
                     z = z)
  
  proj <- proj %>% filter(z != "NA",z>0) %>% 
    mutate(z = plyr::mapvalues(z, from = (z[(z>z.max)]), to = rep(z.max,length(z[(z>z.max)]))))
  
  #turn dataframe to raster
  coordinates(proj) = ~lon+lat
  proj4string(proj)=map.crs # set it to lat-long
  proj = spTransform(proj,map.crs)
  proj <- proj[strata,]
  gridded(proj) = TRUE
  r = raster(proj)
  projection(r) = map.crs
  
  colors <- matlab.like(120)
  #plot maps
  if (legend == T){
    plot(r, col=colors, ylim=c(35,45),
         breaks=seq(0,z.max,length.out=length(colors)),
         zlim = c(0,z.max),
         legend.width = 2,
         axes=F, interpolate = T, las = 1,box = F,
         axis.args=list(at=seq(0, z.max, length.out = 5),
                        labels=round(seq(0,1,length.out = 5),1), 
                        cex.axis=0.8),
         legend.args=list(text=expression(paste("     Thermal \n      Habitat")),
                          side=3, font=2, line=1, cex=.65))
  } else {
    plot(r, col=colors, ylim=c(35,45),
         breaks=seq(0,z.max,length.out=length(colors)),
         axes=F, interpolate = F, las = 1,box = F,legend = F)
  }
  
  
  map("worldHires", xlim=c(-77,-65),ylim=c(35,45), fill=T,border=0,col="gray", add = T)
  
  if (y.axis == T){
    map.axes(cex.axis = .7)
  } else if (y.axis == F){
    axis(1,cex = .7)
    box(lty = 'solid', col = 'black',lwd = 1)
  }
  
  
  contour(th,drawlabels = F,nlevels = 2,add=T, col = "gray20",lty=2, lwd = 1)
  text(-75.5, 45.5, letter,cex = 1.5)
  
}

par(mfrow=c(1,2),mar=c(2.5,2,2,2), xpd=NA, mgp=c(3,1,0), oma=c(0,1,0,0), ann=F)
th_plot(data = "Summer Flounderfall_2.nc",
        letter = "A", z.max = 2.5, y.axis = T, legend = F)
th_plot(data = "Summer Flounderfall_4.nc",
        letter = "B", z.max = 2.5, y.axis = T, legend = T)

```


**Note**: The thermal habitat model output for all species presented in State of the Ecosystem reports is accessible through the [NEFSC ERDDAP server](https://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000).

<!--chapter:end:chapters/Thermal_hab_proj_indicator.Rmd-->

# Habitat Occupancy Models

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Habitat Occupancy

**Indicator category**: Database pull with analysis; Extensive analysis; not yet published; Published methods

**Contributor(s)**: Kevin Friedland
  
**Data steward**: Kevin Friedland, <kevin.friedland@noaa.gov>
  
**Point of contact**: Kevin Friedland, <kevin.friedland@noaa.gov>
  
**Public availability statement**: Source data are available upon request (see [Survdat](#survdat), [CHL/PP](#chl-pp), and Data Sources below for more information). Model-derived time series are available [here](https://comet.nefsc.noaa.gov/erddap/tabledap/SOE_habitat_soe_v1.html).


## Methods
Habitat area with a probability of occupancy greater than 0.5 was modeled for many [species throughout the NE-LME](https://www.nefsc.noaa.gov/ecosys/current-conditions/occupancy-change.html) using random forest decision tree models. 

### Data sources
Models were parameterized using a suite of static and dynamic predictor variables, with occurrence and catch per unit effort (CPUE) of species from spring and fall Northeast Fisheries Science Center (NEFSC) bottom trawl surveys (BTS) serving as response variables. Sources of variables used in the analyses are described below.

#### Station depth
The NEFSC BTS data included depth observations made concurrently with trawls at each station. Station depth was a static variable for these analyses. 

#### Ocean temperature and salinity
Sea surface and bottom water temperature and salinity measurements were included as dynamic predictor variables in the model, and were collected using Conductivity/Temperature/Depth (CTD) instruments. Ocean temperature and salinity measurements had the highest temporal coverage during the spring (February-April) and fall (September-November) months. Station salinity data were available between 1992-2016. 

#### Habitat descriptors
A variety of benthic habitat descriptors were incorporated as predictor variables in occupancy models (Table \@ref(tab:habitatdesc)). The majority of these parameters are based on depth (e.g. *BPI*, *VRM*, *Prcury*, *rugosity*, *seabedforms*, *slp*, and *slpslp*). The vorticity variable is based on current estimates, and the variable *soft_sed* based on sediment grain size. 


```{r habitatdesc, echo = F, results='asis', message=F, warning=F}
tab <- '
|Variables|Notes|References|
|:-----------------------|:-----------------------|:-----------------------|
|Complexity - Terrain Ruggedness Index|The difference in elevation values from a center cell and the eight cells immediately surrounding it. Each of the difference values are squared to make them all positive and averaged. The index is the square root of this average.|@Riley1999|

|Namera bpi|BPI is a second order derivative of the surface depth using the TNC Northwest Atlantic Marine Ecoregional Assessment ("NAMERA") data with an inner radius=5 and outer radius=50.|@Lundblad2006|

|Namera_vrm|Vector Ruggedness Measure (VRM) measures terrain ruggedness as the variation in three-dimensional orientation of grid cells within a neighborhood based the TNC Northwest Atlantic Marine Ecoregional Assessment ("NAMERA") data.|@Hobson1972; @Sappington2007|

|Prcurv (2 km, 10 km, and 20 km)|Benthic profile curvature at 2km, 10km and 20 km spatial scales was derived from depth data.|@Winship2018|

|Rugosity|A measure of small-scale variations of amplitude in the height of a surface, the ratio of the real to the geometric surface area.|@Friedman2012|

|seabedforms|Seabed topography as measured by a combination of seabed position and slope.|[http://www.northeastoceandata.org/](http://www.northeastoceandata.org/)|

|Slp (2 km, 10 km, and 20 km)|Benthic slope at 2km, 10km and 20km spatial scales.|@Winship2018|

|Slpslp (2 km, 10 km, and 20 km)|Benthic slope of slope at 2km, 10km and 20km spatial scales|@Winship2018|

|soft_sed|Soft-sediments is based on grain size distribution from the USGS usSeabed: Atlantic coast offshore surficial sediment data.|[http://www.northeastoceandata.org/](http://www.northeastoceandata.org/)|

|Vort (fall - fa; spring - sp; summer - su; winter - wi)|Benthic current vorticity at a 1/6 degree (approx. 19 km) spatial scale.|@Kinlan2016|

'
df<-readr::read_delim(tab, delim="|")
df<-df[-c(1,2,3) ,c("Variables","Notes","References")]
knitr::kable(
  df, booktabs = TRUE,
  caption = 'Habitat descriptors used in model parameterization.'
)
```

#### Zooplankton
Zooplankton data are acquired through the NEFSC Ecosystem Monitoring Program ("EcoMon"). For more information regarding the collection process for these data, see @Kane2007, @Kane2011, and @Morse2017. The bio-volume of the 18 most abundant zooplankton taxa were considered as potential predictor variables.  

#### Remote sensing data
Both chlorophyll concentration and SST from remote sensing sources were incorporated as static predictor variables in the model. During the period of 1997-2016, chlorophyll concentrations were derived from observations made by the Sea-viewing Wide Field of View Sensor (SeaWIFS), Moderate Resolution Imaging Spectroradiometer (MODIS-Aqua), Medium Resolution Imaging Spectrometer (MERIS), and Visible and Infrared Imaging/Radiometer Suite (VIIRS). 

### Data processing

#### Zooplankton
Missing values in the EcoMon time series were addressed by summing data over five-year time steps for each seasonal time frame and interpolating a complete field using ordinary kriging. Missing values necessitated interpolation for spring data in 1989, 1990, 1991, and 1994. The same was true of the fall data for 1989, 1990, and 1992.

#### Remote sensing data
An overlapping time series of observations from the four sensors listed above was created using a bio-optical model inversion algorithm [@Maritorena2010]. Monthly SST data were derived from MODIS-Terra sensor data (available [here](https://oceancolor.gsfc.nasa.gov/data/terra/)). 

#### Ocean temperature and salinity
Date of collection corrections for ocean temperature data were developed using linear regressions for the spring and fall time frames; standardizing to collection dates of April 3 and October 11 for spring and fall. No correction was performed for salinity data. Annual data for ocean temperature and salinity were combined with climatology by season through an optimal interpolation approach. Specifically, mean bottom temperature or salinity was calculated by year and season on a 0.5&deg; grid across the ecosystem, and data from grid cells with >80% temporal coverage were used to calculate a final seasonal mean. Annual seasonal means were then used to calculate combined anomalies for seasonal surface and bottom climatologies. 

An annual field was then estimated using raw data observations for a year, season, and depth using universal kriging [@automap], with depth included as a covariate (on a standard 0.1&deg; grid). This field was then combined with the climatology anomaly field and adjusted by the annual mean using the variance field from kriging as the basis for a weighted mean between the two. The variance field was divided into quartiles with the lowest quartile assigned a weighting of 4:1 between the annual and climatology values. The optimally interpolated field at these locations was therefore skewed towards the annual data, reflecting their proximity to actual data locations and associated low kriging variance. The highest kriging variance quartile (1:1) reflected less information from the annual field and more from the climatology.

### Data analysis

#### Occupancy models
Prior to fitting the occupancy models, predictor variables were tested for multi-collinearity and removed if found to be correlated. The final model variables were then chosen utilizing a model selection process as shown by @Murphy2010 and implemented with the R package rfUtilities [@rfUtilities-package]. Occupancy models were then fit as two-factor classification models (absence as 0 and presence as 1) using the randomForest R package [@randomForest]. 

#### Selection criteria and variable importance
The irr R package [@irr] was used to calculate Area Under the ROC Curve (AUC) and Cohen's Kappa for assessing accuracy of occupancy habitat models. Variable importance was assessed by plotting the occurrence of a variable as a root variable versus the mean minimum node depth for the variable [@randomForestExplainer], as well as by plotting the Gini index decrease versus accuracy decrease.  

### Plotting

```{r occupancy-MAB, fig.cap="Summer flounder spring (A) and fall (B) occupancy habitat area in the Northeast Large Marine Ecosystem. ", echo = T, fig.show='hold', warning = F, message = F,fig.pos='H'}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4))

soe.plot(SOE.data, "Time", "sumflo spring habitat occupancy", stacked = "A",
         rel.y.num = 1.1, scale.axis = 10^3, end.start = 2007, full.trend = F,
         cex.stacked = 1.5)
soe.plot(SOE.data, "Time", "sumflo fall habitat occupancy", stacked = "B",
         rel.y.num = 1.1, scale.axis = 10^3, end.start = 2007, full.trend = F,
         cex.stacked = 1.5)

soe.stacked.axis("Year", expression("Habitat Area, 10"^3*" km"^2), y.line = 2.5)

```


<!--chapter:end:chapters/occupancy_indicator.Rmd-->

# Verified Records of Southern Kingfish

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Fisheries Observer Data â€“ Verified Records of Southern Kingfish

**Indicator category**: Database pull 

**Contributor(s)**: Debra Duarte, Loren Kellogg
  
**Data steward**: Gina Shield, gina.shield@noaa.gov
  
**Point of contact**: Gina Shield, gina.shield@noaa.gov
  
**Public availability statement**: Due to PII concerns data for this indicator are not publicly available.



## Methods

### Data sources

The Fisheries Sampling Branch deploys observers on commercial fisheries trips from Maine to North Carolina. On observed tows, observers must fully document all kept and discarded species encountered. Observers must comply with a Species Verification Program (SVP), which requires photo or sample submissions of high priority species at least once per quarter. Photos and samples submitted for verification are identified independently by at least two reviewers.

The derived data presented in the Mid-Atlantic State of the Ecosystem report for southern kingfish include records verified by the SVP program only. The occurrence of southern kingfish in SVP records were chosen for inclusion in the report due to the recent increases of the species in SVP observer records since 2010. These data are not a complete list from the New England Fisheries Observer Program (NEFOP). Southern Kingfish are less common than Northern Kingfish in observer data and are possibly misidentified so we have initially included records here only when a specimen record was submitted to and verified through the SVP (see Data extraction). 

### Data extraction 
SQL query:
```{sql, eval = F, echo = T}
/* training trips, 900 trips, incidental takes, most duplicate records (unless different idmethod or idcomment) are not included */
/* this is a general script to pull the data on any species from the SVP*/
/* specimens submitted as photos and samples included separately */
/* nefop_ims.species_verification query that approximates is: select * from nefop_ims.species_verification where SUBJECT_CODE <> '01' and YEAR > '2009' and species = '6617' order by species, year, tripid, haulnum, idmethod but this could includes training trips, duplicates, 900 trips*/


SELECT ss.year YEAR, MONTH, ss.obid as OBSID, ss.tripid TRIPID, NEGEAR, DATELAND, SS.HAUL HAUL, CODE, SPECIES, correct, INCCODE, INCORRECTSPP, idmethod, GIS_LATSBEG, GIS_LONSBEG, GIS_LATSEND, GIS_LONSEND, GIS_LATHBEG, GIS_LONHBEG, GIS_LATHEND, GIS_LONHEND, rr.QTR QTR, rr.link1 link1, rr.link3 LINK3, idcomments

from 

(SELECT substr(c.tripid, 1,3) obid, c.tripid, c.year, HAUL, TO_CHAR(t.dateland, 'DD-MON-YY') as DATELAND, CODE, SPECIES, correct, INCORRECTSPP as INCCODE, comname as INCORRECTSPP, idmethod, idcomments

FROM 

(select id_num, a.tripid, a.year, lpad(CAST(haulnum as varchar2(4 byte)),4,'0') haul, inc, dateverify, species as CODE, comname as SPECIES, correct, incorrectspp,
  idmethod, idcomments, obs_contacted, filelocation
from nefop_ims.species_verification a, obdbs.obspec b
where a.species=b.nespp4) c 
left join obdbs.obspec d 
on c.incorrectspp=d.nespp4
join OBSCON.OBSCON_TRIPS_FSB t on c.year||c.tripid=t.year||substr(t.LINK1,10,6)
where c.YEAR like '%%' and c.TRIPID like '%%' and c.inc is null and DATELAND between to_date('01-JAN-10', 'DD-MON-RR') and to_date('31-DEC-20','DD-MON-RR')

UNION

SELECT substr(g.tripid,1,3) obid, g.tripid, g.year, HAUL, TO_CHAR(s.dateland, 'DD-MON-YY') as DATELAND, CODE, SPECIES, correct, INCORRECTSPP as INCCODE, comname as INCORRECTSPP,
  idmethod, idcomments
FROM ((select id_num, e.tripid, e.year, lpad(CAST(haulnum as varchar2(4 byte)),4,'0') haul, inc, dateverify, species as CODE, comname as SPECIES, correct, incorrectspp,
  idmethod, idcomments, obs_contacted, filelocation
from nefop_ims.species_verification e, obdbs.obspec f
where e.species=f.nespp4) g 
left join obdbs.obspec h 
on g.incorrectspp=h.nespp4
join obdbs.asmtrp_entry s on g.year||g.tripid=s.yearland||s.tripid) 
where g.YEAR like '%%' and g.TRIPID like '%%' and g.inc is null and DATELAND between to_date('01-JAN-10', 'DD-MON-RR') and to_date('31-DEC-20','DD-MON-RR')

UNION

SELECT substr(r.tripid, 1,3) obid, r.tripid, r.year, HAUL, TO_CHAR(s.dateland, 'DD-MON-YY') as DATELAND, CODE, SPECIES, correct, INCORRECTSPP as INCCODE, comname as INCORRECTSPP,
  idmethod, idcomments
FROM ((select id_num, a.tripid, a.year, lpad(CAST(haulnum as varchar2(4 byte)),4,'0') haul, inc, dateverify, species as CODE, comname as SPECIES, correct, incorrectspp,
  idmethod, idcomments, obs_contacted, filelocation
from nefop_ims.species_verification a, obdbs.obspec b
where a.species=b.nespp4) r 
left join obdbs.obspec d 
on r.incorrectspp=d.nespp4
join OBPRELIM.TRP_BASE s on r.year||r.tripid=s.yearland||S.TRIPID)
where r.YEAR like '%%' and r.TRIPID like '%%' and r.inc is null and DATELAND between to_date('01-JAN-10', 'DD-MON-RR') and to_date('31-DEC-20','DD-MON-RR')) ss

left join

(/* OBDBS OBHAU */

select
   substr(link3,1,15)  LINK1 ,
   LINK3,   
   substr(link3,1,3)  PROGRAM ,
   substr(link3,4,4)   YEAR ,
   substr(link3,8,2)   MONTH ,
   substr(link3,10,6)  TRIPID ,
   substr(link3,16,4)  HAULNUM ,
   decode(substr(link3,8,2),'01',1,'02',1,'03',1,'04',2,
           '05',2,'06',2,'07',3,'08',3,'09',3,'10',4,'11',4,'12',4,null) QTR,
 NEGEAR,
 OBSRFLAG,
 CATEXIST,
case when yearhbeg is not null and monthhbeg is not null and dayhbeg is not null then
   to_date(ltrim(rtrim(YEARHBEG))||ltrim(rtrim(MONTHHBEG))||ltrim(rtrim(DAYHBEG))||ltrim(rtrim(TIMEHBEG)),'YYYYMMDDHH24MI')
else null
end datehbeg,
case when yearhend is not null and monthend is not null and dayhend is not null then
   to_date(ltrim(rtrim(YEARHEND))||ltrim(rtrim(MONTHEND))||ltrim(rtrim(DAYHEND))||ltrim(rtrim(TIMEHEND)),'YYYYMMDDHH24MI')
else null
end datehend,
 LATHBEG,
 LONHBEG,
 LATHEND,
 LONHEND,
 cast( substr(latsbeg,1,2) + substr(latsbeg,3,2) / 60 + substr(latsbeg,5,2) / 3600 as number(8,6)) GIS_LATSBEG ,
 -1 * cast(substr(lonsbeg,1,2) + substr(lonsbeg,3,2) / 60 + substr(lonsbeg,5,2) / 3600 as number(8,6)) GIS_LONSBEG ,
  cast(substr(latsend,1,2) + substr(latsend,3,2) / 60 + substr(latsend,5,2) / 3600 as number(8,6)) GIS_LATSEND,
  -1 * cast(substr(lonsend,1,2) + substr(lonsend,3,2) / 60 + substr(lonsend,5,2) / 3600 as number(8,6)) GIS_LONSEND,
 cast(substr(lathbeg,1,2) + substr(lathbeg,3,2) / 60 + substr(lathbeg,5,2) / 3600 as number(8,6)) GIS_LATHBEG,
 -1 * cast(substr(lonhbeg,1,2) + substr(lonhbeg,3,2) / 60 + substr(lonhbeg,5,2) / 3600 as number(8,6)) GIS_LONHBEG,
 cast(substr(lathend,1,2) + substr(lathend,3,2) / 60 + substr(lathend,5,2) / 3600 as number(8,6)) GIS_LATHEND,
 -1 * cast(substr(lonhend,1,2) + substr(lonhend,3,2) / 60 + substr(lonhend,5,2) / 3600 as number(8,6)) GIS_LONHEND,
 AREA,
 HAUCOMMENTS
   from obdbs.obhau_base

    
UNION
    select
   substr(link3,1,15)  LINK1 ,
   LINK3,
   substr(link3,1,3)  PROGRAM ,
   substr(link3,4,4)   YEAR ,
   substr(link3,8,2)   MONTH ,
   substr(link3,10,6)  TRIPID ,
   substr(link3,16,4)  HAULNUM ,
   decode(substr(link3,8,2),'01',1,'02',1,'03',1,'04',2,
           '05',2,'06',2,'07',3,'08',3,'09',3,'10',4,'11',4,'12',4,null) QTR,

 NEGEAR,
 OBSRFLAG,
 CATEXIST,
case when yearhbeg is not null and monthhbeg is not null and dayhbeg is not null then
   to_date(ltrim(rtrim(YEARHBEG))||ltrim(rtrim(MONTHHBEG))||ltrim(rtrim(DAYHBEG))||ltrim(rtrim(TIMEHBEG)),'YYYYMMDDHH24MI')
else null
end datehbeg,
case when yearhend is not null and monthend is not null and dayhend is not null then
   to_date(ltrim(rtrim(YEARHEND))||ltrim(rtrim(MONTHEND))||ltrim(rtrim(DAYHEND))||ltrim(rtrim(TIMEHEND)),'YYYYMMDDHH24MI')
else null
end datehend,
 LATHBEG,
 LONHBEG, 
 LATHEND,
 LONHEND,
 cast( substr(latsbeg,1,2) + substr(latsbeg,3,2) / 60 + substr(latsbeg,5,2) / 3600 as number(8,6)) GIS_LATSBEG ,
 -1 * cast(substr(lonsbeg,1,2) + substr(lonsbeg,3,2) / 60 + substr(lonsbeg,5,2) / 3600 as number(8,6)) GIS_LONSBEG ,
 cast(substr(latsend,1,2) + substr(latsend,3,2) / 60 + substr(latsend,5,2) / 3600 as number(8,6)) GIS_LATSEND,
 -1 * cast(substr(lonsend,1,2) + substr(lonsend,3,2) / 60 + substr(lonsend,5,2) / 3600 as number(8,6)) GIS_LONSEND,
 cast(substr(lathbeg,1,2) + substr(lathbeg,3,2) / 60 + substr(lathbeg,5,2) / 3600 as number(8,6)) GIS_LATHBEG,
 -1 * cast(substr(lonhbeg,1,2) + substr(lonhbeg,3,2) / 60 + substr(lonhbeg,5,2) / 3600 as number(8,6)) GIS_LONHBEG,
 cast(substr(lathend,1,2) + substr(lathend,3,2) / 60 + substr(lathend,5,2) / 3600 as number(8,6)) GIS_LATHEND,
 -1 * cast(substr(lonhend,1,2) + substr(lonhend,3,2) / 60 + substr(lonhend,5,2) / 3600 as number(8,6)) GIS_LONHEND,
 AREA,
 HAUCOMMENTS
   from obv10.obhau_base


UNION

/*  ASM HAU  */

select
   substr(link3,1,15)  LINK1 ,
   LINK3,
   substr(link3,1,3)  PROGRAM ,
   substr(link3,4,4)   YEAR ,
   substr(link3,8,2)   MONTH ,
   substr(link3,10,6)  TRIPID ,
   substr(link3,16,4)  HAULNUM ,

   decode(substr(link3,8,2),'01',1,'02',1,'03',1,'04',2,
           '05',2,'06',2,'07',3,'08',3,'09',3,'10',4,'11',4,'12',4,null) QTR,
 NEGEAR,
 OBSRFLAG,
 cast(null as varchar2(1)) CATEXIST,
   to_date(ltrim(rtrim(YEARHBEG))||ltrim(rtrim(MONTHHBEG))||ltrim(rtrim(DAYHBEG))||ltrim(rtrim(TIMEHBEG)),'YYYYMMDDHH24MI')datehbeg, 
   to_date(ltrim(rtrim(YEARHEND))||ltrim(rtrim(MONTHEND))||ltrim(rtrim(DAYHEND))||ltrim(rtrim(TIMEHEND)),'YYYYMMDDHH24MI')datehend, 
 LATHBEG,
 LONHBEG,
 LATHEND,
 LONHEND,
 cast(null as number(8,6)) GIS_LATSBEG ,
 cast(null as number) GIS_LONSBEG ,
 cast(null as number(8,6)) GIS_LATSEND,
 cast(null as number) GIS_LONSEND,
 cast(substr(lathbeg,1,2) + substr(lathbeg,3,2) / 60 + substr(lathbeg,5,2) / 3600 as number(8,6)) GIS_LATHBEG,
 -1 * cast(substr(lonhbeg,1,2) + substr(lonhbeg,3,2) / 60 + substr(lonhbeg,5,2) / 3600 as number(8,6)) GIS_LONHBEG,
 cast(substr(lathend,1,2) + substr(lathend,3,2) / 60 + substr(lathend,5,2) / 3600 as number(8,6)) GIS_LATHEND,
 -1 * cast(substr(lonhend,1,2) + substr(lonhend,3,2) / 60 + substr(lonhend,5,2) / 3600 as number(8,6)) GIS_LONHEND,
 AREA,
 HAUCOMMENTS
 
   from asmhau_prelim p
 left outer join obtriptrack t 
      on substr(p.link3,1,15) = t.link1 
  where (substr(link3,4,6) between '201005' and '201012' and substr(link3,1,3) between '230' and '234')
     or (substr(link3,4,6) > '201012' and substr(link3,1,3) <> '900')
    and substr(link3,1,15) not in (select link1 from obv10.obtrp_base) ) rr
    
on ss.YEAR||ss.TRIPID||ss.HAUL = rr.YEAR||rr.TRIPID||ltrim(rr.HAULNUM)

where ss.OBID like '%%' and ss.TRIPID like '%%' and (CODE like '%6617%') and HAUL like '%%' and SPECIES like '%%' and IDCOMMENTS like '%%' 

ORDER BY SPECIES, YEAR, TRIPID, HAUL
;

```

### Data analysis
Time series were summed by year and plotted, and mapped data for individual records were plotted according to the location where gear was hauled. As coordinate data were not always available for each record, the map does not include all occurrences of southern kingfish, but was included for spatial context.

R code use to aggregate data:
```{r agg_code, eval = F, echo = T}
library(dplyr)

total_by_year<- source %>%
  filter(SPECIES == "KINGFISH, SOUTHERN") %>% #Filter species
  group_by(YEAR) %>%
  dplyr::summarise(n = n()) #Sum observations per year
```

### Plotting

```{r, echo = T, eval = F}

#For map and plot. Latitude and longitude data for this figure are not publicly available. 
sk.dat <- SOE.data[grepl("Lat",SOE.data$Units) &
                          grepl("Southern Kingfish",SOE.data$Var),]
lon <- sk.dat[as.numeric(sk.dat$Value) < 0,]$Value
lat <- sk.dat[as.numeric(sk.dat$Value) > 0,]$Value

#create data.frame
df <- data.frame(year = sk.dat$Time,
                 lon = lon,
                 lat = lat)

#set color palette
colors1 <- adjustcolor(matlab.like2(8),.5)
colors2 <- adjustcolor(matlab.like2(8),.5)
colors3 <- adjustcolor(matlab.like2(8),.5)
colors4 <- adjustcolor(matlab.like2(8),1)
colors <- c(colors1[1:2], colors2[3:4], colors3[5:6],colors4[7:8])

#map values to colors
df <- df %>% arrange(year) %>%
  mutate(colors = plyr::mapvalues(year, from = c("2010","2011","2012","2013",
                                                 "2014","2015","2016","2017"),
                                                               to = c(colors)))
colors <- df$colors

#projection
map.crs <- CRS("+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0
               +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")

#data.frame to sp object
coordinates(df) <- ~lon+lat
df@proj4string <- map.crs
df <- spTransform(df, map.crs)

#plot
par(fig = c(0,1,0,1))
plot(coast, xlim = c(-76,-73), ylim = c(35,40.5),col = "grey")
plot(df, pch = 16, col = colors, add = T, cex = 2.5)


occur <- SOE.data[SOE.data$Var == "Southern Kingfish observer sightings",]$Value
time <- SOE.data[SOE.data$Var == "Southern Kingfish observer sightings",]$Time

ts <- zoo(occur,time)
par(fig = c(0.5,1, 0.1, .5), new = T, bty = "l",mar = c(5,6,3,1)) 
barplot(occur,time, col = matlab.like2(8), xlab = c("Time"),ylab = "S. Kingfish Occurrence, n",
        cex.lab = 1, las = 1, cex.axis = 1)
axis(1,at = seq(1250,18500,length.out = 8),labels = c("2010","2011","2012","2013",
                                            "2014","2015","2016","2017"), cex.axis=1)
```

```{r SK-plot, fig.align='center', echo = F, fig.cap = "Verified records of Southern Kingfish occurrence in the Mid-Atlantic."}
image.dir <- here::here('images')

knitr::include_graphics(file.path(image.dir, 'southern_kingfish.PNG'))
```


<!--chapter:end:chapters/observer_data_indicator.Rmd-->

# Long-term Sea Surface Temperature


```{r,  echo = F, message=F,warning=F}

#Load packages
library(knitr)
library(rmarkdown)
library(dplyr)


```
**Description**: Long-term sea-surface temperatures

**Indicator category**: Database pull

**Contributor(s)**: Kevin Friedland
  
**Data steward**: Kevin Friedland, <kevin.friedland@noaa.gov>
  
**Point of contact**: Kevin Friedland, <kevin.friedland@noaa.gov>
  
**Public availability statement**: Source data are available [here](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.ersst.v5.html).


## Methods
Data for long-term sea-surface temperatures were derived from the NOAA extended reconstructed sea surface temperature data set (ERSST V5). The ERSST V5 dataset is parsed into 2&deg; x 2&deg; gridded bins between 1854-present with monthly temporal resolution. Data were interpolated in regions with limited spatial coverage, and heavily damped during the period between 1854-1880 when collection was inconsistent  [@Huang2017; @huang2017extended]. For this analysis, 19 bins were selected that encompassed the Northeast US Continental Shelf region [see @Friedland2007]. 


### Data sources
This indicator is derived from the [NOAA ERSST V5 dataset](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.ersst.v5.html) [@Huang2017].


### Data extraction 

```{r coordinates, echo = F, eval = T, results='asis'}
df <- data.frame(  
  Longitude = c(-74,-74,-72,-70,-70,-70,-68,-68),
  Latitude = c(40,38,40,44,42,40,44,42)
  )

knitr::kable(df,
      caption="Coordinates used in NOAA ERSST V5 data extraction.",
      format = "html",
      align = "c") %>%
  kableExtra::kable_styling(full_width = F) %>% 
  kableExtra::column_spec(c(1,2), width =  "4cm")
```

R code used in extraction and processing:
```r, echo = T, eval = F}
# Include R code here
# all year
years="ersst"

# a year
#years=".2016"

# which data
setwd("C:/2_ersst/datafiles_v4")
setwd("C:/2_ersst/datafiles_v5")


# NES standard bounded by 34-46N and 78-62W
minlon= -78; maxlon= -62; minlat= 34; maxlat= 46
dataoutfile="C:/2_ersst/nes_std_area_v5.csv"

#  DELETE ONLY FILE WILL APPEND AND DOUBLE DATA
file.remove(dataoutfile)

##################################################  END SET

# LABRODOR SEA  
#minlon= -66; #maxlon= -44; #minlat= 50; #maxlat= 70
#dataoutfile="C:/2_ersst/lab_sea.csv"

# New eel
#minlon= -80; maxlon= -40; minlat= 20; maxlat= 40
#dataoutfile="C:/2_ersst/new eel.csv"

# G of Mex
#minlon= -98; maxlon= -82; minlat= 18; maxlat= 30
#dataoutfile="C:/2_ersst/gomex_area.csv"

# USSE INCLUDING  28-36N and 80-76W not bounding
#minlon= -80; #maxlon= -76; #minlat= 28; #maxlat= 36
#dataoutfile="C:/2_ersst/usse_area.csv"

# GSL standard bounded by 34-46N and 78-62W
#minlon= -68; maxlon= -60; minlat= 46; maxlat= 48
#dataoutfile="C:/2_ersst/gsl.csv"

# PACIFIC area bounded by 10-30N and 166-146W
#minlon= -166; #maxlon= -146; #minlat= 10; #maxlat= 30
#dataoutfile="C:/1_analyses/ersst/pac_islands_area.csv"

# Baltic area bounded by 52-66N and 14-28E
#minlon= 14; #maxlon= 28; #minlat= 52; #maxlat= 66
#dataoutfile="C:/2_ersst/baltic area.csv"

# North Atlantic Area bounded by 30-70N and 80W-20E
#minlon= -80; #maxlon= -2; #minlat= 30; #maxlat= 70
#dataoutfile="C:/2_ersst/na area1.csv"
# and ...
#minlon= 0; #maxlon= 20; #minlat= 30; #maxlat= 70
#dataoutfile="C:/1_analyses/ersst/na area2.csv"

# NorthEAST Atlantic Area bounded by 55-70N and 10W-20E
#minlon= -10; #maxlon= -2; #minlat= 56; #maxlat= 70
#dataoutfile="C:/1_analyses/ersst/ne atl 1.csv"
# and ...
#minlon= 0; #maxlon= 10; #minlat= 56; #maxlat= 70
#dataoutfile="C:/1_analyses/ersst/ne atl 2.csv"

# Pacific steelhead
#minlon= -160; #maxlon= -122; #minlat= 40; #maxlat= 62
#dataoutfile="C:/1_analyses/ersst/pac sh.csv"

#North Pacific in two parts
#1
#minlon= -180; #maxlon= -120; #minlat= 30; #maxlat= 72
#dataoutfile="C:/1_analyses/ersst/n_pac_1.csv"
#2
#minlon= 120; #maxlon= 178; #minlat= 30; #maxlat= 72
#dataoutfile="C:/1_analyses/ersst/n_pac_2.csv"

# North Atlantic Area bounded by 20-70N and 100W-30E
#minlon= -100; #maxlon= -2; #minlat= 20; #maxlat= 70
#dataoutfile="C:/1_analyses/ersst/na area1.csv"
# and ...
#minlon= 0; #maxlon= 30; #minlat= 20; #maxlat= 70
#dataoutfile="C:/1_analyses/ersst/na area2.csv"









# constants for area
R <- 6371 # Earth mean radius [km]
dheight =  222

#library(ncdf)
library(ncdf4)

# ERSST data  
# lon goes from 0E to 358E with lon at center of box
# lat goes from 88S to 88N with lat at center of box

# start with lon based on degrees lonew
# array  1  2  ...  90    91    92  ...  180
# lon    0  2  ... 178   180   182  ...  358
# lonew  0  2  ... 178  -180  -178  ...   -2
# star with lat + deg N, - deg S
# array    1  ...  45  ...  89
# lon    -88  ...   0  ...  88

# -> -> -> TO KEEP THINGS SIMPLE, RETRIEVALS CAN'T BE CONTINUOUS FROM - LONS TO + LONs
#          have to extract from -180W to -2W separately from 0E to 180E

# -> -> -> OUTPUT APPENDS SO NEED TO DELETE FILE IF ALREADY EXISTS

# -> -> -> USE APPROPRIATE lon lat and outfile block:






# set lon limits in array units
if ( minlon < 0){ 
  arrayminlon=(minlon+360)/2+1
} else { 
  arrayminlon=minlon/2+1 
} 

if ( maxlon < 0){ 
  arraymaxlon=(maxlon+360)/2+1
} else { 
  arraymaxlon=maxlon/2+1 
} 

# set lat limits in array units
arrayminlat=minlat/2+45
arraymaxlat=maxlat/2+45


filelist=list.files(pattern=years)

numfiles=length(filelist)

for (filenum in 1:numfiles){

#  ersst = open.ncdf(filelist[filenum]) 
  ersst = nc_open(filelist[filenum]) 
  print(filelist[filenum])

#  sst = get.var.ncdf( ersst, "sst") 
  sst <- ncvar_get(ersst,"sst" )
  
  year=as.numeric(substr(filelist[filenum],10,13))
  month=as.numeric(substr(filelist[filenum],14,15))

  for (arrlons in arrayminlon:arraymaxlon){
    for (arrlats in arrayminlat:arraymaxlat){
      

      if ( arrlons < 91){ 
        regenlon=(arrlons-1)*2
      } else { 
        regenlon=(arrlons-1)*2-360
      } 
      
    
      regenlat=(arrlats-45)*2
      
      long1=regenlon-1 *pi/180
      lat1=regenlat-1 *pi/180
      long2=regenlon+1 *pi/180
      lat2=regenlat-1 *pi/180
      dwidth1 <- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R
      long1=regenlon-1 *pi/180
      lat1=regenlat+1 *pi/180
      long2=regenlon+1 *pi/180
      lat2=regenlat+1 *pi/180
      dwidth2 <- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R
      area=((dwidth1 + dwidth1)/2) * dheight
      
      dataline <- matrix(c(year, month, regenlon, regenlat, round(sst[arrlons,arrlats],digits=2),area),1,6)
      
      
      if(is.finite(sst[arrlons,arrlats])) {
      write.table(dataline,file=dataoutfile,sep=",",row.name=F,col.names=F,append=TRUE)   
    }
      
    }
  }

#    close.ncdf(ersst) 
nc_close(ersst)
  
  }


```


### Plotting


```{r long-term-sst,fig.cap="Long-term sea surface temperatures on the Northeast Continental Shelf.",  echo=T, message=FALSE, warning=FALSE , fig.width=8, fig.align = 'center',fig.pos='H'}
# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


opar <- par(mar = c(4, 6, 2, 6))
soe.plot(SOE.data, "Time", "long term sst", end.start = 2007, 
         line.forward = TRUE, point.cex = 0.8, rel.y.text = 1.1,
         x.line = 2, y.line = 2.3, x.label = 'Year', rel.y.num = 1.1,
         y.label = expression(paste("Mean SST (",degree,"C)")))

```







<!--chapter:end:chapters/long_term_sst_indicator.Rmd-->

# Annual SST Cycles

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Annual SST Cycles

**Indicator category**: Database pull with analysis

**Contributor(s)**: Sean Hardison, Vincent Saba
  
**Data steward**: Sean Hardison, <sean.hardison@noaa.gov>
  
**Point of contact**: Sean Hardison, <sean.hardison@noaa.gov>
  
**Public availability statement**: Source data are available [here](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html). 



## Methods

### Data sources
Data for annual sea surface tempature cycles were derived from the NOAA optimum interpolation sea surface temperature high resolution dataset ([NOAA OISST V2 dataset](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html)) provided by NOAA/OAR/ESRL PSD, Boulder, CO. The data extend from 1981 to present, and provide a 0.25&deg; x 0.25&deg; global grid of SST measurements [@Reynolds2007]. Gridded SST data were masked according to the extent of Ecological Production Units in the Northeast Large Marine Ecosystem (NE-LME) (See ["EPU_Extended" shapefiles](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis)).


### Data extraction 
Daily mean sea surface temperature data for 2017 and for each year during the period of 1981-2012 were downloaded from the NOAA [OI SST V2 site](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html) to derive the long-term climatological mean for the period. The use of a 30-year climatological reference period is a standard procedure for metereological observing [@WMO2017]. These reference periods serve as benchmarks for comparing current or recent observations, and for the development of standard anomaly data sets. The reference period of 1982-2012 was chosen to be consistent with previous versions of the State of the Ecosystem report. 

R code used in extraction and processing:
```r, echo = T, eval = F}

#libraries
library(ncdf4);library(dplyr)
library(readr);library(tidyr)
library(sp);library(rgdal)
library(raster);library(stringr)

#get spatial polygons for Ecological Production Units (EPUs) that are used to clip SST data.
EPU <- readOGR('Extended_EPU')

map.crs <- CRS("+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0
               +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")

#find long term daily mean SSTs and 2017 SST anomaly
MAB_sst_daily_mean <- NULL
GB_sst_daily_mean <- NULL
GOM_sst_daily_mean <- NULL

#I split the data into three directories to loop through in separate R sessions concurrently. 
for (dir. in 1:3){
  
  #Loop through directories
  setwd(paste0('c:/users/sean.hardison/documents/sst_data/',dir.))
  print(getwd())
  
  for (f in 1:length(list.files())){
    
    if (!str_detect(list.files()[f],".nc")){
      print(paste(list.files()[f],"is not a raster")) #Based on file type
      next
    }
    
    for (j in c("MAB","GB","GOM")){
      
      sub_region <- EPU[EPU@data$EPU == j,]
      y <- as.numeric(str_extract(list.files()[f],"[0-9]+")) #get year
      
      for (i in 1:365){
        print(paste(j,y,i))
        daily_mean <- raster(paste0(list.files()[f]), band = i) #get band
        
        #set crs
        daily_mean@crs <- sub_region@proj4string 
        
        
        #rotate to lon scale from 0-360 to -180-180
        daily_mean <- rotate(daily_mean)
        
        #mask raster with spatialpolygon
        daily_mean_clipped <- mask(daily_mean, sub_region)
        
        
        #add mean value to data.frame
        assign(paste0(j,"_sst_daily_mean"),rbind(get(paste0(j,"_sst_daily_mean")),
                                                 c(mean(daily_mean_clipped@data@values, na.rm = T),y,i)))
        
      }
    }
    
    
  }
}

#Put results into data.frames
mab <- data.frame(EPU = "MAB",
                  year = MAB_sst_daily_mean[,2],
                  day = MAB_sst_daily_mean[,3],
                  Value = MAB_sst_daily_mean[,1])

gb <- data.frame(EPU = "GB",
                 year = GB_sst_daily_mean[,2],
                 day = GB_sst_daily_mean[,3],
                 Value = GB_sst_daily_mean[,1])

gom <- data.frame(EPU = "GOM",
                  year = GOM_sst_daily_mean[,2],
                  day = GOM_sst_daily_mean[,3],
                  Value = GOM_sst_daily_mean[,1])


data3 <- rbind(mab, gb, gom)

#Save as 1 of 3 files (one for each directory containing daily mean data)
save(data3, file = "dir3_sst.Rdata")

#--------------------------2017 SSTs----------------------------#
MAB_2017 <- NULL
GB_2017 <- NULL
GOM_2017 <- NULL

for (j in c("MAB","GB","GOM")){
  
  sub_region <- EPU[EPU@data$EPU == j,]
  
  for (i in 1:365){
    print(paste(j,i))
    daily_mean <- raster("sst.day.mean.2017.nc", band = i) #get band
    
    
    #set crs
    daily_mean@crs <- sub_region@proj4string 
    
    
    #rotate to lon scale from 0-360 to -180-180
    daily_mean <- rotate(daily_mean)
    
    #mask raster with spatialpolygon
    daily_mean_clipped <- mask(daily_mean, sub_region)
    
    
    #add mean value to data.frame
    assign(paste0(j,"_2017"),rbind(get(paste0(j,"_2017")),
                                             c(mean(daily_mean_clipped@data@values, na.rm = T),"2017",i)))
    
  }
}
#Put results into data.frames
mab_2017 <- data.frame(EPU = "MAB",
                  year = MAB_2017[,2],
                  day = MAB_2017[,3],
                  Value = MAB_2017[,1])

gb_2017 <- data.frame(EPU = "GB",
                 year = GB_2017[,2],
                 day = GB_2017[,3],
                 Value = GB_2017[,1])

gom_2017 <- data.frame(EPU = "GOM",
                  year = GOM_2017[,2],
                  day = GOM_2017[,3],
                  Value = GOM_2017[,1])


#Final 2017 daily mean data
sst_2017 <- rbind(mab_2017, gb_2017, gom_2017)
#save(sst_2017, file = "sst_2017.Rdata")

```

### Data analysis 
We calculated the long-term mean and standard deviation of SST over the period of 1982-2012 for each EPU, as well as the daily mean for 2017.  

```{r analyses, echo = T, eval = F}
#----------------------Load results--------------------------#
load("dir1_sst.Rdata")
load("dir2_sst.Rdata")
load("dir3_sst.Rdata")
load("sst_2017.Rdata")

#Get long term mean and standard deviation
d <- rbind(data1, data2, data3)

ltm <- d %>% group_by(EPU, day) %>% dplyr::summarise(mean  = mean(Value),
                                                     sd = sd(Value)) 

```

### Plotting
```{r plotting, echo = T, eval = T, fig.cap = "Long-term mean SSTs for the Mid-Atlantic Bight (A), Georges Bank (B), and Gulf of Maine (C). Orange and cyan shading show where the 2017 daily SST values were above or below the long-term mean respectively; red and dark blue shades indicate days when the 2017 mean exceeded +/- 1 standard deviation from the long-term mean.", fig.width=8, fig.height=3.25, fig.align='center'}


# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

##---------------------------------MAB-----------------------------------------#
par(mfrow = c(1,3))
doy <- as.numeric(SOE.data[SOE.data$Var == "sst mean 2017 MAB",]$Time)
val_2017 <- SOE.data[SOE.data$Var == "sst mean 2017 MAB",]$Value
val_LT <- SOE.data[SOE.data$Var == "sst mean long term MAB",]$Value
val_LT_sd <- SOE.data[SOE.data$Var == "sst sd long term MAB",]$Value


# val_2017 <- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT <- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT_sd <- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
doy <- seq(doy[1],doy[length(doy)],length.out = 365*1)


above_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT[i]){
    above_mean[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i]){
    below_mean[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT_sd[i] + val_LT[i]){
    above_sd[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT_sd [i] + val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i] - val_LT_sd[i]){
    below_sd[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT[i] - val_LT_sd [i]){
    below_sd[i] <- NA
  }
}

#Lines for polygons
above_sd[is.na(above_sd)] <- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- val_LT[which(is.na(below_mean))]

upper <- val_LT_sd + val_LT
lower <- val_LT - val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,25), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,61,122,183,245,306,365), cex.axis= 1.25)
mtext(2, line = 2.3, text = expression(paste("Mean SST (",degree,"C)")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(15,25*.95,"A",cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-val_LT), rev(above_mean)),
        col = "orange", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)),
        col = "red", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,val_LT, type = "l", lwd = 1, "grey90")


##-------------------------------------GB-------------------------------------#

doy <- as.numeric(SOE.data[SOE.data$Var == "sst mean 2017 GB",]$Time)
val_2017 <- SOE.data[SOE.data$Var == "sst mean 2017 GB",]$Value
val_LT <- SOE.data[SOE.data$Var == "sst mean long term GB",]$Value
val_LT_sd <- SOE.data[SOE.data$Var == "sst sd long term GB",]$Value


# val_2017 <- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT <- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT_sd <- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
doy <- seq(doy[1],doy[length(doy)],length.out = 365*1)


above_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT[i]){
    above_mean[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i]){
    below_mean[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT_sd[i] + val_LT[i]){
    above_sd[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT_sd [i] + val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i] - val_LT_sd[i]){
    below_sd[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT[i] - val_LT_sd [i]){
    below_sd[i] <- NA
  }
}

#Lines for polygons
above_sd[is.na(above_sd)] <- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- val_LT[which(is.na(below_mean))]

upper <- val_LT_sd + val_LT
lower <- val_LT - val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,21), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,61,122,183,245,306,365), cex.axis= 1.25)
#mtext(2, line = 2.5, text = expression(paste("Mean SST (",degree,"C)")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(15,21*.95,"B",cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-val_LT), rev(above_mean)),
        col = "orange", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)),
        col = "red", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,val_LT, type = "l", lwd = 1, "grey90")

#----------------------------------------------------------------------------#

## SST GOM

doy <- as.numeric(SOE.data[SOE.data$Var == "sst mean 2017 GOM",]$Time)
val_2017 <- SOE.data[SOE.data$Var == "sst mean 2017 GOM",]$Value
val_LT <- SOE.data[SOE.data$Var == "sst mean long term GOM",]$Value
val_LT_sd <- SOE.data[SOE.data$Var == "sst sd long term GOM",]$Value


# val_2017 <- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT <- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT_sd <- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
doy <- seq(doy[1],doy[length(doy)],length.out = 365*1)


above_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT[i]){
    above_mean[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i]){
    below_mean[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT_sd[i] + val_LT[i]){
    above_sd[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT_sd [i] + val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i] - val_LT_sd[i]){
    below_sd[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT[i] - val_LT_sd [i]){
    below_sd[i] <- NA
  }
}

#Lines for polygons
above_sd[is.na(above_sd)] <- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- val_LT[which(is.na(below_mean))]

upper <- val_LT_sd + val_LT
lower <- val_LT - val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,21), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,61,122,183,245,306,365), cex.axis= 1.25)
#(2, line = 2.5, text = expression(paste("Mean SST (",degree,"C)")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(15,21*.95,"C",cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-val_LT), rev(above_mean)),
        col = "orange", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)),
        col = "red", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,val_LT, type = "l", lwd = 1, "grey90")
box()
```




<!--chapter:end:chapters/Annual_SST_cycle_indicator.Rmd-->

# Seasonal SST Anomalies


```{r, echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Seasonal SST Anomalies

**Indicator category**: Database pull with analysis

**Contributor(s)**: Sean Hardison, Vincent Saba
  
**Data steward**: Sean Hardison, <sean.hardison@noaa.gov>
  
**Point of contact**: Sean Hardison, <sean.hardison@noaa.gov>
  
**Public availability statement**: Source data are available [here](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html).




## Methods
### Data sources
Data for seasonal sea surface tempature anomalies were derived from the NOAA optimum interpolation sea surface temperature high resolution data set ([NOAA OISST V2](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html)) provided by NOAA/OAR/ESRL PSD, Boulder, CO. The data extend from 1981 to present, and provide a 0.25&deg; x 0.25&deg; global grid of SST measurements [@Reynolds2007].  


### Data extraction 
Individual files containing daily mean SST data for each year during the period of 1981-present were downloaded from the [OI SST V5 site](http://www1.ncdc.noaa.gov/pub/data/cmb/ersst/v5/netcdf/). Yearly data provided as layered rasters were masked according to the extent of Northeast US Continental Shelf (See [Seasonal_anom_crop.nc](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis)). Data were split into three month seasons for (Winter = Jan, Feb, Mar; Spring = Apr, May, Jun; Summer = July, August, September; Fall = Oct, Nov, Dec).  


R code used in extraction and processing:
```r, echo = T, eval = F}
#libraries
library(ncdf4);library(dplyr)
library(readr);library(tidyr)
library(sp);library(rgdal)
library(raster);library(stringr)

fall <- stack()
winter <- stack()
spring <- stack()
summer <- stack()
sub_region <- raster("Seasonal_anom_crop.nc") #get an old file to crop with

for (dir. in 1){
  #Loop through directories
  setwd(paste0('c:/users/sean.hardison/documents/sst_data/',dir.))
  print(getwd())
  for (f in 1:length(list.files())){
    
    if (!str_detect(list.files()[f],".nc")){
      print(paste(list.files()[f],"is not a raster")) #Based on file type
      next
    }
    
    for (s in c("winter","spring","summer","fall")){
  
      y <- as.numeric(str_extract(list.files()[f],"[0-9]+")) #Pull out file year for identifying leap years
      print(paste('Year:',y,"Season:",s))
      leap_year <- seq(1984,2016,4)
      
      #Pick out leap years
      if (y %in% leap_year){
          leap <- T
      } else {
        leap <- F
      }
      if (s == "winter")
        if (!leap){
          season <- 1:90 
        } else {
          season <- 1:91
        }
      else if (s == "spring"){
        if (!leap){
          season <- 91:181 
        } else {
          season <- 92:182
        } 
      }
      else if (s == "summer"){
        if (!leap){
          season <- 182:273
        } else {
          season <- 183:274
        } 
      }
      else if (s == "fall"){
        if (!leap){
          season <- 274:365
        } else {
          season <- 275:366
        } 
      }
      
      if(leap){
        print(paste(s,y,"leap"))
      }
      
        
        for (b in season){
          r <- raster(list.files()[f],b) #Import raster bands within a given season

          if (b == season[1]){
            print(list.files()[f])
            print(length(season))
            print(r@file@nbands) #Print file name
          }
          
          #Rotate and crop
          r <- rotate(r) 
          r <- crop(r, extent(sub_region))
          #print(r@file@nbands)
          if (b == season[50]){
            print("Over halfway there")
          }
          
          #Add bands to raster stack for each season. Seaonal bands for each year are bound together.
          assign(s,stack(get(s),r))
          
        }
      print(paste("Done stacking",nlayers(get(s)),"days"))
    } 
  }
}


```

### Data analysis
We calculated the long-term mean (LTM) for each season-specific stack of rasters over the period of 1982-2012, and then subtracted the (LTM) from 2017 daily mean SST values to find the SST anomaly. The use of a 30-year climatological reference period is a standard procedure for the calculation of meteorological anomalies [@WMO2017]. In our case, we chose to use the 1982-2012 reference period to remain consistent with previous State of the Ecosystem reports. 

```{r anomaly calculation, echo = T, eval = F}
fall_1 <- stack("fall_1")
fall_2 <- stack("fall_2")
fall_3 <- stack("fall_3")


spring_1 <- stack("spring_1")
spring_2 <- stack("spring_2")
spring_3 <- stack("spring_3")


winter_1 <- stack("winter_1")
winter_2 <- stack("winter_2")
winter_3 <- stack("winter_3")


summer_1 <- stack("summer_1")
summer_2 <- stack("summer_2")
summer_3 <- stack("summer_3")

winter <- stack(winter_1,winter_2,winter_3)
fall <- stack(fall_1,fall_2,fall_3)
spring <- stack(spring_1,spring_2,spring_3)
summer <- stack(summer_1,summer_2,summer_3)

winter_mean <- stackApply(winter, indices = rep(1,nlayers(winter)),mean)
spring_mean <- stackApply(spring, indices = rep(1,nlayers(spring)),mean)
summer_mean <- stackApply(summer, indices = rep(1,nlayers(summer)),mean)
fall_mean <- stackApply(fall, indices = rep(1,nlayers(fall)),mean)

rm(winter, spring, summer, fall)

#Get 2017 daily mean data
#setwd("z:/shardison/neiea")
daily_mean_2017 <- stack("sst.day.mean.2017.nc")



daily_mean_2017 <- rotate(daily_mean_2017) #rotate
daily_mean_2017 <- crop(daily_mean_2017, extent(sub_region)) #crop

#Find daily mean across seasons
seasonal_sst_2017 <- stackApply(daily_mean_2017, indices = c(rep(1,90),rep(2,91),
                                                             rep(3, 92), rep(4, 92)), mean)

#Get seasonal anomaly
winter_anom <- seasonal_sst_2017[[1]] - winter_mean
spring_anom <- seasonal_sst_2017[[2]] - spring_mean
summer_anom <- seasonal_sst_2017[[3]] - summer_mean
fall_anom <- seasonal_sst_2017[[4]] - fall_mean
```

### Plotting
```{r sst-anomaly-maps, fig.cap="Sea surface temperature anomalies in winter (A), spring (B), summer (C), and fall (D) 2017. Black lines depict the 200 m isobath." , fig.width = 8,fig.height = 5.5, fig.show='hold',fig.pos='H', echo=T, message=F, warning=F}


# Relative working directory
r.dir <- here::here("R")

#Source constants for GIS code
source(file.path(r.dir,"GIS_source.R"))

#SST map function
SST_plot <- function(raster_data, text,pos = "left"){
  
  #adjusts par for legend placement 
  if (pos == "right"){
    par(mar = c(4.5,0,3,10), mex = .3,  mgp = c(4, .35, 0))
  } else{
     par(mar = c(4.1,5,3,6), mex = .3,  mgp = c(4, .35, 0))
  }
  
  #read .nc and convert to raster
  d <- raster(file.path(gis.dir,raster_data))
  crs(d) <- map.crs
  #crop data
  d <- crop(d, e)
  
  #get zmax (~max SST anomaly)
  z.max=7

  #plot raster
  plot(d, col=colors(n=color_levels),
       breaks=seq(-z.max,z.max,length.out=color_levels+1),
       legend.width = 1.5,bty ="L",axes=F,
       interpolate = T, las = 1,box = FALSE,legend = FALSE, asp = 1)
  #plot contour
  contour(bathy,drawlabels = T,add=T, nlevels = 2, zlim = c(-200,-200))
  #plot coastline
  plot(coast, col = 'grey',add = T , lty=0)
  #axes

  
  if (pos == "left"){
    axis(1, at = seq(-77,-65,2), labels = seq(-77,-65,2) , cex.axis = 0.7)
    axis(2, cex.axis = 0.7)
    abline(h = 34.95, lty = 1, lwd = 2)
    abline(v =-77.2, lty = 1, lwd = 2)
  } else {
    axis(1, at = seq(-77,-65,2), labels = seq(-77,-65,2) ,
         cex.axis = 0.7, pos = 34.9)
    axis(2, cex.axis = 0.7, pos = -77.225)
    abline(h = 34.9, lty = 1, lwd = 2)
    abline(v =-77.225, lty = 1, lwd = 2)

  }

  #labels
  text(-76,45,text,cex = 1.5)

  
}

#plot maps
par(oma=c(0,0,0,0), mfrow = c(2,2))
SST_plot("Winter_2017_anomaly_1982_2012.nc","A")
SST_plot("Spring_2017_anomaly_1982_2012.nc","B",pos = "right")
SST_plot("Summer_2017_anomaly_1982_2012.nc","C")
SST_plot("Fall_2017_anomaly_1982_2012.nc","D",pos = "right") 

#plot only legend
z.max=7

#set new par
par(mfrow=c(1, 1), mar=c(2, 5, 2, 0), new=FALSE)

#any data
d <- raster(matrix(runif(100), ncol=10))

#legend plot
plot(d, col=colors(n=color_levels),
     breaks=seq(-z.max,z.max,length.out=color_levels+1),
     legend.shrink = 1,
     legend.width = 1,legend.only = T,
     axes=F, interpolate = T, las = 1,box = FALSE,
     axis.args=list(at=seq(-z.max, z.max, length.out = 5),
                    labels=seq(-7,7,length.out = 5), 
                    cex.axis=0.8),
     legend.args=list(text=expression(paste('Temperature Anomaly (',degree,"C)")),
                      side=4, font=2, line=2, cex=1))

```

**Note**: The derived data sets used to create the above figures are available [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis).

<!--chapter:end:chapters/Seasonal_SST_anomaly_maps_indicator.Rmd-->

# Chlorophyll *a* and Primary Production {#chl-pp}
```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Chlorophyll *a* and Primary Production

**Indicator category**: Database pull; Database pull with analysis; Published methods

**Contributor(s)**: Kimberly Hyde
  
**Data steward**: Kimberly Hyde, kimberly.hyde@noaa.gov
  
**Point of contact**: Kimberly Hyde, kimberly.hyde@noaa.gov
  
**Public availability statement**: Source data used in these analyses will be made publicly available. Derived data used in State of the Ecosystem Reports can be found [here](http://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000).


## Methods
### Data sources
Level 1A ocean color remote sensing data from the Sea-viewing Wide Field-of-view Sensor (SeaWiFS) [@NASA1] on the OrbView-2 satellite and the Moderate Resolution Imaging Spectroradiometer (MODIS) [@NASA2] on the Aqua satellite were acquired from the NASA Ocean Biology Processing Group (OBPG).  Sea Surface Temperature (SST) data included the 4 km nighttime NOAA Advanced Very High Resolution Radiometer (AVHRR) Pathfinder [@Casey2010; @Saha2018] and the Group for High Resolution Sea Surface Temperature (GHRSST) Multiscale Ultrahigh Resolution (MUR, version 4.1) Level 4 [@SOE4; @SOE14] data.  

### Data extraction
NA

### Data analysis
The SeaWiFS and MODIS L1A files were processed using the NASA Ocean Biology Processing Group [SeaDAS](https://seadas.gsfc.nasa.gov/) software version 7.4.  All MODIS files were spatially subset to the U.S. East Coast (SW longitude=-82.5, SW latitude=22.5, NE longitude=-51.5, NE latitude=48.5) using [L1AEXTRACT_MODIS](https://seadas.gsfc.nasa.gov/help/seadas-processing/ProcessL1aextract_modis.html). SeaWiFS files were subset using the same coordinates prior to begin downloaded from the [Ocean Color Web Browser](https://oceancolor.gsfc.nasa.gov/cgi/browse.pl?sen=am).  SeaDAS's [L2GEN](https://seadas.gsfc.nasa.gov/help/seadas-processing/ProcessL2gen.html) program was used to generate Level 2 (L2) files using the default settings and optimal ancillary files, and the [L2BIN](https://seadas.gsfc.nasa.gov/help/seadas-processing/ProcessL2bin.html) program spatially and temporally aggregated the L2 files to create daily Level 3 binned (L3B) files.  The daily files were binned at 2 km resolution that are stored in a global, nearly equal-area, [integerized sinusoidal grids](https://oceancolor.gsfc.nasa.gov/docs/format/l3bins/) and use the default [L2 ocean color flag masks](https://oceancolor.gsfc.nasa.gov/atbd/ocl2flags/).  The global SST data were also subset to the same East Coast region and remapped to the same sinusoidal grid.    

The L2 files contain several ocean color products including the default chlorophyll &alpha; product (CHL-OCI), photosynthetic available radiation (PAR), remote sensing reflectance $(R_{rs}(\lambda))$, and several inherent optical property products (IOPs).  The CHL-OCI product combines two algorithms, the O'Reilly band ratio (OCx) algorithm [@SOE11] and the Hu color index (CI) algorithm [@SOE5].  The SeaDAS default CHL-OCI algorithm diverges slightly from @SOE5 in that the transition between CI and OCx occurs at 0.15 < CI < 0.2 mg m^-3^ to ensure a smooth [transition](https://oceancolor.gsfc.nasa.gov/atbd/chlor_a/). The regional chlorophyll &alpha; algorithm by @SOE12 was used to create a second chlorophyll product (CHL-PAN).  CHL-PAN is an empirical algorithm derived from *in situ* sampling within the Northeast Large Marine Ecosystem (NE-LME) and demonstrated significant improvements from the standard NASA operational algorithm in the NES-LME [@SOE13].  A 3rd-order polynomial function (Equation \@ref(eq:one)) is used to derive [CHL-PAN] from Rrs band ratios (RBR): 

\begin{equation}
log[\textrm{CHL-PAN}] = A_{0} + A_{1}X + A_{2}X^{2} + A_{3}X^{3},  
(\#eq:one) 
\end{equation}

where $X = log(R_{rs}(\lambda_{1})/R_{rs}(\lambda_{2}))$ and $A_{i} (i = 0, 1, 2, \textrm{or }  3)$ are sensor and RBR specific coefficients:

* If SeaWiFS and RBR is $R_{rs}(490)/R_{rs}(555)(R_{^3{\mskip -5mu/\mskip -3mu}_5})$ then: $A_0=0.02534, A_1=-3.033, A_2=2.096, A_3=-1.607$
* If SeaWiFS and RBR is $R_{rs}(490)/R_{rs}(670)(R_{^3{\mskip -5mu/\mskip -3mu}_6})$  then: $A_0=1.351, A_1=-2.427, A_2=0.9395, A_3=-0.2432$
* If MODIS and RBR is $R_{rs}(488)/R_{rs}(547)(R_{^3{\mskip -5mu/\mskip -3mu}_5})$  then: $A_0=0. 03664, A_1=-3.451, A_2=2.276, A_3=-1.096$
* If MODIS and RBR is $R_{rs}(488)/R_{rs}(667)(R_{^3{\mskip -5mu/\mskip -3mu}_6})$  then: $A_0=1.351, A_1=-2.427, A_2=0.9395, A_3=-0.2432$

C~3/5~ and C~3/6~ were calculated for each sensor specific RBR (R~3/5~ and R~3/6~ respectively) and then the following criteria were used to determine to derive CHL-PAN:

<ol type="a">
  <li>If $R_{^3{\mskip -5mu/\mskip -3mu}_5}>0.15$ or $R_{6} <0.0001$ then $\textrm{CHL-PAN} = C_{^3{\mskip -5mu/\mskip -3mu}_5};$</li>
  <li> Otherwise, $\textrm{CHL-PAN} = \textrm{max}(C_{^3{\mskip -5mu/\mskip -3mu}_5}, C_{^3{\mskip -5mu/\mskip -3mu}_6})$,</li>
</ol>

where $R_6$ is $R_{rs}(670)$ (SeaWiFS) or $R_{rs}(667)$ [@SOE13]. 

The Vertically Generalized Production Model (VGPM) estimates net primary production (PP) as a function of chlorophyll &alpha;, photosynthetically available light and the photosynthetic efficiency [@SOE1].  In the VGPM-Eppley version, the original temperature-dependent function to estimate the chlorophyll-specific photosynthetic efficiency is replaced with the exponential "Eppley" function (equation PP1) as modified by @SOE7. The VGPM calculates the daily amount of carbon fixed based on the maximum rate of chlorophyll-specific carbon fixation in the water column, sea surface daily photosynthetically available radiation, the euphotic depth (the depth where light is 1% of that at the surface), chlorophyll &alpha; concentration, and the number of daylight hours (Equation \@ref(eq:two)).  

\begin{equation}
P_{max}^{b}(SST) = 4.6 * 1.065^{SST-20^{0}} 
(\#eq:two) 
\end{equation}
Where $P_{max}^{b}$ is the maximum carbon fixation rate and *SST* is sea surface temperature.

\begin{equation}
PP_{eu} = 0.66125 * P_{max}^{b} * \frac{I_{0}}{I_{0}+4.1} * Z_{eu} * \textrm{CHL} * \text{DL}
(\#eq:three) 
\end{equation}

Where $PP_{eu}$ is the daily amount of carbon fixed integrated from the surface to the euphotic depth (mgC m^-2^ day^-1^), $P_{max}^{b}$ is the maximum carbon fixation rate within the water column (mgC mgChl^-1^ hr^-1^), $I_{0}$ is the daily integrated molar photon flux of sea surface PAR (mol quanta m^-2^ day^-1^), Zeu is the euphotic depth (m), CHL is the daily interpolated CHIi-OCI (mg m^-3^), and DL is the photoperiod (hours) calculated for the day of the year and latitude according to @SOE6. The light dependent function $(I_{0}/(I_{0}+4.1))$ describes the relative change in the light saturation fraction of the euphotic zone as a function of surface PAR ($I_0$).  Zeu is derived from an estimate of the total chlorophyll concentration within the euphotic layer (*CHL~eu~*) based on the Case I models of @SOE8:

* For $\textrm{CHL}_{eu} > 10.0\;\;\;\;\;Z_{eu} = 568.2 * \textrm{CHL}_{eu}^{-0.746}$
* For $\textrm{CHL}_{eu} \leq 10.0\;\;\;\;\;Z_{eu} = 200.0 * \textrm{CHL}_{eu}^{-0.293}$
* For $\textrm{CHL}_{0} \leq 1.0\;\;\;\;\;\textrm{CHL}_{eu} = 38.0 * \textrm{CHL}_{0}^{0.425}$
* For $\textrm{CHL}_{0} > 1.0\;\;\;\;\;\textrm{CHL}_{eu} = 40.2 * \textrm{CHL}_{0}^{0.507}$

Where $\textrm{CHL}_0$ is the surface chlorophyll concentration.

Prior to being input into the VGPM-Eppley model, the daily CHL-OCI and AVHRR SST data were temporally interpolated and smoothed (CHL-OCI~INT~ and SST~INT~ respectively) to increase the data coverage and better match data collected from different sensors and different times.  The daily PAR data are not affected by cloud cover and MUR SST data is a blended/gap free data product so these products were not interpolated.  

Daily data at each pixel location covering the entire date range were extracted to create a pixel time series $(D_{x,y})$.  $(D_{x,y})$ are linearly interpolated based on days in the time series using [interpx.pro](https://github.com/callumenator/idl/blob/master/external/JHUAPL/INTERPX.PRO). Prior to interpolation, the CHL data are log-transformed to account for the log-normal distribution of chlorophyll data [@SOE2].  Interpolating the entire times series requires a large amount of processing time so the series was processed one year at a time.  Each yearly series included 60 days from the previous year and 60 days from the following year to improve the interpolation at the beginning and end of the year.  Following interpolation, the data are smoothed with a tri-cube filter (width=7) using IDL's [CONVOL](https://www.harrisgeospatial.com/docs/CONVOL.html) program.  In order to avoid over interpolating data when there were several days of missing data in the time series, the interpolated data were removed and replaced with blank data if the window of interpolation spanned more than 7 days for CHL or 10 days for SST.  After all D~x,y~ pixels had been processed, the one-dimensional pixel time series were converted back to two-dimensional daily files. 

Statistics, including the arithmetic mean, geometric mean (for CHL and PP), standard deviation, and coefficient of variation were calculated at daily (3 and 8-day running means), weekly, monthly, and annual time steps and for several climatological periods.  Annual statistics used the monthly means as inputs to avoid a summer time bias when more data is available due to reduced cloud cover.  The daily, weekly, monthly and annual climatological statistics include the entire time series for each specified period.  For example, the climatological January uses the monthly mean from each January in the time series and the climatological annual uses the annual mean from each year.  The CHL and PP climatological statistics include data from both SeaWiFS (1997-2007) and MODIS (2008-2017).  

Weekly, monthly and annual anomalies were calculated for each product by taking the difference between the mean of the input time period (i.e. week, month, year) and the climatological mean for the same period.  Because bio-optical data are typically log-normally distributed [@SOE2], the CHL and PP data were first log-transformed prior to taking the difference and then untransformed, resulting in an anomaly ratio.

The ecological production unit (EPU) shapefile that excludes the estuaries was used to spatially extract all data location within an ecoregion from the statistic and anomaly files.  The median values, which are equivalent to the geometric mean, were used for the CHL and PP data.  For the extended time series, the 1998-2007 data use the SeaWiFS ocean color products and MODIS-Aqua products were used from 2008 to 2017.  Prior to June 2002, AVHRR Pathfinder data are used as the SST source and MUR SST in subsequent years.

### Plotting

The following figures show examples of how Chlorophyll *a* and primary production data were incoporated into the 2017 State of the Ecosystem reports. The figure immediately below shows primary production anomaly plotted with the small-large copepod index (see [Zooplankton](#zooabund)) in the Mid-Atlantic Bight. 

```{r, MAB-zooplankton, fig.cap="Small-large copepod index with primary productivity anomaly in the Mid-Atlantic.", message=F, warning=F}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))

par(mar = c(4,4.75,2,4))

#small-large copepod index
mab_prim_prod <- finder(SOE.data, "annual ppd ratio anomaly MAB")
mab_copepod_anom <- finder(SOE.data, "Small-Large copepod Index MAB")
usr <- par("usr")
plot(NULL,ylim = c(-1.5,1.5), pch = 16, lwd = 3,xlim = c(1998,2017),
       cex = 1, ylab = "", las = 1, xaxt = "n", cex.axis = 1, xlab = "")
abline(h = 0, col  = "grey", lwd = 3, lty = 2) 
with(SOE.data[SOE.data$Var == mab_copepod_anom,], points(Time[22:39], Value[22:39],
       type = "o", col = "black", ylim = c(-1.5,1.5), pch = 16, lwd = 3,las = 1))
mtext(side = 2, line = 3, "Small-Lg Copepod Index", cex = 1.1)

par(new = T)

#primary productivity
with(SOE.data[SOE.data$Var ==  mab_prim_prod[1],], plot(Time, Value,
       type = "o", col =  "springgreen2", ylim = c(.8,1.25), ylab = "", axes = F,log = "y",
       yaxt = "n", pch = 16, lwd = 3, cex = 1, las = 1, xaxt = "n",xlab = ""))
axis(side = 4, las = 1, cex.axis = 1)
axis(side = 1, cex.axis = 1)
mtext(side = 4, line = 3, "Mean Primary Production Ratio", cex = 1.2)
mtext(side = 1, "", cex = 1.2, text = "Year",line = 2.5)
legend("bottomright",
         legend = c("Copepod Index", "Prim. Prod. Ratio"), col = c("black","springgreen2"),
         bty = "n", lwd = 3)

```

Chl *a* and primary production data for 2017-2018 were also examined in relation to the long-term means of each series. The figure below shows data specific to the Mid-Atlantic Bight. 

```{r SST-PPD-CHL-MAB, fig.cap="Primary production (A) and Chl *a* (B) over 2017 (colored polygons) compared against long-term mean (black line) and +/- 1 standard deviation (grey polygon) in the Mid-Atlantic",echo = T, fig.show='hold', fig.align='default',warning = F, message = F,fig.pos='H',fig.height = 4,fig.width = 8}
## SST
par(mar = c(4, 4, 2, 1), mfrow = c(1,2))

## primary production
doy <- seq(1,52,1)
ppd_val_2017 <- SOE.data[SOE.data$Var == "geometric mean ppd 2017 MAB",]$Value
ppd_val_LT <- SOE.data[SOE.data$Var == "geometric mean ppd long term MAB",]$Value
ppd_val_LT_sd <- SOE.data[SOE.data$Var == "sd ppd long term MAB",]$Value

above_mean <- NULL
for (i in 1:length(ppd_val_2017)){
  if (ppd_val_2017[i] >= ppd_val_LT[i]){
    above_mean[i] <- ppd_val_2017[i]
  } else if (ppd_val_2017[i] < ppd_val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(ppd_val_2017)){
  if (ppd_val_2017[i] <= ppd_val_LT[i]){
    below_mean[i] <- ppd_val_2017[i]
  } else if (ppd_val_2017[i] > ppd_val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(ppd_val_2017)){
  if (ppd_val_2017[i] >= ppd_val_LT_sd[i] + ppd_val_LT[i]){
    above_sd[i] <- ppd_val_2017[i]
  } else if (ppd_val_2017[i] < ppd_val_LT_sd [i] + ppd_val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(ppd_val_2017)){
  if (ppd_val_2017[i] <= ppd_val_LT[i] - ppd_val_LT_sd[i]){
    below_sd[i] <- ppd_val_2017[i]
  } else if (ppd_val_2017[i] > ppd_val_LT[i] - ppd_val_LT_sd [i]){
    below_sd[i] <- NA
  }
}


#Lines for polygons
above_sd[is.na(above_sd)] <- ppd_val_LT_sd[which(is.na(above_sd))] + ppd_val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- ppd_val_LT[which(is.na(below_sd))] - ppd_val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- ppd_val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- ppd_val_LT[which(is.na(below_mean))]

upper <- ppd_val_LT_sd + ppd_val_LT
lower <- ppd_val_LT - ppd_val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]),ylim = c(0,2.25), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,8,16,24,32,40,48), cex.axis= 1.25)
mtext(2, line = 2.5, text = expression(paste("Primary Production (gC m"^-2*"d"^-1*")")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(2,2.25*.95,"A", cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (ppd_val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-ppd_val_LT), rev(above_mean)),
        col = "darkolivegreen1", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(ppd_val_LT + ppd_val_LT_sd)), rev(above_sd)),
        col = "darkolivegreen3", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(ppd_val_LT - ppd_val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,ppd_val_LT, type = "l", lwd = 1, "grey90")
#points(doy,ppd_val_2017, type = "l", lwd = .5, "black")
box(lty = 'solid', col = 'black',lwd = 1.25)

par(mar = c(4, 4, 2, 1))
## Chlorophyll a
chl_val_2017 <- SOE.data[SOE.data$Var == "geometric mean chl 2017 MAB",]$Value
chl_val_LT <- SOE.data[SOE.data$Var == "geometric mean chl long term MAB",]$Value
chl_val_LT_sd <- SOE.data[SOE.data$Var == "sd chl long term MAB",]$Value

above_mean <- NULL
for (i in 1:length(chl_val_2017)){
  if (chl_val_2017[i] >= chl_val_LT[i]){
    above_mean[i] <- chl_val_2017[i]
  } else if (chl_val_2017[i] < chl_val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(chl_val_2017)){
  if (chl_val_2017[i] <= chl_val_LT[i]){
    below_mean[i] <- chl_val_2017[i]
  } else if (chl_val_2017[i] > chl_val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(chl_val_2017)){
  if (chl_val_2017[i] >= chl_val_LT_sd[i] + chl_val_LT[i]){
    above_sd[i] <- chl_val_2017[i]
  } else if (chl_val_2017[i] < chl_val_LT_sd [i] + chl_val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(chl_val_2017)){
  if (chl_val_2017[i] <= chl_val_LT[i] - chl_val_LT_sd[i]){
    below_sd[i] <- chl_val_2017[i]
  } else if (chl_val_2017[i] > chl_val_LT[i] - chl_val_LT_sd [i]){
    below_sd[i] <- NA
  }
}


#Lines for polygons
above_sd[is.na(above_sd)] <- chl_val_LT_sd[which(is.na(above_sd))] + chl_val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- chl_val_LT[which(is.na(below_sd))] - chl_val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- chl_val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- chl_val_LT[which(is.na(below_mean))]

upper <- chl_val_LT_sd + chl_val_LT
lower <- chl_val_LT - chl_val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]),ylim = c(0,2), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,8,16,24,32,40,48), cex.axis= 1.25)
mtext(2, line = 2.5, text = expression(paste("Chl a (mg m"^-3*")")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)

# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (chl_val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-chl_val_LT), rev(above_mean)),
        col = "darkolivegreen1", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(chl_val_LT + chl_val_LT_sd)), rev(above_sd)),
        col = "darkolivegreen3", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(chl_val_LT - chl_val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,chl_val_LT, type = "l", lwd = 1, "grey90")
#points(doy,chl_val_2017, type = "l", lwd = .5, "black")
box(lty = 'solid', col = 'black',lwd = 1.25)
text(2,2*.95,"B", cex = 1.5)

```



<!--chapter:end:chapters/CHL_PPD_indicator.Rmd-->

# Zooplankton {#zooabund}

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```

**Description**: Zooplankton abundance anomalies

**Indicator category**: Database pull with analysis; Synthesis of published information; Extensive analysis, not yet published; Published methods

**Contributor(s)**: Ryan Morse
  
**Data steward**: Harvey Walsh, <harvey.walsh@noaa.gov>; Mike Jones, <michael.jones@noaa.gov>
  
**Point of contact**: Ryan Morse, <ryan.morse@noaa.gov>; Harvey Walsh, <harvey.walsh@noaa.gov>
  
**Public availability statement**: Source data are publicly available [here](ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/). Derived data can be found [here](https://comet.nefsc.noaa.gov/erddap/tabledap/zoo_abundance_soe_v1.html).

## Methods

### Data sources
Zooplankton data are from the NOAA MARMAP and EcoMon cruises detailed extensively in @Kane2007, @Kane2011, and @Morse2017.

### Data extraction 
Data are from the publicly available zooplankton dataset on the NOAA FTP server. The excel file has a list of excluded samples and cruises based on @Kane2007 and @Kane2011.

R code used in extraction process.
```{r, echo = T, eval = F}
# load data
URL='ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/EcoMon_Plankton_Data_v3_0.xlsx'
ZPD=openxlsx::read.xlsx(URL, sheet='Data')
```

### Data analysis
Data are processed similarly to @Kane2007 and @Perretti2017a, where a mean annual abundance by date is computed by area for each species meeting inclusion metrics set in @Morse2017. This is accomplished by binning all samples for a given species to bi-monthly collection dates based on median cruise date and taking the mean, then fitting a spline interpolation between mean bi-monthly abundance to give expected abundance on any given day of the year. 

Abundance anomalies are computed from the expected abundance on the day of sample collection. Abundance anomaly time series are constructed for *Centropages typicus*, *Pseudocalanus* spp., *Calanus finmarchicus*, and total zooplankton biovolume. The small-large copepod size index is computed by averaging the individual abundance anomalies of *Pseudocalanus* spp., *Centropages hamatus*, *Centropages typicus*, and *Temora longicornis*, and subtracting the abundance anomaly of *Calanus finmarchicus*. This index tracks the overall dominance of the small bodied copepods relative to the largest copepod in the NEUS region, *Calanus finmarchicus*.


R code used for:
```{r, echo = T, eval = F}

#libraries
library(vegan)
library(stats)
library(mgcv)
library(reshape2)
library(readxl)
library(lubridate)
library(sp)
library(maptools)
library(marmap)
library(rgeos)


# load data
URL='ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/EcoMon_Plankton_Data_v3_0.xlsx'
ZPD=openxlsx::read.xlsx(URL, sheet='Data')
# Fix date, time
dt=as_date(ZPD$date, origin = "1899-12-30")
DOY=yday(dt) #day of year
month=as.numeric(format(dt, '%m'))
year=as.numeric(format(dt, '%Y'))
ZPD$year=year
ZPD$month=month
ZPD$dt=dt
ZPD$DOY=DOY
ZPD$day=as.numeric(format(dt, '%d'))
ZPD$lat2=ceiling(ZPD$lat) #use for binning into 1 degree bins for removal of undersampled bins
ZPD$lon2=floor(ZPD$lon) #use for binning into 1 degree bins for removal of undersampled bins
# ASSIGN EPU based on GPS data
## load shapefiles from EDAB EPU analysis ## not available here

gbk=readShapeSpatial("EPU_GBKPoly.shp")
gom=readShapeSpatial("EPU_GOMPoly.shp")
mab=readShapeSpatial("EPU_MABPoly.shp")
scs=readShapeSpatial("EPU_SCSPoly.shp")
#combine shapefiles GOM and GBK
gom.gbk.shp=gUnion(gom, gbk, byid=F, id=NULL)
gom.gbk.shp=gUnion(gom, gbk, byid=F, id=NULL)
gom.scs.shp=gUnion(gom, scs, byid=F, id=NULL)
mab.gbk.shp=gUnion(mab, gbk, byid=F, id=NULL)
NES.shp=gUnion(mab.gbk.shp, gom.scs.shp, byid=F, id=NULL)
#extract just lat/lons for lines
gbk.lonlat =as.data.frame(lapply(slot(gbk, "polygons"), function(x) lapply(slot(x, "Polygons"), function(y) slot(y, "coords"))))
gom.lonlat =as.data.frame(lapply(slot(gom, "polygons"), function(x) lapply(slot(x, "Polygons"), function(y) slot(y, "coords"))))
mab.lonlat =as.data.frame(lapply(slot(mab, "polygons"), function(x) lapply(slot(x, "Polygons"), function(y) slot(y, "coords"))))
scs.lonlat =as.data.frame(lapply(slot(scs, "polygons"), function(x) lapply(slot(x, "Polygons"), function(y) slot(y, "coords"))))
gom.gbk.lonlat =as.data.frame(lapply(slot(gom.gbk.shp, "polygons"), function(x) lapply(slot(x, "Polygons"), function(y) slot(y, "coords"))))
NES.lonlat =as.data.frame(NES.shp@polygons[[1]]@Polygons[[1]]@coords)# create matrix to use in in.out function
# create matrix to use in in.out function from package 'mgcv'
gom.mat=as.matrix(gom.lonlat)
gbk.mat=as.matrix(gbk.lonlat)
mab.mat=as.matrix(mab.lonlat)
scs.mat=as.matrix(scs.lonlat)
gom.gbk.mat=as.matrix(gom.gbk.lonlat)
# assign samples to EPU
m4=as.matrix(ZPD[,6:5]) #lon,lat from ZPD
ZPD$epu=NA
ZPD$epu[which(in.out(gbk.mat, m4))]='GBK'
ZPD$epu[which(in.out(gom.mat, m4))]='GOM'
ZPD$epu[which(in.out(scs.mat, m4))]='SCS'
ZPD$epu[which(in.out(mab.mat, m4))]='MAB'
test=ZPD[is.na(ZPD$epu),] #unassigned
nms=data.frame(colnames(ZPD)) # column names of orginal data
# limit data set to zooplankton from 1977 on
ZPDb=ZPD[,c(seq(1,14,1), seq(290,297,1), seq(106,197,1))] # check to make sure these are correct against 'nms' if data source changes!!!
ZPDb=ZPDb[order(ZPDb$date),]
ZPDb=ZPDb[which(ZPDb$year > 1976),] # remove NA data in years prior to 1977
# Select only taxa present in yearly data > x percent of samples
X=20 # percent criteria to use as minimum percent in samples
ZPDa=ZPDb
ZPDa=ZPDa[!is.na(ZPDa$zoo_gear),] # Remove NA in zooplankton rows
# Reduce to taxa occurrance > x percent in samples
p.a=ZPDa[,24:114]
p.a[p.a > 0]=1 # presence/absence
count=colSums(p.a)
pct=(count/dim(ZPDa)[1])*100
crit=which(pct>X)
ZPDa=ZPDa[c(1:23,crit+23)] # data limited to taxa occurring in > X percent of samples

#Take median date from each cruise and assign cruise to bimonth for bi-monthly means aggregation
cruises=unique(ZPDa$cruise_name)
for (i in 1:length(cruises)){
  ZPDa$medmonth[ZPDa$cruise_name == cruises[i]]=median(ZPDa$DOY[ZPDa$cruise_name == cruises[i]])
}
ZPDa$bmm=NA
ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(0,59))]=1
ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(60,120))]=3
ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(121,181))]=5
ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(182,243))]=7
ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(244,304))]=9
ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(305,366))]=11

ZPDa[,14]=as.numeric(ZPDa[,14])
ZPDa[,23]=as.numeric(ZPDa[,23])
ZPDsave=ZPDa #from above routine, Yearly (all data) 

SEASON='Yearly'
ZPDa=ZPDsave
# LOG transform data using ZPDa from above (select season first)
test=log10(ZPDa[,24:50]+1) #choose columns with zooplankton data
ZPDlog=ZPDa
ZPDlog[,24:50]=test
nm=matrix(colnames(ZPDlog))

area='GBK'
gbk.yr.spln=data.frame()
for (i in 23:50){
  num=i
  name=nm[num,1]
  mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T)
  func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method="natural",  ties = mean)  
  x.daily=func(seq(1, 12, 0.0302)) #365 days
  gbk.yr.spln=rbind(gbk.yr.spln,x.daily)
}
gbk.yr.spln=t(gbk.yr.spln)
rownames(gbk.yr.spln)=seq(1:365); colnames(gbk.yr.spln)=nm[23:50,1]

area='GOM'
gom.yr.spln=data.frame()
for (i in 23:50){
  num=i
  name=nm[num,1]
  mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T)
  func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method="natural",  ties = mean)
  x.daily=func(seq(1, 12, 0.0302)) #365 days
  gom.yr.spln=rbind(gom.yr.spln,x.daily)
}
gom.yr.spln=t(gom.yr.spln)
rownames(gom.yr.spln)=seq(1:365); colnames(gom.yr.spln)=nm[23:50,1]

area='MAB'
mab.yr.spln=data.frame()
for (i in 23:50){
  num=i
  name=nm[num,1]
  mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T)
  func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method="natural",  ties = mean)
  x.daily=func(seq(1, 12, 0.0302)) #365 days
  mab.yr.spln=rbind(mab.yr.spln,x.daily)
}
mab.yr.spln=t(mab.yr.spln)
rownames(mab.yr.spln)=seq(1:365); colnames(mab.yr.spln)=nm[23:50,1]

area='SCS'
scs.yr.spln=data.frame()
for (i in 23:50){
  num=i
  name=nm[num,1]
  mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T)
  func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method="natural",  ties = mean)
  x.daily=func(seq(1, 12, 0.0302)) #365 days
  scs.yr.spln=rbind(scs.yr.spln,x.daily)
}
scs.yr.spln=t(scs.yr.spln)
rownames(scs.yr.spln)=seq(1:365); colnames(scs.yr.spln)=nm[23:50,1]

# Subtract mean expected value from observed abundance to get anomaly
gbk.anom=ZPDlog[which(ZPDlog$epu=='GBK'),]
gom.anom=ZPDlog[which(ZPDlog$epu=='GOM'),]
mab.anom=ZPDlog[which(ZPDlog$epu=='MAB'),]
scs.anom=ZPDlog[which(ZPDlog$epu=='SCS'),]

gbk.anom.b=data.frame(matrix(NA, nrow = dim(gbk.anom)[1], ncol = dim(gbk.anom)[2]))
for (i in 1:dim(gbk.anom)[1]){
  gbk.anom.b[i,23:50]=gbk.anom[i,23:50]-gbk.yr.spln[which(gbk.anom$DOY[i]==rownames(gbk.yr.spln)),]
}
gom.anom.b=data.frame(matrix(NA, nrow = dim(gom.anom)[1], ncol = dim(gom.anom)[2]))
for (i in 1:dim(gom.anom)[1]){
  gom.anom.b[i,23:50]=gom.anom[i,23:50]-gom.yr.spln[which(gom.anom$DOY[i]==rownames(gom.yr.spln)),]
}
mab.anom.b=data.frame(matrix(NA, nrow = dim(mab.anom)[1], ncol = dim(mab.anom)[2]))
for (i in 1:dim(mab.anom)[1]){
  mab.anom.b[i,23:50]=mab.anom[i,23:50]-mab.yr.spln[which(mab.anom$DOY[i]==rownames(mab.yr.spln)),]
}
scs.anom.b=data.frame(matrix(NA, nrow = dim(scs.anom)[1], ncol = dim(scs.anom)[2]))
for (i in 1:dim(scs.anom)[1]){
  scs.anom.b[i,23:50]=scs.anom[i,23:50]-scs.yr.spln[which(scs.anom$DOY[i]==rownames(scs.yr.spln)),]
}  

scs.anom.b=rbind(scs.anom.b,test)
# Aggregrate by Year, Yearly anomaly by epu
gbk.yr.anom=aggregate(gbk.anom.b, by=list(gbk.anom$year), FUN=mean, na.rm=T); rownames(gbk.yr.anom)=gbk.yr.anom[,1]; gbk.yr.anom[,1]=NULL; colnames(gbk.yr.anom)=colnames(gbk.anom)
gom.yr.anom=aggregate(gom.anom.b, by=list(gom.anom$year), FUN=mean, na.rm=T); rownames(gom.yr.anom)=gom.yr.anom[,1]; gom.yr.anom[,1]=NULL; colnames(gom.yr.anom)=colnames(gom.anom)
mab.yr.anom=aggregate(mab.anom.b, by=list(mab.anom$year), FUN=mean, na.rm=T); rownames(mab.yr.anom)=mab.yr.anom[,1]; mab.yr.anom[,1]=NULL; colnames(mab.yr.anom)=colnames(mab.anom)
scs.yr.anom=aggregate(scs.anom.b, by=list(scs.anom$year), FUN=mean, na.rm=T); rownames(scs.yr.anom)=scs.yr.anom[,1]; scs.yr.anom[,1]=NULL; colnames(scs.yr.anom)=colnames(scs.anom)

# Small-Large body copepod abundance anomaly
lgtx=c(25) #column for Calanus finmarchicus
smtx=c(26,24,28,27) #Pseudocalanus spp, Centropoges typicus, Centropages hamatus, Temora longicornis

dataTsm=gbk.yr.anom[,smtx]
dataTlg=gbk.yr.anom[,lgtx]
anom.gbk=rowMeans(dataTsm)-(dataTlg)
dataTsm=gom.yr.anom[,smtx]
dataTlg=gom.yr.anom[,lgtx]
anom.gom=rowMeans(dataTsm)-(dataTlg)
dataTsm=mab.yr.anom[,smtx]
dataTlg=mab.yr.anom[,lgtx]
anom.mab=rowMeans(dataTsm)-(dataTlg)
dataTsm=scs.yr.anom[,smtx]
dataTlg=scs.yr.anom[,lgtx]
anom.scs=rowMeans(dataTsm)-(dataTlg)
```

### Plotting

```{r, zooplankton-abundance, fig.cap = "*Centropages typicus* abundance anomaly in the Mid-Atlantic Bight.", message = F, warning=F}
# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))


#C. typicus abundance anomaly
opar <- par(mar = c(4,4.75,2,4))
soe.plot(SOE.data, "Time", "Centropages typicus abundance anomaly MAB", 
         anomaly = T, stacked = "A", suppressAxis = T, x.start = 1986, cex.stacked = 1.75)
soe.stacked.axis("Year", "Abundance anomaly", rel.x.text = 1.2, rel.y.text = 1.2, 
                 y.line = 3.1, outer = F)

```


<!--chapter:end:chapters/Zooplankton_indicators.Rmd-->

# Fish Condition Indicator

**Description**: Relative condition

**Indicator category**: Database pull with analysis

**Contributor(s)**: Laurel Smith
  
**Data steward**: Laurel Smith, <laurel.smith@noaa.gov>
  
**Point of contact**: Laurel Smith, <laurel.smith@noaa.gov>
  
**Public availability statement**: NEFSC survey data used in these analyses are available upon request (see [BTS metadata](https://inport.nmfs.noaa.gov/inport/item/22560) for access procedures). Derived condition data are available [here](https://comet.nefsc.noaa.gov/erddap/tabledap/gf_condition_soe_v1.html).



## Methods
Relative condition (Kn) was introduced by @Cren1951a as a way to remove the influence of length on condition, and @Blackwell2000 noted that Kn may be useful in detecting prolonged physical stress on a fish populations. Relative condition is calculated as
$Kn = W/W',$ where W is the weight of an individual fish and W' is the predicted length-specific mean weight for the fish population in a given region. "Here, relative condition was calculated for finfish stocks commonly caught on the Northeast Fisheries Science Centerâ€™s (NEFSC) autumn and spring bottom trawl surveys, from 1992-present. 

Where data allowed, predicted length-weight parameters were calculated for Wâ€™ by species, sex and season over the time period 1992-2012. When sample sizes of individual fish weights and lengths were too low, parameters were calculated for aggregated spring and fall survey data over the same time period. Fall survey relative condition was calculated by sex for those species that exhibited differences in growth between sexes and aggregated across sex for those that did not.

### Data sources
Individual fish lengths (to the nearest 0.5 cm) and weights (grams) were collected on the NEFSC bottom trawl surveys from 1992-present aboard RVs Albatross IV, Delaware II and the Henry B. Bigelow  (see [Survdat](#survdat)). A small number of outlier values were removed when calculating the length-weight parameters.

### Data extraction
Data were extracted from NEFSC's survey database (SVDBS) using SQL. 

SQL query:
```{sql, eval = F, echo = T}
SELECT cruise6,stratum,tow,station,
  year,month,day,time,beglat,beglon,setdepth,
  surftemp,bottemp,
  svspp,sex,length,age,maturity,indid,indwt,stom_volume,stom_wgt, expcatchwt, expcatchnum
from connection to oracle
(select b.cruise6,b.stratum,b.tow,b.station,
  s.est_year year,est_month month,est_day day,
  substr(est_time,1,2)||substr(est_time,4,2) time,
  round(substr(beglat,1,2) + (substr(beglat,3,7)/60),6) beglat,
  round(((substr(beglon,1,2) + (substr(beglon,3,7)/60)) * -1), 6) beglon,
  setdepth,surftemp, bottemp,
  b.svspp,sex,length,age,maturity,indid,indwt,stom_volume,stom_wgt, expcatchwt, expcatchnum
from union_fscs_svbio b, union_fscs_svcat p, union_fscs_svsta s, svdbs_cruises c
where 
  season = &sson and
    b.svspp in ('013','015','023','026','028','032','072','073','074','075','076','077','078','102','103','104','105','106','107','108','121','131','135','141','143','145','155','164','193','197') and
  (b.cruise6=s.cruise6) and
  (c.cruise6=b.cruise6) and
  (p.cruise6=c.cruise6) and
  (p.stratum=b.stratum) and
  (b.stratum=s.stratum) and
  (p.station=b.station) and
  (b.station=s.station) and
  (p.svspp=b.svspp) and
  (p.tow=b.tow) and
  (b.tow=s.tow) );

  %put &sqlxmsg;
  %put &sqlxrc;

create view spp as

select comname, svspp
from connection to oracle
(select comname, svspp
from svspecies_list);

  %put &sqlxmsg;
  %put &sqlxrc;

execute (commit) by oracle;

```

### Data analysis
The following growth curve was fit through individual fish lengths and weights from the NEFSC bottom trawl survey data from 1992-2012 to produce reference length-weight parameters:


$$\textrm{Weight} = e^{Fall_{coef}} * \textrm{Length}^{Fall_{exp}},$$

where length is in cm and weight is in kg. Fall survey data were used where sample sizes allowed for growth curve estimation, otherwise data from spring and fall seasons were combined. 

Individual fish lengths from NEFSC fall bottom trawl survey from 1992-2017 were then used to calculate predicted weights using the reference length-weight parameters. Relative condition (Kn) was calculated annually by species and sex (for sexually dimorphic species) by dividing individual fish weights by the predicted weight. 

The following R code was used in the analysis:
```{r, echo = T, eval = F}
# Length-weight parameter calculation:
function (data, min.n = 25, min.range = 5, data.avail = NA, data.avail.bigelow = NA) 
{
	if(is.null(dim(data.avail))) data.avail <- lw.data.availability(data, min.n, min.range)
	data.avail <- data.avail[apply(data.avail[,2:5], 1, any),]
	if(is.null(dim(data.avail.bigelow)))data.avail.bigelow <- lw.data.availability(data[data$data.source == "Bigelow",], min.n, min.range)
	data.avail.bigelow <- data.avail.bigelow[apply(data.avail.bigelow[,2:5], 1, any),]
	data.spp <- as.numeric(rownames(data.avail[data.avail$sex.season == TRUE,]))
	lw.output <- data.frame(matrix(ncol = 12))
	names(lw.output) <- c("species.name", "species.code", "source", "sex", "season", "slope", "slope.p", "intercept", "intercept.p", "min.length", "max.length", "check.diff")
	for(sp in data.spp){
		sp.data <- lw.data[lw.data$species == sp,]
		sp.name <- unique(as.character(species.names$scientific_name[species.names$svspp == sp]))
		print(sp.name)
#All model
		print("Species Level")
		this.data <- bigelow.test(sp.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], "weight.log~length.log", "species")
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log, data = this.data)
			model.coefs <- coef(this.model)
			model.summary <- coef(summary(this.model))
			length.range <- range(sp.data$length)
			length.log <- log(seq(length.range[1], length.range[2], by=.5))
			species <- rep(sp, length(length.log))
			predict.length <- data.frame(species, length.log)
			check.diffs <- plot.lw(this.data, this.model, "species", sp.name, predict.length)
			lw.output <- rbind(lw.output, c(sp.name, sp, ds, NA, NA, model.coefs["length.log"], model.summary["length.log", "Pr(>|t|)"], model.coefs["(Intercept)"], model.summary["(Intercept)", "Pr(>|t|)"], length.range[1], length.range[2], check.diffs))
		}
#Sex model
		print("Sex Level")
		this.data <- sp.data[sp.data$sex > 0,]
		model.definition <- "weight.log~length.log * factor(sex)"
		this.data <- bigelow.test(this.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, "sex")
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log * factor(sex), data = this.data)
			male.range <- range(this.data$length[this.data$sex == 1])
			female.range <- range(this.data$length[this.data$sex == 2])			
			model.summary <- coef(summary(this.model))
			if(any(model.summary[grep("sex", rownames(model.summary)), "Pr(>|t|)"] <= .05)){
				model.coefs <- coef(this.model)
				length.log <- rep(log(seq(length.range[1], length.range[2], by=.5)),2)
				sex <- c(rep(1, length(length.log)/2), rep(2, length(length.log)/2))
				predict.length <- data.frame(length.log, sex)
				check.diffs <- plot.lw(this.data, this.model, "sex", sp.name, predict.length)
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 1, NA, model.coefs["length.log"], model.summary["length.log", "Pr(>|t|)"], model.coefs["(Intercept)"], model.summary["(Intercept)", "Pr(>|t|)"], male.range[1], male.range[2], check.diffs[1]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 2, NA, model.coefs["length.log"] + model.coefs["length.log:factor(sex)2"], model.summary["length.log:factor(sex)2", "Pr(>|t|)"], model.coefs["(Intercept)"] + model.coefs["factor(sex)2"], model.summary["factor(sex)2", "Pr(>|t|)"], female.range[1], female.range[2], check.diffs[2]))
			}
			else{
				print(paste("Model parameters not significantly different for", model.definition))
			}
		}
#Season model
		model.definition <- "weight.log~length.log * factor(season)"
		this.data <- bigelow.test(sp.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, "season")
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log * factor(season), data = this.data)
			fall.range <- range(this.data$length[this.data$season == "FALL"])
			spring.range <- range(this.data$length[this.data$season == "SPRING"])
			model.summary <- coef(summary(this.model))
			if(any(model.summary[grep("season", rownames(model.summary)), "Pr(>|t|)"] <= .05)){
				model.coefs <- coef(this.model)
				length.log <- rep(log(seq(length.range[1], length.range[2], by=.5)),2)
				season <- c(rep("FALL", length(length.log)/2), rep("SPRING", length(length.log)/2))
				predict.length <- data.frame(length.log, season)
				check.diffs <- plot.lw(this.data, this.model, "season", sp.name, predict.length)
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, NA, "FALL", model.coefs["length.log"], model.summary["length.log", "Pr(>|t|)"], model.coefs["(Intercept)"], model.summary["(Intercept)", "Pr(>|t|)"], fall.range[1], fall.range[2], check.diffs[1]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, NA, "SPRING", model.coefs["length.log"] + model.coefs["length.log:factor(season)SPRING"], model.summary["length.log:factor(season)SPRING", "Pr(>|t|)"], model.coefs["(Intercept)"] + model.coefs["factor(season)SPRING"], model.summary["factor(season)SPRING", "Pr(>|t|)"], spring.range[1], spring.range[2], check.diffs[1]))
			}
			else{
				print(paste("Model parameters not significantly different for", model.definition))
			}
		}
#Sex-Season model
		this.data <- sp.data[sp.data$sex > 0,]
		model.definition <- "weight.log~length.log * factor(sex) * factor(season)"
		this.data <- bigelow.test(this.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, c("sex","season"))
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log * factor(sex) * factor(season), data = this.data)
			model.summary <- coef(summary(this.model))
			male.fall.range <- range(this.data$length[this.data$season == "FALL" & this.data$sex == 1])
			male.spring.range <- range(this.data$length[this.data$season == "SPRING" & this.data$sex == 1])
			female.fall.range <- range(this.data$length[this.data$season == "FALL" & this.data$sex == 2])
			female.spring.range <- range(this.data$length[this.data$season == "SPRING" & this.data$sex == 2])
			if(any(model.summary[grep("season", rownames(model.summary)), "Pr(>|t|)"] <= .05 | any(model.summary[grep("season", rownames(model.summary)), "Pr(>|t|)"] <= .05))){
				model.coefs <- coef(this.model)
				male.fall.int <- model.coefs["(Intercept)"]
				male.fall.int.p <- model.summary["(Intercept)", "Pr(>|t|)"]
				male.fall.slope <- model.coefs["length.log"]
				male.fall.slope.p <- model.summary["length.log", "Pr(>|t|)"]
				male.spring.int <- male.fall.int + model.coefs["factor(season)SPRING"]
				male.spring.int.p <- model.summary["factor(season)SPRING", "Pr(>|t|)"]
				male.spring.slope <- male.fall.slope + model.coefs["length.log:factor(season)SPRING"]
				male.spring.slope.p <- model.summary["length.log:factor(season)SPRING", "Pr(>|t|)"]
				female.fall.int <- male.fall.int + model.coefs["factor(sex)2"]
				female.fall.int.p <- model.summary["factor(sex)2", "Pr(>|t|)"]
				female.fall.slope <- male.fall.slope + model.coefs["length.log:factor(sex)2"]
				female.fall.slope.p <- model.summary["length.log:factor(sex)2", "Pr(>|t|)"]
				female.spring.int <- male.spring.int + model.coefs["factor(sex)2"] + model.coefs["factor(sex)2:factor(season)SPRING"]
				female.spring.int.p <- model.summary["factor(sex)2:factor(season)SPRING",  "Pr(>|t|)"]
				female.spring.slope <- male.spring.slope + model.coefs["length.log:factor(sex)2"] + model.coefs["length.log:factor(sex)2:factor(season)SPRING"]
				female.spring.slope.p <- model.summary["length.log:factor(sex)2:factor(season)SPRING", "Pr(>|t|)"]
				length.log <- rep(log(seq(length.range[1], length.range[2], by=.5)),4)
				sex <- c(rep("1", length(length.log)/2), rep("2", length(length.log)/2))
				season <- rep(c(rep("FALL", length(length.log)/4), rep("SPRING", length(length.log)/4)),2)
				predict.length <- data.frame(length.log, sex, season)
				check.diffs <- plot.lw(this.data, this.model, c("sex", "season"), sp.name, predict.length)
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 1, "FALL", male.fall.slope, male.fall.slope.p, male.fall.int, male.fall.int.p, male.fall.range[1], male.fall.range[2], check.diffs[1]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 1, "SPRING", male.spring.slope, male.spring.slope.p, male.spring.int, male.spring.int.p, male.spring.range[1], male.spring.range[2],check.diffs[2]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 2, "FALL", female.fall.slope, female.fall.slope.p, female.fall.int, female.fall.int.p, female.fall.range[1], female.fall.range[2],check.diffs[3]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 2, "SPRING", female.spring.slope, female.spring.slope.p, female.spring.int, female.spring.int.p, female.spring.range[1], female.spring.range[2],check.diffs[4]))
			}
			else{
				print(paste("Model parameters not significantly different for", model.definition))
			}
		}
	}
lw.output <- lw.output[!is.na(lw.output$species.code),]
lw.output
}
#Relative Condition:
proc import datafile = "lw_parameters.csv"
 out = LWparams
 dbms = csv
 replace;
 getnames = yes;
run;

data LWparams; set LWparams;
 if LW_SVSPP = 13 then svspp = '013';
 if LW_SVSPP = 15 then svspp = '015';
 if LW_SVSPP = 23 then svspp = '023';
 if LW_SVSPP = 26 then svspp = '026';
 if LW_SVSPP = 28 then svspp = '028';
 if LW_SVSPP = 32 then svspp = '032';
 if LW_SVSPP = 72 then svspp = '072';
 if LW_SVSPP = 73 then svspp = '073';
 if LW_SVSPP = 74 then svspp = '074';
 if LW_SVSPP = 75 then svspp = '075';
 if LW_SVSPP = 76 then svspp = '076';
 if LW_SVSPP = 77 then svspp = '077';
 if LW_SVSPP = 78 then svspp = '078';
 if LW_SVSPP ge 100 then svspp = LW_SVSPP;
 if sexMF = 'M' then sex = '1';
 if sexMF = 'F' then sex = '2';
 if sexMF = ' ' then sex = '0';
 if EXPONENT_FALL = . then EXPONENT_FALL=SEASONLESS_EXPONENT;
 if EXPONENT_SPRING = . then EXPONENT_SPRING=SEASONLESS_EXPONENT;
 if COEFFICIENT_FALL = . then COEFFICIENT_FALL=SEASONLESS_COEFFICIENT;
 if COEFFICIENT_SPRING = . then COEFFICIENT_SPRING=SEASONLESS_COEFFICIENT;

proc sort data=LWparams;
 by svspp sex;

proc sort data=lenwt;
 by svspp sex;

data lwdatpar (keep =cruise6 stratum tow station year month day time beglat beglon setdepth
  surftemp bottemp svspp sex length age maturity indid indwt stom_volume stom_wgt expcatchwt expcatchnum
  COEFFICIENT_SPRING EXPONENT_SPRING COEFFICIENT_FALL EXPONENT_FALL SEASONLESS_COEFFICIENT 
  SEASONLESS_EXPONENT);
 merge lenwt (in=d) LWparams (in=p);
 by svspp sex;

data sortlw; set lwdatpar;
 proc sort; by svspp sex year;

data lwdata; set sortlw;
 if indwt = . then delete;
 if length = . then delete;
 if indwt >0;
 svspp1 = svspp*1;
 indwtg=indwt*1000.0;
 cond=indwtg/(length**3);
 if EXPONENT_FALL gt 0 then predwt = (exp(COEFFICIENT_FALL))*length**EXPONENT_FALL;
 if EXPONENT_FALL = . then predwt = (exp(SEASONLESS_COEFFICIENT))*length**SEASONLESS_EXPONENT;
  if EXPONENT_FALL gt 0 then predwtPK = exp(COEFFICIENT_FALL+(EXPONENT_FALL*log(length)));
 if EXPONENT_FALL = . then predwtPK = exp(SEASONLESS_COEFFICIENT+(SEASONLESS_EXPONENT*log(length)));

***Relative condition;
 RelWt = indwt/predwt*100;

proc sort data=lwdata;
 by svspp1 sex year;
run;

```

### Plotting

```{r condition-factor, eval = T, fig.width = 8, fig.cap = "Normalized condition factors of managed species in the Northeast Large Marine Ecosystem.", message=F, warning=F, fig.height = 7.5}

library(dplyr)
library(tidyr)
library(stringr)

data.dir <- here::here("data")

#Function for normalizing 
ztrans <- function(x){
    meanx <- mean(x, na.rm = T)
    sdx   <- sd(  x, na.rm = T)
    z     <- (x - meanx) / sdx
    return(z)
}

#Get data
load(file.path(data.dir, "SOE_data_erddap.Rdata"))

CF <- SOE.data %>% filter(str_detect(Var, "condition"))

#Processing
CF_mat <- CF %>%
  group_by(Var) %>% 
  mutate(Value = ztrans(Value)) %>% #Normalize
  tidyr::spread(.,Time,Value) %>% #Convert to wide for plotting
  arrange(Var) %>% 
  dplyr::select(-Units, -EPU) 
  

#Figure palette
graph.colors<-colorRampPalette(c('#C6E2FF','#00008B'))

#Main figure
par(mar = c(2, 2, 3, 18), fig = c(0, 1, 0.1, 1))


#Image rotates matrix 90 degrees so use transverse matrix
image(z = t(as.matrix(CF_mat[,2:26])),
      breaks = c(-10, -1, 0, 1, 10), col = graph.colors(4),
      xlim = c(0,1), ylim = c(0-0.037,1+0.037), axes = F,
      xlab = '', ylab = '', useRaster = T)


#Set y axis labels
row_names <- CF_mat$Var
mtext(text = row_names,side = 4, las = 1,
      cex = 0.7, at = seq(0,1,0.0303),
      line = 1)

#Figure key
par(mar = c(0.1, 0.5, 2.25, 0.5),
    fig = c(0.2, 0.5, 0, 0.1),
    new = T)
image(x = seq(0,1,1/4), z = t(matrix(1:4,1,4)), col = graph.colors(4), axes = F, 
      xlim = c(0,1), ylim = c(0,1))
axis(3, at =  seq(0,1,1/4), labels = c(-3, -1, 0, 1, 3), cex = 2)



```


<!--chapter:end:chapters/Condition_indicator.Rmd-->

# Fish Productivity Indicator
 

```{r,  echo = F, message=F}

#Load packages
library(knitr)
library(rmarkdown)

```
**Description**: Groundfish productivity estimated as the ratio of small fish to large fish

**Indicator category**: Database pull with analysis; Published methods

**Contributor(s)**: Charles Perretti
  
**Data steward**: Charles Perretti, <charles.perretti@noaa.gov>
  
**Point of contact**: Charles Perretti, <charles.perretti@noaa.gov>
  
**Public availability statement**: Source data are available upon request.



## Methods


### Data sources
Survey data from NEFSC trawl database. These data in their derived form are available through [Survdat](#survdat).


### Data extraction 
Data were extracted from [Survdat](#survdat).


### Data analysis
We defined size thresholds separating small and large fish for each species based on the 20th percentile of the length distribution across all years. This threshold was then used to calculate a small and large fish index (numbers below and above the threshold, respectively) each year. Although the length percentile corresponding to age-1 fish will vary with species, we use the 20th percentile as an approximation. Biomass was calculated using lengthâ€“weight relationships directly from the survey data. Following @wigley_length-weight_2003, the length-weight relationship was modeled as 
$$\ln W = \ln a + b \ln L$$
where $W$ is weight (kg), $L$ is length (cm), and $a$ and $b$ are parameters fit via linear regression. The ratio of small fish numbers of the following year to larger fish biomass in the current year was used as the index of recruitment success. The fall and spring recruitment success anomalies were averaged to provide an annual index of recruitment success.

Further details of methods described in @perretti_regime_2017.

### Plotting

```{r, echo = T, eval = T, message=F, warning=F, fig.cap = "Groundfish productivity across all stocks in the Northeast Large Marine Ecosystem."}


# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

#### Functions for plotting ####
library(ggplot2)
library(rpart)
library(dplyr)
library(stringr)

all_epu <- SOE.data %>%
  filter(str_detect(Var, "All EPU productivity"))

all_epu$Var <- str_trim(str_replace(all_epu$Var, "All EPU productivity",""))


# Adjust plot properties
adjustAxes <- 
  ggplot2::theme(axis.title   = element_text(size = 18),
                 axis.text    = element_text(size = 15),
                 plot.title   = element_text(size = 20))

ggplot <- function(...) { ggplot2::ggplot(...)  + 
    ggplot2::theme_bw() + 
    adjustAxes}


# Plot stacked bar with cpts for single var 
plot_stackbarcpts_single <- function(YEAR, var2bar,
                                     x, xlab, ylab,
                                     titl,
                                     file_suffix,
                                     leg_font_size = 10,
                                     remove_leg = FALSE,
                                     leg_ncol = 1,
                                     wcpts = TRUE,
                                     wdashed = TRUE,
                                     height = 5.5,
                                     width = 8) {
  
  dat2bar <- data.frame(YEAR, var2bar,
                        x)
  
  dat2plot <-
    dat2bar %>%
    tidyr::gather(variable, value, -YEAR, -var2bar) %>%
    dplyr::mutate(var2bar = gsub(pattern      = "_", 
                                 replacement  = " ", 
                                 x            = var2bar),
                  var2bar = gsub(pattern      = "Atl.", 
                                 replacement  = "", 
                                 x            = var2bar),
                  var2bar = gsub(pattern      = "Atl", 
                                 replacement  = "", 
                                 x            = var2bar),
                  var2bar = gsub(pattern      = "NS and combined", 
                                 replacement  = "", 
                                 x            = var2bar),
                  var2bar = gsub(pattern      = "YT", 
                                 replacement  = "Yellowtail", 
                                 x            = var2bar),
                  var2bar = gsub(pattern      = " GoM", 
                                 replacement  = " GOM", 
                                 x            = var2bar))
  
  
  p <-   
    ggplot(dat2plot,
           aes(x = YEAR)) +
    geom_bar(data = dat2plot %>% filter(value > 0),
             aes(y = value, fill = var2bar),
             stat = "identity") +
    geom_bar(data = dat2plot %>% filter(value < 0),
             aes(y = value, fill = var2bar),
             stat = "identity") +
    geom_hline(size = 0.3, aes(yintercept = 0)) +
    xlab(xlab) +
    ylab(ylab) +
    ggtitle(titl) +
    guides(fill = guide_legend(ncol = leg_ncol)) +
    theme(axis.title   = element_text(size = 16),
          axis.text    = element_text(size = 15),
          plot.title   = element_text(size = 20),
          legend.text  = element_text(size = leg_font_size),
          legend.title = element_blank())
  
  if(remove_leg) p <- p + theme(legend.position = "none")
  
  print(p)
  
#  ggsave(plot = p,
#         filename = "./productivity_all.eps",
#         width = width,
#         height = height)
}

# Plot stacked bars
plot_stackbarcpts <- function(YEAR, var2bar,
                              top, mid, bot,
                              top_lab, 
                              mid_lab, 
                              bot_lab,
                              xlab = "", 
                              ylab = "",
                              titl = "") {
  
  dat2bar <- data.frame(YEAR, var2bar,
                        top, mid, 
                        bot)
  
  dat2plot <-
    dat2bar %>%
    tidyr::gather(variable, value, -YEAR, -var2bar) %>%
    dplyr::mutate(variable = ifelse(variable == "top", 
                                    top_lab, 
                                    ifelse(variable == "mid",
                                           mid_lab, bot_lab)))
  
  
  dat2plot$variable <- 
    factor(dat2plot$variable,
           levels = c(top_lab, mid_lab, bot_lab))
  
  p <-   
    ggplot(dat2plot,
           aes(x = YEAR)) +
    geom_bar(data = dat2plot %>% filter(value > 0),
             aes(y = value, fill = var2bar),
             stat = "identity") +
    geom_bar(data = dat2plot %>% filter(value < 0),
             aes(y = value, fill = var2bar),
             stat = "identity") +
    facet_wrap(~ variable, ncol = 1) +
    geom_hline(size = 0.3, aes(yintercept = 0)) +
    xlab(xlab) +
    ylab(ylab) +
    ggtitle(titl) +
    guides(fill = guide_legend(ncol = 1)) +
    theme(axis.title   = element_text(size = 16),
          axis.text    = element_text(size = 15),
          plot.title   = element_text(size = 20),
          strip.text   = element_text(size = 15),
          legend.title = element_blank())
  
  print(p)
}

# Recruit per spawner (all stocks in one figure)
plot_stackbarcpts_single(YEAR = all_epu$Time,
                         var2bar = all_epu$Var,
                         x = all_epu$Value,
                         titl = "",
                         xlab = "",
                         ylab = "Small fish per large fish biomass (anomaly)",
                         height = 7,
                         width = 9)

```

```{r plot-all, fig.height=8, fig.cap = "Groundfish productivity visualized by EPU."}

by_epu <- SOE.data %>% filter(str_detect(Var, "productivity"),
                                                    !EPU %in% c("All","SS"))
by_epu$Var <- str_trim(str_replace(by_epu$Var,
                                   " GOM productivity| GB productivity| MAB productivity", ""))

# Recruit per spawner (by EPU)
dat2plot <-
  by_epu %>%
  tidyr::spread(EPU, Value)


plot_stackbarcpts(YEAR = dat2plot$Time,
                  var2bar = dat2plot$Var,
                  top = dat2plot$GOM, 
                  mid = dat2plot$GB,
                  bot = dat2plot$MAB,
                  top_lab = "Gulf of Maine",
                  mid_lab = "Georges Bank",
                  bot_lab = "Mid. Atlantic Bight",
                  xlab = "",
                  ylab = "Small fish per Large fish biomass (anomaly)")

```


<!--chapter:end:chapters/productivity_for_tech_memo.Rmd-->

# References {-}

```{r include=FALSE}
#`r if (knitr::is_html_output()) '# References {-}'`
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'htmlwidgets', 'webshot', 'DT',
  'miniUI', 'tufte', 'servr', 'citr', 'rticles'
), 'packages.bib')
```

<!--chapter:end:chapters/references.Rmd-->

