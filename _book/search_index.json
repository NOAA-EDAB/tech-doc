[
["index.html", "Technical Documentation, State of the Ecosystem Report Introduction", " Technical Documentation, State of the Ecosystem Report Northeast Fisheries Science Center 22 August 2019 Introduction The purpose of this document is to collate the methods used to access, collect, process, and analyze derived data (“indicators”) used to describe the status and trend of social, economical, ecological, and biological conditions in the Northeast Shelf Large Marine Ecosystem (see figure, below). These indicators are further synthesized in State of the Ecosystem Reports produced annually by the Northeast Fisheries Science Center for the New England Fisheries Management Council and the Mid-Atlantic Fisheries Management Council. The metadata for each indicator (in accordance with the Public Access to Research Results (PARR) directive) and the methods used to construct each indicator are described in the subsequent chapters, with each chapter title corresponding to an indicator or analysis present in State of the Ecosystem Reports. Indicators included in this document were selected to clearly align with management objectives, which is required for integrated ecosystem assessment (Levin et al. 2009), and has been advised many times in the literature (Degnbol and Jarre 2004; Jennings 2005; Rice and Rochet 2005; Link 2005). A difficulty with pratical implementation of this in ecosystem reporting can be the lack of clearly specified ecosystem-level management objectives (although some have been suggested (Murawski 2000)). In our case, considerable effort had already been applied to derive both general goals and operational objectives from both US legislation such as the Magnuson-Stevens Fisheries Conservation and Management Act (MSA) and regional sources (DePiper et al. 2017). These objectives are somewhat general and would need refinement together with managers and stakeholders, however, they serve as a useful starting point to structure ecosystem reporting. Map of Northeast U.S. Continental Shelf Large Marine Ecosystem from Hare et al. (2016). References "],
["erddap.html", "1 Data and Code Access", " 1 Data and Code Access 1.0.1 About The Technical Documentation for the State of the Ecosystem reports is a bookdown document; hosted on the NOAA Northeast Fisheries Science Center Ecosystems Dynamics and Assessment Branch Github page, and developed in R. Derived data used to populate figures in this document are queried directly from the ecodata R package or the NEFSC ERDDAP server. ERDDAP queries are made using the R package rerddap. 1.0.2 Accessing source data and build code In this technical documentation, we hope to shine a light on the processing and analytical steps involved to get from source data to final product. This means that whenever possible, we have included the code involved in source data extraction, processing, and analyses. We have also attempted to thoroughly describe all methods in place of or in supplement to provided code. Example plotting code for each indicator is presented in sections titled “Plotting”, and these code chunks can be used to recreate the figures presented in State of the Ecosystem reports1. Source data for the derived indicators in this document are linked to in the text unless there are privacy concerns involved. In that case, it may be possible to access source data by reaching out to the Point of Contact associated with that data set. Derived data sets make up the majority of the indicators present in the State of the Ecosystem reports, and these data sets are available for download through the ecodata R package. 1.0.3 Building the document Start a local build of the SOE bookdown document by first cloning the project’s associated git repository. Next, if you would like to build a past version of the document, use git checkout [version_commit_hash] to revert the project to a past commit of interest, and set build_latest &lt;- FALSE in the following code chunk. This will ensure the project builds from a cached data set, and not the most updated versions present on the NEFSC ERDDAP server. Once the tech-doc.Rproj file is opened in RStudio, run bookdown::serve_book() from the console to build the document. build_latest &lt;- FALSE if (build_latest){ # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) #Source function for querying ERDDAP server source(file.path(r.dir,&quot;get_erddap.R&quot;)) #Set URL for COMET (server where NEFSC ERDDAP lives) comet &lt;- &#39;https://comet.nefsc.noaa.gov/erddap/&#39; #List datasets on the NEFSC ERDDAP tab_list &lt;- ed_datasets(url = comet) #Get updated data set IDs erddap_datasets &lt;- tab_list %&gt;% filter(str_detect(Dataset.ID, &quot;soe_v&quot;)) %&gt;% get_erddap(id = NULL) #Save and clean updated IDs for use in rest of report save(erddap_datasets, file = file.path(data.dir, &quot;ERDDAP_datasets.Rdata&quot;)) # Exclude stock assessment status data, which have unique structure erddap_datasets &lt;- erddap_datasets %&gt;% dplyr::filter(!str_detect(Dataset.ID, &quot;assess&quot;)) #Create SOE parent data set, filter out NAs. This queries based on #data set IDs that were collected above SOE.data.erd &lt;- sprintf(&quot;http://comet.nefsc.noaa.gov/erddap/tabledap/%s.csv&quot;, erddap_datasets$Dataset.ID) %&gt;% purrr::map(function(x) { readr::read_csv(url(x)) }) %&gt;% do.call(rbind,.) %&gt;% mutate(Value = as.numeric(Value)) %&gt;% dplyr::filter(!is.na(Value)) #Convert to data.table SOE.data &lt;- as.data.table(SOE.data.erd) #Save data save(SOE.data, file = file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) } 1.0.3.1 A note on data structures The majority of the derived time series used in State of the Ecosystem reports are in long format. This approach was taken so that all disparate data sets could be “bound” together for ease of use in our base plotting functions. There are multiple R scripts sourced throughout this document in an attempt to keep code concise. These scripts include BasePlot_source.R, GIS_source.R, and get_erddap.R. The scripts BasePlot_source.R and GIS_source.R refer to deprecated code used prior to the 2019 State of the Ecosystem reports. Indicators that were not included in reports after 2018 make use of this syntax, whereas newer indicators typically use ggplot2 for plotting.↩ "],
["aggroups.html", "2 Aggregate Groups 2.1 Methods", " 2 Aggregate Groups Description: Mappings of species into aggregate group categories for different analyses Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Synthesis of published information Contributor(s): Geret DePiper, Sarah Gaichas, Sean Hardison, Sean Lucey Data steward: Sean Lucey Sean.Lucey@noaa.gov Point of contact: Sean Lucey Sean.Lucey@noaa.gov Public availability statement: Source data is available to the public (see Data Sources). 2.1 Methods The State of the Ecosystem (SOE) reports are delivered to the New England Fishery Management Council (NEFMC) and Mid-Atlantic Fishery Management Council (MAFMC) to provide ecosystems context. To better understand that broader ecosystem context, many of the indicators are reported at an aggregate level rather than at a single species level. Species were assigned to an aggregate group following the classification scheme of Garrison and Link (2000) and Link et al. (2006). Both works classified species into feeding guilds based on food habits data collected at the Northeast Fisheries Science Center (NEFSC). In 2017, the SOE used seven specific feeding guilds (plus an “other” category; Table 2.1). These seven were the same guilds used in Garrison and Link (2000), which also distinguished ontogentic shifts in species diets. For the purposes of the SOE, species were only assigned to one category based on the most prevalent size available to commercial fisheries. However, several of those categories were confusing to the management councils, so in 2018 those categories were simplified to five (plus “other”; Table 2.2) along the lines of Link et al. (2006). In addition to feeding guilds, species managed by the councils have been identified. This is done to show the breadth of what a given council is responsible for within the broader ecosystem context. Table 2.1: Aggregate groups use in 2017 SOE. Classifications are based on Garrison and Link 2000. Feeding.Guild Description Apex Predator Top of the food chain Piscivore Fish eaters Macrozoo-piscivore Shrimp and small fish eaters Macroplanktivore Amphipod and shrimp eaters Mesoplanktivore Zooplankton eaters Benthivore Bottom eaters Benthos Things that live on the bottom Other Things not classified above Table 2.2: Aggregate groups use in 2018 SOE. Classifications are based on Link et al. 2006. Feeding.Guild Description Apex Predator Top of the food chain Piscivore Fish eaters Planktivore Zooplankton eaters Benthivore Bottom eaters Benthos Things that live on the bottom Other Things not classified above 2.1.1 Data sources In order to match aggregate groups with various data sources, a look-up table was generated which includes species’ common names (COMNAME) along with their scientific names (SCINAME) and several species codes. SVSPP codes are used by the NEFSC Ecosystems Surveys Branch in their fishery-independent Survey Database (SVDBS), while NESPP3 codes refer to the codes used by the Commercial Fisheries Database System (CFDBS) for fishery-dependent data. A third species code provided is the ITISSPP, which refers to species identifiers used by the Integrated Taxonomic Information System (ITIS). Digits within ITIS codes are hierarchical, with different positions in the identifier referring to higher or lower taxonomic levels. More information about the SVDBS, CFDBS, and ITIS species codes are available in the links provided below. Management responsibilities for different species are listed under the column “Fed.managed” (NEFMC, MAFMC, or JOINT for jointly managed species). More information about these species is available on the FMC websites listed below. Species groupings listed in the “NEIEA” column were developed for presentation on the Northeast Integrated Ecosystem Assessment (NE-IEA) website. These groupings are based on EMAX groupings (Link et al. 2006), but were adjusted based on conceptual models developed for the NE-IEA program that highlight focal components in the NE-LME (i.e. those components with the largest potential for perturbing ecosystem dynamics). NE-IEA groupings were further simplified to allow for effective communication through the NE-IEA website. 2.1.1.1 Supplemental information See the following links for more information regarding the NEFSC ESB Bottom Trawl Survey, CFDBS, and ITIS: https://www.itis.gov/ https://inport.nmfs.noaa.gov/inport/item/22561 https://inport.nmfs.noaa.gov/inport/item/22560 https://inport.nmfs.noaa.gov/inport/item/27401 More information about the NE-IEA program is available here. More information about the New Engalnd Fisheries Management Council is available here. More information about the Mid-Atlantic Fisheries Management Council is available here. 2.1.2 Data extraction Species lists are pulled from SVDBS and CFDBS. They are merged using the ITIS code. Classifications from Garrison and Link (Garrison and Link 2000) and Link et al. (Link et al. 2006) are added manually. The R code used in the extraction process is presented below. #Species list if(Sys.info()[&#39;sysname&#39;]==&quot;Windows&quot;){ data.dir &lt;- &quot;L:\\\\EcoAP\\\\Data\\\\survey&quot; out.dir &lt;- &quot;L:\\\\EcoAP\\\\Data\\\\survey&quot; memory.limit(4000) } if(Sys.info()[&#39;sysname&#39;]==&quot;Linux&quot;){ data.dir &lt;- &quot;/home/slucey/slucey/EcoAP/Data/survey&quot; out.dir &lt;- &quot;/home/slucey/slucey/EcoAP/Data/survey&quot; out.dir.2 &lt;- &quot;/home/slucey/slucey/EcoAP/Data/Commercial&quot; uid &lt;- &#39;slucey&#39; cat(&quot;Oracle Password: &quot;) pwd &lt;- scan(stdin(), character(), n = 1) } library(RODBC); library(data.table) if(Sys.info()[&#39;sysname&#39;]==&quot;Windows&quot;){ channel &lt;- odbcDriverConnect() } else { channel &lt;- odbcConnect(&#39;sole&#39;, uid, pwd) } #Grab svspp code by itis code svspp &lt;- as.data.table(sqlQuery(channel, &quot;select SVSPP, ITISSPP, COMNAME, SCINAME from ITIS_Lookup&quot;)) #Grab cfspp by itis code cfspp &lt;- as.data.table(sqlQuery(channel, &quot;select NESPP4, SPECIES_ITIS, COMMON_NAME, SCIENTIFIC_NAME from CFDBS.Species_itis_ne&quot;)) setnames(cfspp, &#39;SPECIES_ITIS&#39;, &#39;ITISSPP&#39;) cfspp[, NESPP3 := as.numeric(substr(sprintf(&#39;%04d&#39;, NESPP4), 1, 3))] setkey(cfspp, NESPP3) cfspp &lt;- unique(cfspp) cfspp[, NESPP4 := NULL] setkey(cfspp, ITISSPP, NESPP3) cfspp &lt;- unique(cfspp, by = key(cfspp)) #Merge to master species list spp &lt;- merge(svspp, cfspp, by = &#39;ITISSPP&#39;, all = T) spp &lt;- spp[!(is.na(SVSPP) &amp; is.na(NESPP3)), ] #Fix known issues spp &lt;- spp[!SVSPP %in% c(193, 310), ] spp[ITISSPP == 630979, SVSPP := 193] #Ocean Pout spp[ITISSPP == 620992, SVSPP := 310] #Deepsea red crab spp[ITISSPP == 172783, SVSPP := 104] #Fourspot flounder spp[ITISSPP == 166283, SVSPP := 112] #John Dory spp[ITISSPP == 161731, SVSPP := 36] #Menhaden spp[ITISSPP == 98671, SVSPP := 311] #Cancer Crabs unk spp[ITISSPP == 98455, SVSPP := 317] #Spider crabs spp[ITISSPP == 159772, NESPP3 := 150] #Hagfish spp[ITISSPP == 166284, NESPP3 := 188] #John Dory spp[ITISSPP == 98670, NESPP3 := 714] #Cancer Crabs unk spp[ITISSPP %in% c(630979, 620992), COMNAME := COMMON_NAME] spp[ITISSPP %in% c(630979, 620992), SCINAME := SCIENTIFIC_NAME] #Drop extra columns spp[is.na(COMNAME), COMNAME := COMMON_NAME] spp[is.na(SCINAME), SCINAME := SCIENTIFIC_NAME] spp[, c(&#39;COMMON_NAME&#39;, &#39;SCIENTIFIC_NAME&#39;) := NULL] setcolorder(spp, c(&#39;ITISSPP&#39;, &#39;SVSPP&#39;, &#39;NESPP3&#39;, &#39;COMNAME&#39;, &#39;SCINAME&#39;)) setkey(spp, SVSPP, NESPP3) #------------------------------------------------------------------------------- #Add management authority mafmc &lt;- c(103, 121, 131, 135, 141, 143, 151, 403, 409, 502, 503, 621) nefmc &lt;- c(22:28, 32, 69, 72:77, 101, 102, 105:107, 155, 193, 310, 401, 894) joint &lt;- c(15, 197) spp[SVSPP %in% mafmc, MANAGE := &#39;MAFMC&#39;] spp[SVSPP %in% nefmc, MANAGE := &#39;NEFMC&#39;] spp[SVSPP %in% joint, MANAGE := &#39;JOINT&#39;] #------------------------------------------------------------------------------- #Add functional groups #From NEFMC EBFM PDT work #See Functional_group_table_Mike.csv in slucey/EcoAP/EBFM_PDT spp[, EBFM.PDT := factor(NA, levels = c(&#39;Apex Predator&#39;, &#39;Benthivore&#39;, &#39;Benthos&#39;, &#39;Macroplanktivore&#39;, &#39;Macrozoo-piscivore&#39;, &#39;Mesoplanktivore&#39;, &#39;Piscivore&#39;, &#39;Other&#39;))] spp[SVSPP %in% c(11, 700, 704, 747), EBFM.PDT := &#39;Apex Predator&#39;] spp[SVSPP %in% c(14, 22, 25, 74, 102, 105, 106, 107, 141, 143, 151, 164, 172, 176, 177, 192, 193, 301, 310, 312, 314, 317, 322, 501), EBFM.PDT := &#39;Benthivore&#39;] spp[SVSPP %in% c(331, 336, 401, 403, 409), EBFM.PDT := &#39;Benthos&#39;] spp[NESPP3 %in% c(775, 781, 805, 806), EBFM.PDT := &#39;Benthos&#39;] spp[SVSPP %in% c(76, 163, 168, 171, 502, 503), EBFM.PDT := &#39;Macroplanktivore&#39;] spp[SVSPP %in% c(13, 24, 26, 27, 75, 77, 84, 108, 112, 155, 156, 311), EBFM.PDT := &#39;Macrozoo-piscivore&#39;] spp[SVSPP %in% c(32, 33, 34, 35, 36, 121, 131), EBFM.PDT := &#39;Mesoplanktivore&#39;] spp[SVSPP %in% c(15, 23, 28, 69, 72, 73, 101, 103, 104, 135, 139, 145, 197), EBFM.PDT := &#39;Piscivore&#39;] spp[is.na(EBFM.PDT), EBFM.PDT := &#39;Other&#39;] #Fix known issues spp[SVSPP == 160, EBFM.PDT := &#39;Macroplanktivore&#39;] #Add EMAX load(file.path(data.dir, &#39;EMAX_groups.RData&#39;)) emax &lt;- emax[!is.na(SVSPP), ] spp &lt;- merge(spp, emax[, list(SVSPP, EMAX, Fall.q, Spring.q)], by = &#39;SVSPP&#39;, all.x = T) #reduce rows setkey(spp, SVSPP, NESPP3) species &lt;- unique(spp, by = key(spp)) #Remove EMAX q&#39;s species[, c(&#39;Fall.q&#39;, &#39;Spring.q&#39;) := NULL] #Expand EMAX abbreviations species[EMAX == &#39;BG&#39;, EMAX := &#39;Benthivore Gadiformes&#39;] species[EMAX == &#39;BE&#39;, EMAX := &#39;Benthivore Elasmobranchs&#39;] species[EMAX == &#39;BP&#39;, EMAX := &#39;Benthivore Pleuronectiformes&#39;] species[EMAX == &#39;BS&#39;, EMAX := &#39;Benthivore Scorpaeniformes&#39;] species[EMAX == &#39;BO&#39;, EMAX := &#39;Benthivore Others&#39;] species[EMAX == &#39;BC&#39;, EMAX := &#39;Benthivore Perciformes&#39;] species[EMAX == &#39;PG&#39;, EMAX := &#39;Piscivore Gadiformes&#39;] species[EMAX == &#39;PE&#39;, EMAX := &#39;Piscivore Elasmobranchs&#39;] species[EMAX == &#39;PO&#39;, EMAX := &#39;Piscivore Others&#39;] species[EMAX == &#39;OE&#39;, EMAX := &#39;Omnivore Elasmobranchs&#39;] species[EMAX == &#39;OO&#39;, EMAX := &#39;Omnivore Others&#39;] species[EMAX == &#39;SC&#39;, EMAX := &#39;Southern Perciformes&#39;] species[EMAX == &#39;SP&#39;, EMAX := &#39;Southern Pleuronectiformes&#39;] species[EMAX == &#39;SE&#39;, EMAX := &#39;Southern Elasmobranchs&#39;] species[EMAX == &#39;SG&#39;, EMAX := &#39;Southern Gadiformes&#39;] species[EMAX == &#39;SO&#39;, EMAX := &#39;Southern Others&#39;] #Rename columns setnames(species, c(&#39;MANAGE&#39;, &#39;EBFM.PDT&#39;, &#39;Feeding.guild&#39;), c(&#39;Fed.Managed&#39;, &#39;SOE.17&#39;, &#39;SOE.18&#39;)) #Add classification from Garrison/Link 2000 size &lt;- data.table(COMNAME = c(rep(&#39;SMOOTH DOGFISH&#39;, 2), rep(&#39;SPINY DOGFISH&#39;, 3), rep(&#39;WINTER SKATE&#39;, 4), rep(&#39;LITTLE SKATE&#39;, 2), rep(&#39;SMOOTH SKATE&#39;, 1), rep(&#39;THORNY SKATE&#39;, 4), rep(&#39;ATLANTIC HERRING&#39;, 2), rep(&#39;ALEWIFE&#39;, 1), rep(&#39;SILVER HAKE&#39;, 3), rep(&#39;ATLANTIC COD&#39;, 4), rep(&#39;HADDOCK&#39;, 3), rep(&#39;POLLOCK&#39;, 4), rep(&#39;WHITE HAKE&#39;, 3), rep(&#39;RED HAKE&#39;, 3), rep(&#39;SPOTTED HAKE&#39;, 2), rep(&#39;AMERICAN PLAICE&#39;, 3), rep(&#39;SUMMER FLOUNDER&#39;, 2), rep(&#39;FOURSPOT FLOUNDER&#39;, 2), rep(&#39;YELLOWTAIL FLOUNDER&#39;, 3), rep(&#39;WINTER FLOUNDER&#39;, 3), rep(&#39;WITCH FLOUNDER&#39;, 2), rep(&#39;WINDOWPANE&#39;, 2), rep(&#39;GULF STREAM FLOUNDER&#39;, 1), rep(&#39;ATLANTIC MACKEREL&#39;, 3), rep(&#39;BUTTERFISH&#39;, 1), rep(&#39;BLUEFISH&#39;, 3), rep(&#39;ATLANTIC CROAKER&#39;, 2), rep(&#39;BLACK SEA BASS&#39;, 2), rep(&#39;SCUP&#39;, 1), rep(&#39;WEAKFISH&#39;, 2), rep(&#39;ACADIAN REDFISH&#39;, 1), rep(&#39;LONGHORN SCULPIN&#39;, 2), rep(&#39;SEA RAVEN&#39;, 2), rep(&#39;NORTHERN SAND LANCE&#39;, 1), rep(&#39;OCEAN POUT&#39;, 1), rep(&#39;FAWN CUSK-EEL&#39;, 1), rep(&#39;GOOSEFISH&#39;, 1), rep(&#39;ATLANTIC SHARPNOSE SHARK&#39;, 1), rep(&#39;NORTHERN SHORTFIN SQUID&#39;, 1), rep(&#39;LONGFIN SQUID&#39;, 1)), SizeCat = c(&#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;XL&#39;, &#39;S&#39;, &#39;M&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;XL&#39;, &#39;S&#39;,&#39;M&#39;,&#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;XL&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;XL&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;S&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;S&#39;, &#39;M&#39;, &#39;L&#39;, &#39;S&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;S&#39;, &#39;M&#39;, &#39;M&#39;, &#39;L&#39;, &#39;L&#39;, &#39;L&#39;, &#39;L&#39;, &#39;L&#39;, &#39;L&#39;), Min.size = c(41, 61, 10, 41, 61, 10, 31, 61, 81, 10, 31, 31, 10, 31, 61, 81, 10, 21, 21, 10, 21, 41, 10, 21, 51, 81, 10, 21, 51, 10, 21, 51, 81, 10, 21, 41, 10, 21, 41, 10, 21, 10, 21, 41, 21, 41, 10, 21, 10, 21, 41, 10, 21, 41, 21, 41, 10, 21, 10, 10, 21, 36, 10, 10, 31, 71, 10, 26, 10, 26, 26, 10, 26, 26, 10, 26, 10, 26, 11, 61, 61, 61, 61, 31, 31), Garrison.Link = as.character(c(1, 1, 2, 2, 6, 3, 3, 6, 6, 3, 3, 4, 5, 5, 6, 6, 2, 2, 2, 4, 4, 6, 3, 3, 6, 6, 5, 5, 5, 4, 4, 4, 4, 3, 4, 6, 3, 3, 4, 3, 6, 5, 5, 5, 6, 6, 3, 6, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 2, 2, 2, 2, 6, 6, 6, 5, 5, 1, 1, 5, 6, 6, 4, 3, 3, 6, 6, 2, 5, 3, 6, 6, 2, 2)) ) #Give Garrison.link guilds meaniful names size[Garrison.Link == 1, Garrison.Link := &#39;Crab Eaters&#39;] size[Garrison.Link == 2, Garrison.Link := &#39;Planktivores&#39;] size[Garrison.Link == 3, Garrison.Link := &#39;Amphipod/Shrimp Eaters&#39;] size[Garrison.Link == 4, Garrison.Link := &#39;Shrimp/Small Fish Eaters&#39;] size[Garrison.Link == 5, Garrison.Link := &#39;Benthivores&#39;] size[Garrison.Link == 6, Garrison.Link := &#39;Piscivores&#39;] #Merge Garrison Link into species list species &lt;- merge(species, size, by = &#39;COMNAME&#39;, all = T) #Set column order setcolorder(species, c(&#39;ITISSPP&#39;, &#39;SVSPP&#39;, &#39;NESPP3&#39;, &#39;COMNAME&#39;, &#39;SCINAME&#39;, &#39;SizeCat&#39;, &#39;Min.size&#39;, &#39;Fed.Managed&#39;, &#39;Garrison.Link&#39;, &#39;SOE.17&#39;, &#39;SOE.18&#39;, &#39;EMAX&#39;)) #Add NE.IEA.web category #get rerddap devtools::install_github(&quot;ropensci/rerddap&quot;) library(rerddap) #Set URL for COMET (server where NEFSC ERDDAP lives) comet &lt;- &#39;https://comet.nefsc.noaa.gov/erddap/&#39; #List datasets on the NEFSC ERDDAP tab_list &lt;- ed_datasets(url = comet) #download a tabular dataset (input is dataset id; this works on vectors) ed_spp &lt;- as.data.table(sprintf(&quot;https://comet.nefsc.noaa.gov/erddap/tabledap/%s.csv&quot;, &quot;species_groups_2018&quot;) %&gt;% purrr::map(function(x) { readr::read_csv(url(x)) })) neiea &lt;- unique(ed_spp[!is.na(NEIEA), list(COMNAME, NEIEA)]) species &lt;- merge(species, neiea, by = &#39;COMNAME&#39;, all = T) save(species, file = file.path(out.dir, &#39;SOE_species_list.RData&#39;)) References "],
["annual-sst-cycles.html", "3 Annual SST Cycles 3.1 Methods", " 3 Annual SST Cycles Description: Annual SST Cycles Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Database pull with analysis Contributor(s): Sean Hardison, Vincent Saba Data steward: Sean Hardison, sean.hardison@noaa.gov Point of contact: Sean Hardison, sean.hardison@noaa.gov Public availability statement: Source data are available here. 3.1 Methods 3.1.1 Data sources Data for annual sea surface tempature cycles were derived from the NOAA optimum interpolation sea surface temperature high resolution dataset (NOAA OISST V2 dataset) provided by NOAA/OAR/ESRL PSD, Boulder, CO. The data extend from 1981 to present, and provide a 0.25° x 0.25° global grid of SST measurements (Reynolds et al. 2007). Gridded SST data were masked according to the extent of Ecological Production Units in the Northeast Large Marine Ecosystem (NE-LME) (See “EPU_Extended” shapefiles). 3.1.2 Data extraction Daily mean sea surface temperature data for 2017 and for each year during the period of 1981-2012 were downloaded from the NOAA OI SST V2 site to derive the long-term climatological mean for the period. The use of a 30-year climatological reference period is a standard procedure for metereological observing (WMO 2017). These reference periods serve as benchmarks for comparing current or recent observations, and for the development of standard anomaly data sets. The reference period of 1982-2012 was chosen to be consistent with previous versions of the State of the Ecosystem report. R code used in extraction and processing: #libraries library(ncdf4);library(dplyr) library(readr);library(tidyr) library(sp);library(rgdal) library(raster);library(stringr) #get spatial polygons for Ecological Production Units (EPUs) that are used to clip SST data. EPU &lt;- readOGR(&#39;Extended_EPU&#39;) map.crs &lt;- CRS(&quot;+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0 +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot;) #find long term daily mean SSTs and 2017 SST anomaly MAB_sst_daily_mean &lt;- NULL GB_sst_daily_mean &lt;- NULL GOM_sst_daily_mean &lt;- NULL #I split the data into three directories to loop through in separate R sessions concurrently. for (dir. in 1:3){ #Loop through directories setwd(paste0(&#39;c:/users/sean.hardison/documents/sst_data/&#39;,dir.)) print(getwd()) for (f in 1:length(list.files())){ if (!str_detect(list.files()[f],&quot;.nc&quot;)){ print(paste(list.files()[f],&quot;is not a raster&quot;)) #Based on file type next } for (j in c(&quot;MAB&quot;,&quot;GB&quot;,&quot;GOM&quot;)){ sub_region &lt;- EPU[EPU@data$EPU == j,] y &lt;- as.numeric(str_extract(list.files()[f],&quot;[0-9]+&quot;)) #get year for (i in 1:365){ print(paste(j,y,i)) daily_mean &lt;- raster(paste0(list.files()[f]), band = i) #get band #set crs daily_mean@crs &lt;- sub_region@proj4string #rotate to lon scale from 0-360 to -180-180 daily_mean &lt;- rotate(daily_mean) #mask raster with spatialpolygon daily_mean_clipped &lt;- mask(daily_mean, sub_region) #add mean value to data.frame assign(paste0(j,&quot;_sst_daily_mean&quot;),rbind(get(paste0(j,&quot;_sst_daily_mean&quot;)), c(mean(daily_mean_clipped@data@values, na.rm = T),y,i))) } } } } #Put results into data.frames mab &lt;- data.frame(EPU = &quot;MAB&quot;, year = MAB_sst_daily_mean[,2], day = MAB_sst_daily_mean[,3], Value = MAB_sst_daily_mean[,1]) gb &lt;- data.frame(EPU = &quot;GB&quot;, year = GB_sst_daily_mean[,2], day = GB_sst_daily_mean[,3], Value = GB_sst_daily_mean[,1]) gom &lt;- data.frame(EPU = &quot;GOM&quot;, year = GOM_sst_daily_mean[,2], day = GOM_sst_daily_mean[,3], Value = GOM_sst_daily_mean[,1]) data3 &lt;- rbind(mab, gb, gom) #Save as 1 of 3 files (one for each directory containing daily mean data) save(data3, file = &quot;dir3_sst.Rdata&quot;) #--------------------------2017 SSTs----------------------------# MAB_2017 &lt;- NULL GB_2017 &lt;- NULL GOM_2017 &lt;- NULL for (j in c(&quot;MAB&quot;,&quot;GB&quot;,&quot;GOM&quot;)){ sub_region &lt;- EPU[EPU@data$EPU == j,] for (i in 1:365){ print(paste(j,i)) daily_mean &lt;- raster(&quot;sst.day.mean.2017.nc&quot;, band = i) #get band #set crs daily_mean@crs &lt;- sub_region@proj4string #rotate to lon scale from 0-360 to -180-180 daily_mean &lt;- rotate(daily_mean) #mask raster with spatialpolygon daily_mean_clipped &lt;- mask(daily_mean, sub_region) #add mean value to data.frame assign(paste0(j,&quot;_2017&quot;),rbind(get(paste0(j,&quot;_2017&quot;)), c(mean(daily_mean_clipped@data@values, na.rm = T),&quot;2017&quot;,i))) } } #Put results into data.frames mab_2017 &lt;- data.frame(EPU = &quot;MAB&quot;, year = MAB_2017[,2], day = MAB_2017[,3], Value = MAB_2017[,1]) gb_2017 &lt;- data.frame(EPU = &quot;GB&quot;, year = GB_2017[,2], day = GB_2017[,3], Value = GB_2017[,1]) gom_2017 &lt;- data.frame(EPU = &quot;GOM&quot;, year = GOM_2017[,2], day = GOM_2017[,3], Value = GOM_2017[,1]) #Final 2017 daily mean data sst_2017 &lt;- rbind(mab_2017, gb_2017, gom_2017) #save(sst_2017, file = &quot;sst_2017.Rdata&quot;) 3.1.3 Data analysis We calculated the long-term mean and standard deviation of SST over the period of 1982-2012 for each EPU, as well as the daily mean for 2017. #----------------------Load results--------------------------# load(&quot;dir1_sst.Rdata&quot;) load(&quot;dir2_sst.Rdata&quot;) load(&quot;dir3_sst.Rdata&quot;) load(&quot;sst_2017.Rdata&quot;) #Get long term mean and standard deviation d &lt;- rbind(data1, data2, data3) ltm &lt;- d %&gt;% group_by(EPU, day) %&gt;% dplyr::summarise(mean = mean(Value), sd = sd(Value)) 3.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) ##---------------------------------MAB-----------------------------------------# par(mfrow = c(1,3)) doy &lt;- as.numeric(SOE.data[SOE.data$Var == &quot;sst mean 2017 MAB&quot;,]$Time) val_2017 &lt;- SOE.data[SOE.data$Var == &quot;sst mean 2017 MAB&quot;,]$Value val_LT &lt;- SOE.data[SOE.data$Var == &quot;sst mean long term MAB&quot;,]$Value val_LT_sd &lt;- SOE.data[SOE.data$Var == &quot;sst sd long term MAB&quot;,]$Value # val_2017 &lt;- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y # val_LT &lt;- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y # val_LT_sd &lt;- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y doy &lt;- seq(doy[1],doy[length(doy)],length.out = 365*1) above_mean &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &gt;= val_LT[i]){ above_mean[i] &lt;- val_2017[i] } else if (val_2017[i] &lt; val_LT [i]){ above_mean[i] &lt;- NA } } below_mean &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &lt;= val_LT[i]){ below_mean[i] &lt;- val_2017[i] } else if (val_2017[i] &gt; val_LT [i]){ below_mean[i] &lt;- NA } } above_sd &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &gt;= val_LT_sd[i] + val_LT[i]){ above_sd[i] &lt;- val_2017[i] } else if (val_2017[i] &lt; val_LT_sd [i] + val_LT[i]){ above_sd[i] &lt;- NA } } below_sd &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &lt;= val_LT[i] - val_LT_sd[i]){ below_sd[i] &lt;- val_2017[i] } else if (val_2017[i] &gt; val_LT[i] - val_LT_sd [i]){ below_sd[i] &lt;- NA } } #Lines for polygons above_sd[is.na(above_sd)] &lt;- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))] below_sd[is.na(above_sd)] &lt;- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] above_mean[is.na(above_mean)] &lt;- val_LT[which(is.na(above_mean))] below_mean[is.na(below_mean)] &lt;- val_LT[which(is.na(below_mean))] upper &lt;- val_LT_sd + val_LT lower &lt;- val_LT - val_LT_sd #Null figure plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,25), las = 1, ylab = &quot;&quot;, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) axis(2, cex.axis = 1.25, las = 1) axis(1, labels = c(&quot;Jan&quot;,&quot;Mar&quot;,&quot;May&quot;,&quot;July&quot;,&quot;Sep&quot;,&quot;Nov&quot;,&quot;Jan&quot;), at = c(1,61,122,183,245,306,365), cex.axis= 1.25) mtext(2, line = 2.3, text = expression(paste(&quot;Mean SST (&quot;,degree,&quot;C)&quot;)), cex = 1.1) mtext(1, line = 2.5, text = &quot;Time&quot;, cex = 1.1) text(15,25*.95,&quot;A&quot;,cex = 1.5) # +/- 1 sd polygon(c(doy, rev(doy)), c(upper, rev(lower)), col = &quot;grey85&quot;, border = NA) #Fills plot polygon(c(doy, rev(doy)), c(below_mean + (val_LT-below_mean), rev(below_mean)), col = &quot;lightblue&quot;, border = NA) polygon(c(doy, rev(doy)), c(above_mean - (above_mean-val_LT), rev(above_mean)), col = &quot;orange&quot;, border = NA) polygon(c(doy, rev(doy)), c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)), col = &quot;red&quot;, border = NA) polygon(c(doy, rev(doy)), c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)), col = &quot;blue&quot;, border = NA) points(doy,val_LT, type = &quot;l&quot;, lwd = 1, &quot;grey90&quot;) ##-------------------------------------GB-------------------------------------# doy &lt;- as.numeric(SOE.data[SOE.data$Var == &quot;sst mean 2017 GB&quot;,]$Time) val_2017 &lt;- SOE.data[SOE.data$Var == &quot;sst mean 2017 GB&quot;,]$Value val_LT &lt;- SOE.data[SOE.data$Var == &quot;sst mean long term GB&quot;,]$Value val_LT_sd &lt;- SOE.data[SOE.data$Var == &quot;sst sd long term GB&quot;,]$Value # val_2017 &lt;- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y # val_LT &lt;- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y # val_LT_sd &lt;- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y doy &lt;- seq(doy[1],doy[length(doy)],length.out = 365*1) above_mean &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &gt;= val_LT[i]){ above_mean[i] &lt;- val_2017[i] } else if (val_2017[i] &lt; val_LT [i]){ above_mean[i] &lt;- NA } } below_mean &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &lt;= val_LT[i]){ below_mean[i] &lt;- val_2017[i] } else if (val_2017[i] &gt; val_LT [i]){ below_mean[i] &lt;- NA } } above_sd &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &gt;= val_LT_sd[i] + val_LT[i]){ above_sd[i] &lt;- val_2017[i] } else if (val_2017[i] &lt; val_LT_sd [i] + val_LT[i]){ above_sd[i] &lt;- NA } } below_sd &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &lt;= val_LT[i] - val_LT_sd[i]){ below_sd[i] &lt;- val_2017[i] } else if (val_2017[i] &gt; val_LT[i] - val_LT_sd [i]){ below_sd[i] &lt;- NA } } #Lines for polygons above_sd[is.na(above_sd)] &lt;- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))] below_sd[is.na(above_sd)] &lt;- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] above_mean[is.na(above_mean)] &lt;- val_LT[which(is.na(above_mean))] below_mean[is.na(below_mean)] &lt;- val_LT[which(is.na(below_mean))] upper &lt;- val_LT_sd + val_LT lower &lt;- val_LT - val_LT_sd #Null figure plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,21), las = 1, ylab = &quot;&quot;, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) axis(2, cex.axis = 1.25, las = 1) axis(1, labels = c(&quot;Jan&quot;,&quot;Mar&quot;,&quot;May&quot;,&quot;July&quot;,&quot;Sep&quot;,&quot;Nov&quot;,&quot;Jan&quot;), at = c(1,61,122,183,245,306,365), cex.axis= 1.25) #mtext(2, line = 2.5, text = expression(paste(&quot;Mean SST (&quot;,degree,&quot;C)&quot;)), cex = 1.1) mtext(1, line = 2.5, text = &quot;Time&quot;, cex = 1.1) text(15,21*.95,&quot;B&quot;,cex = 1.5) # +/- 1 sd polygon(c(doy, rev(doy)), c(upper, rev(lower)), col = &quot;grey85&quot;, border = NA) #Fills plot polygon(c(doy, rev(doy)), c(below_mean + (val_LT-below_mean), rev(below_mean)), col = &quot;lightblue&quot;, border = NA) polygon(c(doy, rev(doy)), c(above_mean - (above_mean-val_LT), rev(above_mean)), col = &quot;orange&quot;, border = NA) polygon(c(doy, rev(doy)), c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)), col = &quot;red&quot;, border = NA) polygon(c(doy, rev(doy)), c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)), col = &quot;blue&quot;, border = NA) points(doy,val_LT, type = &quot;l&quot;, lwd = 1, &quot;grey90&quot;) #----------------------------------------------------------------------------# ## SST GOM doy &lt;- as.numeric(SOE.data[SOE.data$Var == &quot;sst mean 2017 GOM&quot;,]$Time) val_2017 &lt;- SOE.data[SOE.data$Var == &quot;sst mean 2017 GOM&quot;,]$Value val_LT &lt;- SOE.data[SOE.data$Var == &quot;sst mean long term GOM&quot;,]$Value val_LT_sd &lt;- SOE.data[SOE.data$Var == &quot;sst sd long term GOM&quot;,]$Value # val_2017 &lt;- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y # val_LT &lt;- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y # val_LT_sd &lt;- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y doy &lt;- seq(doy[1],doy[length(doy)],length.out = 365*1) above_mean &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &gt;= val_LT[i]){ above_mean[i] &lt;- val_2017[i] } else if (val_2017[i] &lt; val_LT [i]){ above_mean[i] &lt;- NA } } below_mean &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &lt;= val_LT[i]){ below_mean[i] &lt;- val_2017[i] } else if (val_2017[i] &gt; val_LT [i]){ below_mean[i] &lt;- NA } } above_sd &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &gt;= val_LT_sd[i] + val_LT[i]){ above_sd[i] &lt;- val_2017[i] } else if (val_2017[i] &lt; val_LT_sd [i] + val_LT[i]){ above_sd[i] &lt;- NA } } below_sd &lt;- NULL for (i in 1:length(val_2017)){ if (val_2017[i] &lt;= val_LT[i] - val_LT_sd[i]){ below_sd[i] &lt;- val_2017[i] } else if (val_2017[i] &gt; val_LT[i] - val_LT_sd [i]){ below_sd[i] &lt;- NA } } #Lines for polygons above_sd[is.na(above_sd)] &lt;- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))] below_sd[is.na(above_sd)] &lt;- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] above_mean[is.na(above_mean)] &lt;- val_LT[which(is.na(above_mean))] below_mean[is.na(below_mean)] &lt;- val_LT[which(is.na(below_mean))] upper &lt;- val_LT_sd + val_LT lower &lt;- val_LT - val_LT_sd #Null figure plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,21), las = 1, ylab = &quot;&quot;, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) axis(2, cex.axis = 1.25, las = 1) axis(1, labels = c(&quot;Jan&quot;,&quot;Mar&quot;,&quot;May&quot;,&quot;July&quot;,&quot;Sep&quot;,&quot;Nov&quot;,&quot;Jan&quot;), at = c(1,61,122,183,245,306,365), cex.axis= 1.25) #(2, line = 2.5, text = expression(paste(&quot;Mean SST (&quot;,degree,&quot;C)&quot;)), cex = 1.1) mtext(1, line = 2.5, text = &quot;Time&quot;, cex = 1.1) text(15,21*.95,&quot;C&quot;,cex = 1.5) # +/- 1 sd polygon(c(doy, rev(doy)), c(upper, rev(lower)), col = &quot;grey85&quot;, border = NA) #Fills plot polygon(c(doy, rev(doy)), c(below_mean + (val_LT-below_mean), rev(below_mean)), col = &quot;lightblue&quot;, border = NA) polygon(c(doy, rev(doy)), c(above_mean - (above_mean-val_LT), rev(above_mean)), col = &quot;orange&quot;, border = NA) polygon(c(doy, rev(doy)), c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)), col = &quot;red&quot;, border = NA) polygon(c(doy, rev(doy)), c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)), col = &quot;blue&quot;, border = NA) points(doy,val_LT, type = &quot;l&quot;, lwd = 1, &quot;grey90&quot;) box() Figure 3.1: Long-term mean SSTs for the Mid-Atlantic Bight (A), Georges Bank (B), and Gulf of Maine (C). Orange and cyan shading show where the 2017 daily SST values were above or below the long-term mean respectively; red and dark blue shades indicate days when the 2017 mean exceeded +/- 1 standard deviation from the long-term mean. References "],
["aquaculture.html", "4 Aquaculture 4.1 Methods", " 4 Aquaculture Description: Aquaculture indicators Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Synthesis of published information Contributor(s): Sean Hardison, Lisa Calvo, Karl Roscher Data steward: Sean Hardison sean.hardison@noaa.gov Point of contact: Sean Hardison sean.hardison@noaa.gov Public availability statement: Source data are publicly available in referenced reports, and are also available for download here. 4.1 Methods Aquaculture data included in the SOE report were time series of number of oysters sold in Virginia, Maryland, and New Jersey. 4.1.1 Data sources Virginia oyster harvest data are collected from mail and internet-based surveys of active oyster aquaculture operations on both sides of the Chesapeake Bay, which are then synthesized in an annual report (Hudson 2017). In Maryland, shellfish aquaculturists are required to report their monthly harvests to the Maryland Department of Natural Resources (MD-DNR). The MD-DNR then aggregates the harvest data for release in the Maryland Aquaculture Coordinating Council Annual Report (ACC 2017), from which data were collected. Similar to Virginia, New Jersey releases annual reports synthesizing electronic survey results from lease-holding shellfish growers. Data from New Jersey reflects cage reared oysters grown from hatchery seed (Calvo 2017). 4.1.2 Data extraction Data were collected directly from state aquaculture reports. Oyster harvest data in MD was reported in bushels which were then converted to individual oysters by an estimate of 300 oysters bushel\\(^{-1}\\). View processing code for this indicator here. 4.1.3 Data analysis No data analyses occurred for this indicator. 4.1.4 Data processing Aquaculture data were formatted for inclusion in the ecodata R package using the following R code. #Processing code for oyster harvest data in the Mid-Atlantic #These data were collected directly from shellfish aquaculture surveys performed by state agencies and #universities. #NJ: http://njseagrant.org/new-jersey-shellfish-aquaculture-situation-outlook-report-new/ #VA: http://www.vims.edu/research/units/centerspartners/map/aquaculture/docs_aqua/vims_mrr_2018-9.pdf #MD: Data from MD Aquaculture Coordinating Council meeting reports. Newest reports available upon request library(dplyr) library(tidyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_aquaculture &lt;- function(save_clean = F){ aquaculture &lt;- read.csv(file.path(raw.dir,&quot;mab_oyster_harvest.csv&quot;)) if (save_clean){ usethis::use_data(aquaculture, overwrite = T) } else { return(aquaculture) } } 4.1.5 Plotting aqua &lt;- ecodata::aquaculture %&gt;% group_by(Var) %&gt;% mutate(hline = mean(Value)) %&gt;% ungroup() %&gt;% mutate(Var = plyr::mapvalues(Var, from = c(&quot;md oyster harvest&quot;,&quot;nj oyster harvest&quot;,&quot;va oyster harvest&quot;), to = c(&quot;MD&quot;,&quot;NJ&quot;,&quot;VA&quot;))) %&gt;% dplyr::rename(State = Var) aqua$State &lt;- factor(aqua$State, levels = c(&quot;VA&quot;,&quot;MD&quot;,&quot;NJ&quot;)) ggplot() + geom_segment(aes(x=2005,xend=2017,y=mean(aquaculture[aquaculture$Var == &quot;va oyster harvest&quot;,]$Value), yend=mean(aquaculture[aquaculture$Var == &quot;va oyster harvest&quot;,]$Value)), size = hline.size, alpha = hline.alpha, linetype = hline.lty, color = &quot;#1b9e77&quot;, inherit.aes = F) + geom_segment(aes(x=2012,xend=2016,y=mean(aquaculture[aquaculture$Var == &quot;nj oyster harvest&quot;,]$Value), yend=mean(aquaculture[aquaculture$Var == &quot;nj oyster harvest&quot;,]$Value)), size = hline.size, alpha = hline.alpha, linetype = hline.lty, color = &quot;#d95f02&quot;, inherit.aes = F) + geom_segment(aes(x=2012,xend=2017,y=mean(aquaculture[aquaculture$Var == &quot;md oyster harvest&quot;,]$Value), yend=mean(aquaculture[aquaculture$Var == &quot;md oyster harvest&quot;,]$Value)), size = hline.size, alpha = hline.alpha, linetype = hline.lty, color = &quot;#7570b3&quot;, inherit.aes = F) + #Highlight last ten years geom_line(data = aqua, aes(x = Time, y = Value, color = State), size = lwd) + geom_point(data = aqua,aes(x = Time, y = Value, color = State), size = pcex) + scale_color_manual(values = c(VA = &quot;#1b9e77&quot;, MD = &quot;#7570b3&quot;,NJ = &quot;#d95f02&quot;)) + scale_x_continuous(breaks = seq(2005,2018,3),expand = c(0.01, 0.01)) + scale_y_continuous(labels = function(l){trans = l / 1000000})+ ggtitle(&quot;Oyster harvest&quot;)+ ylab(expression(&quot;Oysters sold (10&quot;^6*&quot; n)&quot;)) + xlab(&quot;&quot;)+ theme_ts() Figure 4.1: Oyster aquaculture production in terms of number of oysters sold from Virginia, Maryland, and New Jersey. References "],
["bennet-indicator.html", "5 Bennet Indicator 5.1 Methods", " 5 Bennet Indicator Description: Bennet Indicator Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Database pull with analysis Contributor(s): John Walden Data steward: Sean Hardison, sean.hardison@noaa.gov Point of contact: John Walden, john.walden@noaa.gov Public availability statement: Derived CFDBS data are available for this analysis (see Comland). 5.1 Methods 5.1.1 Data sources Data used in the Bennet Indicator were derived from the Comland data set; a processed subset of the Commercial Fisheries Database System (CFDBS). The derived Comland data set is available for download here. 5.1.2 Data extraction For information regarding processing of CFDBS, please see Comland methods. The Comland dataset containing seafood landings data was subsetted to US landings after 1964 where revenue was \\(\\geq\\) 0 for each EPU (i.e. Mid-Atlantic Bight, Georges Bank, and Gulf of Maine). Each EPU was run in an individual R script, and the code specific to Georges Bank is shown below. #This code is used to load and process comland data. See comland methods for source data (CFDBS) processing methods. drake::readd(proc_get_mass_inshore_survey.R) #Packages PKG &lt;- c(&quot;data.table&quot;,&quot;plyr&quot;,&quot;RColorBrewer&quot;, &quot;ggplot2&quot;,&quot;cowplot&quot;,&quot;gridExtra&quot;,&quot;grid&quot;) for (p in PKG) { if(!require(p,character.only = TRUE)) { install.packages(p) require(p,character.only = TRUE)} } # #Setting Save path #load &quot;comland&quot; data - These data are unavailble due to PII concerns. See aggregated data load below ecosys2&lt;-subset(comland, US==&#39;TRUE&#39; &amp; YEAR&gt;=1964 &amp; SPPVALUE &gt;=0) #Load species and PDT codes load(file.path(data.dir, &quot;Species_codes.RData&quot;)) #Set EPU epu &lt;- &quot;GB&quot; #processing spp&lt;-subset(spp, NESPP3&gt;0) spp2&lt;-unique(spp[,c(3,12)], by=&#39;NESPP3&#39;) spp2&lt;-spp2[which(!duplicated(spp2$NESPP3)),] sp_combine&lt;-merge(ecosys2, spp2, by=&quot;NESPP3&quot;, all.x=TRUE) add.apex &lt;- data.table(NESPP3 = 000, YEAR = 1971, QY = 1, GEAR = &#39;other&#39;, SIZE = &#39;small&#39;, EPU = epu, UTILCD = 0, SPPLIVMT = 0, SPPVALUE = 0, US = TRUE, Feeding.guild = &#39;Apex Predator&#39;) sp_combine &lt;- rbindlist(list(sp_combine, add.apex)) #Subset data into Georges Bank group LANDINGS&lt;-subset(sp_combine) LANDINGS&lt;-LANDINGS[which(!is.na(LANDINGS$Feeding.guild)),] #Set Up data Table landsum&lt;-data.table(LANDINGS) # setkey(landsum, &quot;EPU&quot;, &quot;YEAR&quot;,&quot;Feeding.guild&quot;) setkey(landsum,&quot;EPU&quot;,&quot;YEAR&quot;,&quot;Feeding.guild&quot;) #Sum by feeding guild landsum[,lapply(.SD, sum, na.rm=TRUE), by=key(landsum), .SDcols=c(&quot;SPPLIVMT&quot;,&quot;SPPVALUE&quot;)] 5.1.3 Data analysis Revenue earned by harvesting resources from an LME at time t is a function of both the quantity landed of each species and the prices paid for landings. Changes in revenue between any two years depends on both prices and quantities in each year, and both may be changing simultaneously. For example, an increase in the harvest of higher priced species, such as scallops can lead to an overall increase in total revenue from an LME between time periods even if quantities landed of other species decline. Although measurement of revenue change is useful, the ability to see what drives revenue change, whether it is changing harvest levels, the mix of species landed, or price changes provides additional valuable information. Therefore, it is useful to decompose revenue change into two parts, one which is due to changing quantities (or volumes), and a second which is due to changing prices. In an LME, the quantity component will yield useful information about how the species mix of harvests are changing through time. A Bennet indicator (BI) is used to examine revenue change between 1964 and 2015 for two major LME regions. It is composed of a volume indicator (VI), which measures changes in quantities, and a price indicator (PI) which measures changes in prices. The Bennet (1920) indicator (BI) was first used to show how a change in social welfare could be decomposed into a sum of a price and quantity change indicator (Cross and Färe 2009). It is called an indicator because it is based on differences in value between time periods, rather than ratios, which are referred to as indices. The BI is the indicator equivalent of the more popular Fisher index (Balk 2010), and has been used to examine revenue changes in Swedish pharmacies, productivity change in U.S. railroads (Lim and Lovell 2009), and dividend changes in banking operations (Grifell-Tatjé and Lovell 2004). An attractive feature of the BI is that the overall indicator is equal to the sum of its subcomponents (Balk 2010). This allows one to examine what component of overall revenue is responsible for change between time periods. This allows us to examine whether changing quantities or prices of separate species groups are driving revenue change in each EPU between 1964 and 2015. Revenue in a given year for any species group is the product of quantity landed times price, and the sum of revenue from all groups is total revenue from the LME. In any year, both prices and quantities can change from prior years, leading to total revenue change. At time t, revenue (R) is defined as \\[R^{t} = \\sum_{j=1}^{J}p_{j}^{t}y_{j}^{t},\\] where \\(p_{j}\\) is the price for species group \\(j\\), and \\(y_{j}\\) is the quantity landed of species group \\(j\\). Revenue change between any two time periods, say \\(t+1\\) and \\(t\\), is then \\(R^{t+1}-R^{t}\\), which can also be expressed as: \\[\\Delta R = \\sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t+1}-\\sum_{j=1}^{J}p_{j}^{t}y_{j}^{t}.\\] This change can be decomposed further, yielding a VI and PI. The VI is calculated using the following formula (Moosberg et al. 2007): \\[VI = \\frac{1}{2}(\\sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t+1} - \\sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t} + \\sum_{j=1}^{J}p_{j}^{t}y_{j}^{t+1} - \\sum_{j=1}^{J}p_{j}^{t}y_{j}^{t})\\] The price indicator (PI) is calculated as follows: \\[PI = \\frac{1}{2}(\\sum_{j=1}^{J}y_{j}^{t+1}p_{j}^{t+1} - \\sum_{j=1}^{J}y_{j}^{t+1}p_{j}^{t} + \\sum_{j=1}^{J}y_{j}^{t}p_{j}^{t+1} - \\sum_{j=1}^{J}y_{j}^{t}p_{j}^{t})\\] Total revenue change between time \\(t\\) and \\(t+1\\) is the sum of the VI and PI. Since revenue change is being driven by changes in the individual prices and quantities landed of each species group, changes at the species group level can be examined separately by taking advantage of the additive property of the indicator. For example, if there are five different species groups, the sum of the VI for each group will equal the overall VI, and the sum of the PI for each group will equal the overall PI. #R code to construct Bennet Indicator for Ecosystem Project #Author: John Walden #Date: October 4, 2017 # #Revised January 18, 2018 to calculate the indicator relative to average conditions #during each time period. Set EPU in extraction/processing code chunk above. #filter by specific EPU epu = &quot;GB&quot; value &lt;- subset(landsum, EPU == epu) #Calculate price value$PRICE=value$SPPVALUE/value$SPPLIVMT value[is.na(value)]&lt;-0 #Next two lines are to calculate mean values for landings #and value for the time series by feeding guild meanval&lt;-as.data.frame(value[,j=list(mean(SPPVALUE,na.rm=TRUE), mean(SPPLIVMT,na.rm=TRUE)), by=Feeding.guild]) meanval&lt;-rename(meanval, c(&quot;V1&quot;=&quot;BASEV&quot;, &quot;V2&quot;=&quot;BASEQ&quot;)) meanval$BASEP=meanval$BASEV/meanval$BASEQ; #order by feeding guild value&lt;-value[order(value$Feeding.guild),] meanval&lt;-meanval[order(meanval$Feeding.guild),] #Merge Value data frame with Base Year Value Data Frame value&lt;-merge(value, meanval, by=&quot;Feeding.guild&quot;) #Construct price and Volume Indicators #NOTE: ALL values are normalized to $1,000,000 value$VI=((0.5*(value$BASEP+value$PRICE))*(value$SPPLIVMT-value$BASEQ))/1000000 value$PI=((0.5*(value$BASEQ+value$SPPLIVMT))*(value$PRICE-value$BASEP))/1000000 value&lt;-value[order(value$YEAR),] #The next Data table sets up the yearly aggregate Bennet PI and VI biyear&lt;-data.table(value) setkey(biyear, &quot;YEAR&quot;) biyear&lt;-biyear[,lapply(.SD, sum), by=key(biyear), .SDcols=c(&quot;VI&quot;,&quot;PI&quot;,&quot;BASEV&quot;,&quot;SPPVALUE&quot;)] biyear$revchange&lt;-(biyear$VI+biyear$PI) biyear$BI&lt;-(biyear$VI + biyear$PI) #The Next Steps restructure the year data frame so the yearly #Bennet Indicator can be plotted. Negative values are difficult in GGPLOT. #Since the Bennet indicator can have a negative value, separate data frames #need to be created. First, the data needs to be restructured to use the #stacked bar function in ggplot. GGPLOT is used because it can graph differen#t data layers on the same graph. y1&lt;-biyear[,c(1,2)] y1$indicator=&#39;VI&#39; y2&lt;-biyear[,c(1,3)] y2$indicator=&#39;PI&#39; colnames(y1)[2]&lt;-&quot;value&quot; colnames(y2)[2]&lt;-&quot;value&quot; ytotal&lt;-rbind(y1,y2) 5.1.4 Data processing Bennet indicator time series were formatted for inclusion in the ecodata R package using the following R code. # Process Bennet indicator; price and volume indicators library(dplyr) library(tidyr) library(magrittr) raw.dir &lt;- here::here(&#39;inst&#39;,&#39;extdata&#39;) get_bennet &lt;- function(save_clean = F){ # Find relevant files and load them into workspace files = list.files(raw.dir, pattern=&quot;_vi|_pi|_bennet&quot;) for (i in 1:length(files)) assign(files[i], read.csv(file.path(raw.dir,files[i]))) #Process Bennet indicator data aggregated to the level of EPU (all feeding guilds) bennet &lt;- NULL for (i in ls()){ if (stringr::str_detect(i, &quot;_bennet_&quot;)){ epu &lt;- stringr::str_extract(i, &quot;gb|gom|mab&quot;) #Extract EPU #Process into SOE format out &lt;- get(i) %&gt;% mutate(EPU = epu, Units = &quot;million USD ($2015)&quot;) %&gt;% dplyr::select(-X) %&gt;% gather(.,Var, Value,-YEAR,-EPU, -Units) %&gt;% mutate(Var = paste(Var, &quot;EPU aggregate&quot;), EPU = toupper(EPU)) %&gt;% dplyr::rename(Time = YEAR) %&gt;% as.data.frame() assign(&#39;bennet&#39;,rbind(bennet, out)) } } pi.vi &lt;- NULL for (i in ls()){ if (stringr::str_detect(i, &quot;_pi|_vi&quot;)){ epu &lt;- stringr::str_extract(i, &quot;gb|gom|mab&quot;) indicator &lt;- stringr::str_extract(i, &quot;vi|pi&quot;) out &lt;- get(i) %&gt;% mutate(EPU = toupper(epu), class = toupper(indicator), Units = &quot;million USD ($2015)&quot;) %&gt;% gather(.,Var,Value,-YEAR, -class, -EPU,-Units) %&gt;% unite(., &quot;Var&quot;, c(&quot;Var&quot;,&quot;class&quot;), sep = &quot; &quot;) %&gt;% dplyr::rename(Time = YEAR) %&gt;% as.data.frame() assign(&#39;pi.vi&#39;, rbind(pi.vi, out)) } } bennet &lt;- rbind(pi.vi, bennet) if (save_clean){ usethis::use_data(bennet, overwrite = T) } else { return(bennet) } } 5.1.5 Plotting References "],
["bottom-temperatures.html", "6 Bottom temperatures 6.1 Methods", " 6 Bottom temperatures Description: Time series of annual in situ bottom temperatures on the Northeast Continental Shelf. Indicator category: Extensive analysis; not yet published Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2019); State of the Ecosystem - Mid-Atlantic Bight (2019) Contributor(s): Paula Fratantoni, paula.fratantoni@noaa.gov Data steward: Kimberly Bastille, kimberly.bastille@noaa.gov Point of contact: Paula Fratantoni, paula.fratantoni@noaa.gov Public availability statement: Source data are publicly available at ftp://ftp.nefsc.noaa.gov/pub/hydro/matlab_files/yearly and in the World Ocean Database housed at http://www.nodc.noaa.gov/OC5/SELECT/dbsearch/dbsearch.html under institute code number 258. 6.1 Methods 6.1.1 Data sources The bottom temperature index incorporates near-bottom temperature measurements collected on Northeast Fisheries Science Center surveys between 1977-present. Early measurements were made using surface bucket samples, mechanical bathythermographs and expendable bathythermograph probes, but by 1991 the CTD – an acronym for conductivity temperature and depth – became standard equipment on all NEFSC surveys. Near-bottom refers to the deepest observation at each station that falls within 10 m of the reported water depth. Observations encompass the entire continental shelf area extending from Cape Hatteras, NC to Nova Scotia, Canada, inclusive of the Gulf of Maine and Georges Bank. 6.1.2 Data extraction While all processed hydrographic data are archived in an Oracle database (OCDBS), we work from Matlab-formatted files stored locally. 6.1.3 Data analysis Ocean temperature on the Northeast U.S. Shelf varies significantly on seasonal timescales. Any attempt to resolve year-to-year changes requires that this seasonal variability be quantified and removed to avoid bias. This process is complicated by the fact that NEFSC hydrographic surveys conform to a random stratified sampling design meaning that stations are not repeated at fixed locations year after year so that temperature variability cannot be assessed at fixed station locations. Instead, we consider the variation of the average bottom temperature within four ecoregions: Middle Atlantic Bight, Georges Bank, Gulf of Maine and Scotian Shelf. Within each sub-region, ocean temperature observations are extracted from the collection of measurements made within 10 m of the bottom on each survey and an area-weighted average temperature is calculated for each. The result of this calculation is a timeseries of regional average near-bottom temperature having a temporal resolution that matches the survey frequency in the database. Anomalies are subsequently calculated relative to a reference annual cycle, estimated using a multiple linear regression model to fit an annual harmonic (365-day period) to historical regional average temperatures from 1981-2010. The curve fitting technique to formulate the reference annual cycle follows the methodologies outlined by Mountain (1991). The reference period was chosen because it is the standard climatological period adopted by the World Meteorological Organization. The resulting anomaly time series represents the difference between the time series of regional mean temperatures and corresponding reference temperatures predicted by a reference annual cycle for the same time of year. Finally, a reference annual average temperature (calculated as the average across the reference annual cycle) is added back into the anomaly timeseries to convert temperature anomalies back to ocean bottom temperature. 6.1.4 Data processing Source data were formatted for inclusion in the ecodata R package using the following R code. # Process ocean temperature anomaly data # These data include in situ regional time series of both surface and bottom water temperature anomalies # on the Northeast Continental Shelf. Raw data is split into four files by EPU (SS, GOM, GB, and MAB). library(dplyr) library(tidyr) library(lubridate) #Get raw raw.dir &lt;- here::here(&quot;data-raw&quot;) #input raw get_oceantemp_insitu &lt;- function(save_clean = F){ ss &lt;- read.csv(file.path(raw.dir,&quot;EcoSS_core_Ttopbot.csv&quot;)) %&gt;% mutate(EPU = &quot;SS&quot;) gom &lt;- read.csv(file.path(raw.dir,&quot;EcoGoM_core_Ttopbot.csv&quot;)) %&gt;% mutate(EPU = &quot;GOM&quot;) gb &lt;- read.csv(file.path(raw.dir,&quot;EcoGB_core_Ttopbot.csv&quot;)) %&gt;% mutate(EPU = &quot;GB&quot;) mab &lt;- read.csv(file.path(raw.dir,&quot;EcoMAB_core_Ttopbot.csv&quot;)) %&gt;% mutate(EPU = &quot;MAB&quot;) oceantemp_insitu &lt;- rbind(ss, gom, gb, mab) %&gt;% #bind all dplyr::rename(Time = decimal.year, Var = variable.name, Value = temperature) %&gt;% #rename mutate(Units = &quot;degreesC&quot;, Time = as.Date(format(date_decimal(Time), &quot;%Y-%b-%d&quot;), &quot;%Y-%b-%d&quot;), Var, Var = plyr::mapvalues(Var, from = c(&quot;Tsfc_anom&quot;,#Rename variables &quot;Tsfc_ref&quot;, &quot;Tbot_anom&quot;, &quot;Tbot_ref&quot;), to = c(&quot;sst anomaly in situ&quot;, &quot;reference sst in situ (1981-2010)&quot;, &quot;bottom temp anomaly in situ&quot;, &quot;reference bt in situ (1981-2010)&quot;))) %&gt;% group_by(Time = year(Time), EPU, Var, Units) %&gt;% dplyr::summarise(Value = mean(Value)) %&gt;% as.data.frame() if (save_clean){ usethis::use_data(oceantemp_insitu, overwrite = T) } else { return(oceantemp_insitu) } } 6.1.5 Plotting Figure 6.1: GOM and GB annual bottom temperature anomalies. References "],
["catch-and-fleet-diversity.html", "7 Catch and Fleet Diversity 7.1 Methods", " 7 Catch and Fleet Diversity Description: Permit-level species diversity and Council-level fleet diversity. Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Database pull with analysis; Published methods Contributor(s): Geret DePiper, Min-Yang Lee Data steward: Geret DePiper, geret.depiper@noaa.gov Point of contact: Geret DePiper, geret.depiper@noaa.gov Public availability statement: Source data is not publicly availabe due to PII restrictions. Derived time series are available for download here. 7.1 Methods Diversity estimates have been developed to understand whether specialization, or alternatively stovepiping, is occurring in fisheries of the Northeastern Large Marine Ecosystem. We use the average effective Shannon indices for species revenue at the permit level, for all permits landing any amount of NEFMC or MAFMC Fishery Management Plan (FMP) species within a year (including both Monkfish and Spiny Dogfish). We also use the effective Shannon index of fleet revenue diversity and count of active fleets to assess the extent to which the distribution of fishing changes across fleet segments. 7.1.1 Data sources Data for these diversity estimates comes from a variety of sources, including the Commercial Fishery Dealer Database, Vessel Trip Reports, Clam logbooks, vessel characteristics from Permit database, WPU series producer price index. These data are typically not available to the public. 7.1.2 Data extraction The following describes both the permit-level species and fleet diversity data generation. Price data was extracted from the Commercial Fishery Dealer database (CFDERS) and linked to Vessel Trip Reports by a heirarchical matching algorithm that matched date and port of landing at its highest resolution. Code used in these analyses is available upon request. Output data was then matched to vessel characteristics from the VPS VESSEL data set. For the permit-level estimate, species groups are based off of a slightly refined NESPP3 code, defined in the data as “myspp”, which is further developed in the script to rectify inconsistencies in the data. Species groups used include Highly Migratory Species, Monkfish, Atlantic Sea Scallops, Shrimp, Skates, Atlantic Herring, Ocean Quahog, Surf Clam, Tilefish, Black Sea Bass and Fluke, Butterfish and Red Hake and Unknown Whiting, Bluefish, Spiny Dogfish, Illex, American Lobster, Loligo, Menhaden, Offshore hake, Scup, Sand Dabs, Pout, Wolffish, Winter Flounder, Yellowtail Flounder, Unspecified hakes, White hake, Halibut, Bluefish &amp; Scup (NE only), New England Groundfish (cod, pollock, hadddock, Monkfish, Winter flounder, Witch flounder, White hake, Plaice, redfish), Mid-Atlantic Groundfish (cod, wolffish, plaice, Witch flounder, haddock, pollock, redfish, and halibut), pout and windowpane flounder (MA only), and an “Other” category for all other species. For the fleet diversity metric, gears include scallop dredge (gearcodes DRS, DSC, DTC, and DTS), other dredges (gearcodes DRM, DRO, and DRU), gillnet (gearcodes GND, GNT, GNO, GNR, and GNS), hand (gearcode HND), longline (gearcodes LLB and LLP), bottom trawl (gearcodes OTB, OTF, OTO, OTC. OTS, OHS, OTR, OTT, and PTB), midwater trawls (gearcode OTM and PTM), pot (gearcodes PTL, PTW, PTC, PTE, PTF, PTH, PTL, PTO, PTS, and PTX), purse seine (gearcode PUR), and hydraulic clam dredge (gearcode DRC).Vessels were further grouped by length categories of less than 30 feet, 30 to 50 feet, 50 to 75 feet, and 75 feet and above. All revenue was deflated to real dollars using the “WPU0223” Producer Price Index with a base of January 2015. Stata code for data processing is available here. 7.1.3 Data analysis This permit-level species effective Shannon index is calculated as \\[exp(-\\sum_{i=1}^{N}p_{ijt}ln(p_{ijt}))\\] for all \\(j\\), with \\(p_{ijt}\\) representing the proportion of revenue generated by species or species group \\(i\\) for permit \\(j\\) in year \\(t\\), and is a composite of richness (the number of species landed) and abundance (the revenue generated from each species). The annual arithmetic mean value of the effective Shannon index across permits is used as the indicator of permit-level species diversity. In a similar manner, the fleet diversity metric is estimated as \\[exp(-\\sum_{i=1}^{N}p_{kt}ln(p_{kt})) \\] for all \\(k\\), where \\(p_{kt}\\) represents the proportion of total revenue generated by fleet segment \\(k\\) (gear and length combination) per year \\(t\\). The indices each run from 1996 to 2017. A count of the number of fleets active in every year is also provided to assess whether changes in fleet diversity are caused by shifts in abundance (number of fleets), or evenness (concentration of revenue). The work is based off of analysis conducted in Thunberg and Correia (2015) and published in Gaichas et al. (2016). 7.1.4 Data processing Catch and fleet diversity indicators were formatted for inclusion in the ecodata R package using the following R script. # Process commercial diversity data # More information about these data are available at https://noaa-edab.github.io/tech-doc/catch-and-fleet-diversity.html. # Data are drawn from blend of VTR trip data, CFDBS prices, vessel characteristics from PERMIT databases, # and major VTR gear by permit. Here &quot;MA&quot; and &quot;NE&quot; refer to the Mid-Atlantic and New England regions respectively, and are # not derived by EPU. library(dplyr) library(tidyr) library(stringr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_commercial_div &lt;- function(save_clean = F){ commercial_div &lt;- read.csv(file.path(raw.dir, &quot;Commercial_Diversity_2018.csv&quot;)) %&gt;% dplyr::select(-X, -Source) %&gt;% dplyr::rename(EPU = Region) %&gt;% as.data.frame() commercial_div$Var &lt;- str_replace(commercial_div$Var, &quot;diveristy&quot;, &quot;diversity&quot;) if(save_clean){ usethis::use_data(commercial_div, overwrite = T) } else { return(commercial_div) } } 7.1.5 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(4, 6, 2, 6)) soe.plot(SOE.data, &quot;Time&quot;, &quot;Mid-Atlantic average fleet diversity&quot;, stacked = &quot;A&quot;, rel.y.num = 0.9, end.start = 2008, tol = 0.15, full.trend = F, cex.stacked = 1.5) soe.stacked.axis(&#39;Year&#39;, &#39;Fleet diversity&#39;, y.line = 2.5, outer = F, rel.x.text = 1, rel.y.text = 1) soe.plot(SOE.data,&quot;Time&quot;, &quot;Mid-Atlantic fleet count&quot;, stacked = &quot;B&quot;, rel.y.num = 0.9, end.start = 2008, full.trend = F, cex.stacked = 1.5) soe.stacked.axis(&#39;Year&#39;, &#39;Fleet count&#39;, y.line = 2.5, outer = F, rel.x.text = 1, rel.y.text = 0.95) Figure 7.1: Fleet diversity (A) and fleet count (B) in the Mid Atlantic Bight. References "],
["chesapeake-bay-water-quality-standards-attainment.html", "8 Chesapeake Bay Water Quality Standards Attainment 8.1 Methods", " 8 Chesapeake Bay Water Quality Standards Attainment Description: A multimetric indicator describing the attainment status of Chesapeake Bay with respect to three water quality standards criteria, namely, dissolved oxygen, chlorophyll-a, and water clarity/submerged aquatic vegetation. Indicator category: Published method; Database pull with analysis Found in: State of the Ecosystem - Mid-Atlantic (2019) Contributor(s): Qian Zhang, Rebecca Murphy, Richard Tian, Melinda Forsyth, Emily Trentacoste, Jeni Keisman, and Peter Tango. Data steward: Qian Zhang, qzhang@chesapeakebay.net Point of contact: Qian Zhang, qzhang@chesapeakebay.net Public availability statement: Data are publicly available (see Data Sources below). 8.1 Methods To protect the aquatic living resources of Chesapeake Bay, the Chesapeake Bay Program (CBP) partnership has developed a guidance framework of ambient water quality criteria with designated uses and assessment procedures for dissolved oxygen, chlorophyll-a, and water clarity/submerged aquatic vegetation (SAV) (USEPA 2003). To achieve consistent assessment over time and between jurisdictions, a multimetric indicator was proposed by the CBP partnership to provide a means for tracking the progress in all 92 management segments of Chesapeake Bay (USEPA 20`7`). This indicator has been computed for each three-year assessment period since 1985-1987, providing an integrated measure of Chesapeake Bay’s water quality condition over the last three decades. 8.1.1 Data sources The multimetric indicator required monitoring data on dissolved oxygen (DO) concentrations, chlorophyll-a concentrations, water clarity, SAV acreage, water temperature, and salinity. SAV acreage has been measured by the Virginia Institute of Marine Science in collaboration with the CBP, which is available via http://web.vims.edu/bio/sav/StateSegmentAreaTable.htm. Data for all other parameters were obtained from the CBP Water Quality Database. These data have been routinely reported to the CBP by the Maryland Department of Natural Resources, Virginia Department of Environmental Quality, Old Dominion University, Virginia Institute of Marine Science, and citizen/volunteer monitoring initiatives. 8.1.2 Data analysis Criteria attainment assessment Monitoring data of DO, chlorophyll-a, and water clarity/SAV were processed and compared with water quality criteria thresholds according to different designated uses (DUs). These DUs are migratory spawning and nursery (MSN), open water (OW), deep water (DW), deep channel (DC), and shallow water (SW), which reflect the seasonal nature of water column structure and the life history needs of living resources. Station-level DO and chlorophyll-a data were spatially interpolated in three dimensions. Salinity and water temperature data were used to compute the vertical density structure of the water column, which was translated into layers of different DUs. Criteria attainment was determined by comparing violation rates over a 3-year period to a reference cumulative frequency distribution that represents the extent of allowable violation. This approach was implemented using FORTRAN codes, which are provided as a zipped folder. For water clarity/SAV, the single best year in the 3-year assessment period was compared with the segment-specific acreage goal, the water clarity goal, or a combination of both. For more details, refer to the Methods section of Zhang et al. (2018). Indicator calculation The multimetric indicator quantifies the fraction of segment-DU-criterion combinations that meet all applicable season-specific thresholds for each 3-year assessment period from 1985-1987 to 2015-2017. For each 3-year assessment period, all applicable segment-DU-criterion combinations were evaluated in a binomial fashion and scored 1 for “in attainment” and 0 for “nonattainment”. The classified status of each segment-DU-criterion combination was weighted via segments’ surface area and summed to obtain the multimetric index score. This weighting scheme was adopted for two reasons: (1) segments vary in size over four orders of magnitude, and (2) surface area of each segment does not change with time or DUs, unlike seasonally variable habitat volume or bottom water area (USEPA 20`7`). For more details, refer to the Methods section of Zhang et al. (2018). The indicator provides an integrated measure of Chesapeake Bay’s water quality condition (Figure 1). In 2015-2017, 42% of all tidal water segment-DU-criterion combinations are estimated to have met or exceeded applicable water quality criteria thresholds, which marks the best 3-year status since 1985-1987. The indicator has a positive and statistically significant trend from 1985 to 2017, which shows that Chesapeake Bay is on a positive trajectory toward recovery. This pattern was statistically linked to total nitrogen reduction, indicating responsiveness of attainment status to management actions implemented to reduce nutrients. Patterns of attainment of individual DUs are variable (Figure 2). Improvements in OW-DO, DC-DO, and water clarity/SAV have shown long-term improvements, which have contributed to the improvement in the overall attainment indicator. By contrast, the MSN-DO attainment experienced a sharp spike in the first few assessment periods but generally degraded after the 1997-1999, which has implications to the survival, growth, and reproduction of the migratory and resident tidal freshwater fish during spawning and nursery season in the tidal freshwater to low-salinity habitats. The status and trends of tidal segments’ attainment may be used to inform siting decisions of aquaculture operations in Chesapeake Bay. Figure 8.1: Time series of the multimetric indicator score for estimated Chesapeake Bay water quality standards attainment for each 3-year assessment period between 1985-1987 and 2015-2017. A significant positive trend for the time series is shown by the orange line (p &lt; 0.05). Figure 8.2: Time series of the estimated attainment of water quality standards (i.e., DO: dissolved oxygen; CHLA: chlorophyll-a; Clarity/SAV: water clarity/submerged aquatic vegetation) for five Chesapeake Bay designated uses (MSN: migratory spawning and nursery; OW: open water; DW: deep water; DC: deep channel; SW: shallow water) for each 3-year assessment period between 1985-1987 and 2015-2017. 8.1.3 Data processing The indicator data set was formatted for inclusion in the ecodata R package using the following R script. #Chesapeake bay water quality attainment indicator library(dplyr) library(tidyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_ches_bay_wq &lt;- function(save_clean =F){ ches_bay_wq &lt;- read.csv(file.path(raw.dir, &quot;Attainment_indicator.csv&quot;)) %&gt;% dplyr::select(Time = Year.1, Value = Total) %&gt;% mutate(Var = &quot;chesapeake bay water quality attainment&quot;, Units = &quot;estimated attainment, percent&quot;, EPU = &quot;MAB&quot;) if (save_clean){ usethis::use_data(ches_bay_wq, overwrite = T) } else { return(ches_bay_wq) } } References "],
["chl-pp.html", "9 Chlorophyll a and Primary Production 9.1 Methods", " 9 Chlorophyll a and Primary Production Description: Chlorophyll a and Primary Production Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Database pull; Database pull with analysis; Published methods Contributor(s): Kimberly Hyde Data steward: Kimberly Hyde, kimberly.hyde@noaa.gov Point of contact: Kimberly Hyde, kimberly.hyde@noaa.gov Public availability statement: Source data used in these analyses will be made publicly available. Derived data used in State of the Ecosystem Reports can be found here. 9.1 Methods 9.1.1 Data sources Level 1A ocean color remote sensing data from the Sea-viewing Wide Field-of-view Sensor (SeaWiFS) (NASA Ocean Biology Processing Group 2018) on the OrbView-2 satellite and the Moderate Resolution Imaging Spectroradiometer (MODIS) (NASA Ocean Biology Processing Group 2017) on the Aqua satellite were acquired from the NASA Ocean Biology Processing Group (OBPG). Sea Surface Temperature (SST) data included the 4 km nighttime NOAA Advanced Very High Resolution Radiometer (AVHRR) Pathfinder (Casey et al. 2010; Saha et al. 2018) and the Group for High Resolution Sea Surface Temperature (GHRSST) Multiscale Ultrahigh Resolution (MUR, version 4.1) Level 4 (Chin, Vazquez-Cuervo, and Armstrong 2017; Project 2015) data. 9.1.2 Data extraction NA 9.1.3 Data analysis The SeaWiFS and MODIS L1A files were processed using the NASA Ocean Biology Processing Group SeaDAS software version 7.4. All MODIS files were spatially subset to the U.S. East Coast (SW longitude=-82.5, SW latitude=22.5, NE longitude=-51.5, NE latitude=48.5) using L1AEXTRACT_MODIS. SeaWiFS files were subset using the same coordinates prior to begin downloaded from the Ocean Color Web Browser. SeaDAS’s L2GEN program was used to generate Level 2 (L2) files using the default settings and optimal ancillary files, and the L2BIN program spatially and temporally aggregated the L2 files to create daily Level 3 binned (L3B) files. The daily files were binned at 2 km resolution that are stored in a global, nearly equal-area, integerized sinusoidal grids and use the default L2 ocean color flag masks. The global SST data were also subset to the same East Coast region and remapped to the same sinusoidal grid. The L2 files contain several ocean color products including the default chlorophyll a; product (CHL-OCI), photosynthetic available radiation (PAR), remote sensing reflectance \\((R_{rs}(\\lambda))\\), and several inherent optical property products (IOPs). The CHL-OCI product combines two algorithms, the O’Reilly band ratio (OCx) algorithm (O’Reilly et al. 1998) and the Hu color index (CI) algorithm (Hu, Lee, and Franz 2012). The SeaDAS default CHL-OCI algorithm diverges slightly from Hu, Lee, and Franz (2012) in that the transition between CI and OCx occurs at 0.15 &lt; CI &lt; 0.2 mg m-3 to ensure a smooth transition. The regional chlorophyll a algorithm by Pan et al. (2008) was used to create a second chlorophyll product (CHL-PAN). CHL-PAN is an empirical algorithm derived from in situ sampling within the Northeast Large Marine Ecosystem (NE-LME) and demonstrated significant improvements from the standard NASA operational algorithm in the NES-LME (Pan et al. 2010). A 3rd-order polynomial function (Equation (9.1)) is used to derive [CHL-PAN] from Rrs band ratios (RBR): \\[\\begin{equation} log[\\textrm{CHL-PAN}] = A_{0} + A_{1}X + A_{2}X^{2} + A_{3}X^{3}, \\tag{9.1} \\end{equation}\\] where \\(X = log(R_{rs}(\\lambda_{1})/R_{rs}(\\lambda_{2}))\\) and \\(A_{i} (i = 0, 1, 2, \\textrm{or } 3)\\) are sensor and RBR specific coefficients: If SeaWiFS and RBR is \\(R_{rs}(490)/R_{rs}(555)(R_{^3{\\mskip -5mu/\\mskip -3mu}_5})\\) then: \\(A_0=0.02534, A_1=-3.033, A_2=2.096, A_3=-1.607\\) If SeaWiFS and RBR is \\(R_{rs}(490)/R_{rs}(670)(R_{^3{\\mskip -5mu/\\mskip -3mu}_6})\\) then: \\(A_0=1.351, A_1=-2.427, A_2=0.9395, A_3=-0.2432\\) If MODIS and RBR is \\(R_{rs}(488)/R_{rs}(547)(R_{^3{\\mskip -5mu/\\mskip -3mu}_5})\\) then: \\(A_0=0. 03664, A_1=-3.451, A_2=2.276, A_3=-1.096\\) If MODIS and RBR is \\(R_{rs}(488)/R_{rs}(667)(R_{^3{\\mskip -5mu/\\mskip -3mu}_6})\\) then: \\(A_0=1.351, A_1=-2.427, A_2=0.9395, A_3=-0.2432\\) C3/5 and C3/6 were calculated for each sensor specific RBR (R3/5 and R3/6 respectively) and then the following criteria were used to determine to derive CHL-PAN: If \\(R_{^3{\\mskip -5mu/\\mskip -3mu}_5}&gt;0.15\\) or \\(R_{6} &lt;0.0001\\) then \\(\\textrm{CHL-PAN} = C_{^3{\\mskip -5mu/\\mskip -3mu}_5};\\) Otherwise, \\(\\textrm{CHL-PAN} = \\textrm{max}(C_{^3{\\mskip -5mu/\\mskip -3mu}_5}, C_{^3{\\mskip -5mu/\\mskip -3mu}_6})\\), where \\(R_6\\) is \\(R_{rs}(670)\\) (SeaWiFS) or \\(R_{rs}(667)\\) (Pan et al. 2010). The Vertically Generalized Production Model (VGPM) estimates net primary production (PP) as a function of chlorophyll a, photosynthetically available light and the photosynthetic efficiency (Behrenfeld and Falkowski 1997). In the VGPM-Eppley version, the original temperature-dependent function to estimate the chlorophyll-specific photosynthetic efficiency is replaced with the exponential “Eppley” function (equation PP1) as modified by Morel (1991). The VGPM calculates the daily amount of carbon fixed based on the maximum rate of chlorophyll-specific carbon fixation in the water column, sea surface daily photosynthetically available radiation, the euphotic depth (the depth where light is 1% of that at the surface), chlorophyll a concentration, and the number of daylight hours (Equation (9.2)). \\[\\begin{equation} P_{max}^{b}(SST) = 4.6 * 1.065^{SST-20^{0}} \\tag{9.2} \\end{equation}\\] Where \\(P_{max}^{b}\\) is the maximum carbon fixation rate and SST is sea surface temperature. \\[\\begin{equation} PP_{eu} = 0.66125 * P_{max}^{b} * \\frac{I_{0}}{I_{0}+4.1} * Z_{eu} * \\textrm{CHL} * \\text{DL} \\tag{9.3} \\end{equation}\\] Where \\(PP_{eu}\\) is the daily amount of carbon fixed integrated from the surface to the euphotic depth (mgC m-2 day-1), \\(P_{max}^{b}\\) is the maximum carbon fixation rate within the water column (mgC mgChl-1 hr-1), \\(I_{0}\\) is the daily integrated molar photon flux of sea surface PAR (mol quanta m-2 day-1), Zeu is the euphotic depth (m), CHL is the daily interpolated CHIi-OCI (mg m-3), and DL is the photoperiod (hours) calculated for the day of the year and latitude according to Kirk (1994). The light dependent function \\((I_{0}/(I_{0}+4.1))\\) describes the relative change in the light saturation fraction of the euphotic zone as a function of surface PAR (\\(I_0\\)). Zeu is derived from an estimate of the total chlorophyll concentration within the euphotic layer (CHLeu) based on the Case I models of Morel and Berthon (1989): For \\(\\textrm{CHL}_{eu} &gt; 10.0\\;\\;\\;\\;\\;Z_{eu} = 568.2 * \\textrm{CHL}_{eu}^{-0.746}\\) For \\(\\textrm{CHL}_{eu} \\leq 10.0\\;\\;\\;\\;\\;Z_{eu} = 200.0 * \\textrm{CHL}_{eu}^{-0.293}\\) For \\(\\textrm{CHL}_{0} \\leq 1.0\\;\\;\\;\\;\\;\\textrm{CHL}_{eu} = 38.0 * \\textrm{CHL}_{0}^{0.425}\\) For \\(\\textrm{CHL}_{0} &gt; 1.0\\;\\;\\;\\;\\;\\textrm{CHL}_{eu} = 40.2 * \\textrm{CHL}_{0}^{0.507}\\) Where \\(\\textrm{CHL}_0\\) is the surface chlorophyll concentration. Prior to being input into the VGPM-Eppley model, the daily CHL-OCI and AVHRR SST data were temporally interpolated and smoothed (CHL-OCIINT and SSTINT respectively) to increase the data coverage and better match data collected from different sensors and different times. The daily PAR data are not affected by cloud cover and MUR SST data is a blended/gap free data product so these products were not interpolated. Daily data at each pixel location covering the entire date range were extracted to create a pixel time series \\((D_{x,y})\\). \\((D_{x,y})\\) are linearly interpolated based on days in the time series using interpx.pro. Prior to interpolation, the CHL data are log-transformed to account for the log-normal distribution of chlorophyll data (Campbell 1995). Interpolating the entire times series requires a large amount of processing time so the series was processed one year at a time. Each yearly series included 60 days from the previous year and 60 days from the following year to improve the interpolation at the beginning and end of the year. Following interpolation, the data are smoothed with a tri-cube filter (width=7) using IDL’s CONVOL program. In order to avoid over interpolating data when there were several days of missing data in the time series, the interpolated data were removed and replaced with blank data if the window of interpolation spanned more than 7 days for CHL or 10 days for SST. After all Dx,y pixels had been processed, the one-dimensional pixel time series were converted back to two-dimensional daily files. Statistics, including the arithmetic mean, geometric mean (for CHL and PP), standard deviation, and coefficient of variation were calculated at daily (3 and 8-day running means), weekly, monthly, and annual time steps and for several climatological periods. Annual statistics used the monthly means as inputs to avoid a summer time bias when more data is available due to reduced cloud cover. The daily, weekly, monthly and annual climatological statistics include the entire time series for each specified period. For example, the climatological January uses the monthly mean from each January in the time series and the climatological annual uses the annual mean from each year. The CHL and PP climatological statistics include data from both SeaWiFS (1997-2007) and MODIS (2008-2017). Weekly, monthly and annual anomalies were calculated for each product by taking the difference between the mean of the input time period (i.e. week, month, year) and the climatological mean for the same period. Because bio-optical data are typically log-normally distributed (Campbell 1995), the CHL and PP data were first log-transformed prior to taking the difference and then untransformed, resulting in an anomaly ratio. The ecological production unit (EPU) shapefile that excludes the estuaries was used to spatially extract all data location within an ecoregion from the statistic and anomaly files. The median values, which are equivalent to the geometric mean, were used for the CHL and PP data. For the extended time series, the 1998-2007 data use the SeaWiFS ocean color products and MODIS-Aqua products were used from 2008 to 2017. Prior to June 2002, AVHRR Pathfinder data are used as the SST source and MUR SST in subsequent years. 9.1.4 Data processing CHL and PPD time series were formatted for inclusion in the ecodata R package using the following R code. library(dplyr) library(tidyr) library(ggplot2) library(stringr) raw.dir &lt;- here::here(&quot;data-raw&quot;) ppd &lt;- read.csv(file.path(raw.dir,&quot;SOE_V2019_2-NES_ECOREGIONS-PPD-STATS_ANOMS-SEAWIFS_MODIS.csv&quot;)) %&gt;% mutate(ALGORITHM = word(str_replace(ALGORITHM, &quot;_&quot;, &quot; &quot;))) %&gt;% unite(.,VARIABLE, c(&quot;VARIABLE&quot;,&quot;SENSOR&quot;,&quot;ALGORITHM&quot;), sep = &quot; &quot;) %&gt;% mutate(VARIABLE = ifelse(str_detect(FILENAME, &quot;1998_2018&quot;), paste(VARIABLE,&quot;1998_2018&quot;), ifelse(str_detect(FILENAME, &quot;1998_2017&quot;), paste(VARIABLE, &quot;1998_2017&quot;), ifelse(str_detect(FILENAME, &quot;1997_2018&quot;), paste(VARIABLE, &quot;1997_2018&quot;), ifelse(str_detect(FILENAME, &quot;1997_2017&quot;), paste(VARIABLE, &quot;1997_2017&quot;), VARIABLE))))) %&gt;% dplyr::select(TIME, UNITS, VARIABLE, VALUE, REGION) %&gt;% dplyr::rename(Time = TIME, Units = UNITS, Var = VARIABLE, EPU = REGION, Value = VALUE) chl &lt;- read.csv(file.path(raw.dir,&quot;SOE_V2019_2-NES_ECOREGIONS-CHLOR_A-STATS_ANOMS-SEAWIFS_MODIS.csv&quot;)) %&gt;% mutate(ALGORITHM = word(str_replace(ALGORITHM, &quot;_&quot;, &quot; &quot;))) %&gt;% unite(.,VARIABLE, c(&quot;VARIABLE&quot;,&quot;SENSOR&quot;,&quot;ALGORITHM&quot;), sep = &quot; &quot;) %&gt;% mutate(VARIABLE = ifelse(str_detect(FILENAME, &quot;1998_2018&quot;), paste(VARIABLE,&quot;1998_2018&quot;), ifelse(str_detect(FILENAME, &quot;1998_2017&quot;), paste(VARIABLE, &quot;1998_2017&quot;), ifelse(str_detect(FILENAME, &quot;1997_2018&quot;), paste(VARIABLE, &quot;1997_2018&quot;), ifelse(str_detect(FILENAME, &quot;1997_2017&quot;), paste(VARIABLE, &quot;1997_2017&quot;), VARIABLE))))) %&gt;% dplyr::select(TIME, UNITS, VARIABLE, VALUE, REGION) %&gt;% dplyr::rename(Time = TIME, Units = UNITS, Var = VARIABLE, EPU = REGION, Value = VALUE) chl_pp &lt;- rbind(ppd,chl) usethis::use_data(chl_pp, overwrite = T) 9.1.5 Plotting The following figures show examples of how Chlorophyll a and primary production data have been included into State of the Ecosystem reports. The figure immediately below shows primary production anomaly plotted with the small-large copepod index (see Zooplankton) in the Mid-Atlantic Bight. Figure 9.1: MAB small-large zooplankton index and the annual primary production anomaly. Chl a and primary production data were also examined in relation to the long-term means of each series. The figure below shows data specific to the Mid-Atlantic Bight. interp_chl_pp &lt;- function(epu, year = 2018, Variable){ out &lt;- ecodata::chl_pp %&gt;% filter(str_detect(Var,Variable), EPU == epu) %&gt;% separate(.,Time, c(&quot;Year&quot;,&quot;Week&quot;),sep = 4) %&gt;% filter(Year == year) %&gt;% group_by(EPU) %&gt;% mutate(Time = 1:length(Year)) ltm_out &lt;- ecodata::chl_pp %&gt;% filter(str_detect(Var,Variable), EPU == epu) %&gt;% separate(.,Time, c(&quot;Year&quot;,&quot;Week&quot;),sep = 4) %&gt;% group_by(Week) %&gt;% dplyr::summarise(LTM = mean(Value, na.rm = T), SD = sd(Value, na.rm = T)) %&gt;% mutate(Time = 1:length(Week), sd.low = LTM - SD, sd.high = LTM + SD) %&gt;% left_join(.,out, by = c(&quot;Time&quot;)) %&gt;% mutate(status = ifelse(Value &lt; sd.high &amp; Value &gt; sd.low, &quot;near_mean&quot;, ifelse(Value &gt; sd.high, &quot;high&quot;, ifelse(Value &lt; sd.low,&quot;low&quot;,NA))), group = &quot;PLOT&quot;) return(ltm_out) } MAB_chl &lt;- interp_chl_pp(epu = &quot;MAB&quot;, Variable = &quot;WEEKLY_CHLOR_A_MEDIAN MODIS-Aqua PAN&quot;) MAB_chl_weekly &lt;- ggplot(data = MAB_chl) + geom_line(aes(x = Time, y = LTM)) + geom_ribbon(aes(x = Time, ymin = sd.low, ymax = sd.high), alpha = 0.1, fill = &quot;grey1&quot;) + geom_line(aes(x = Time, y = Value), size = 1,color = &quot;#33a02c&quot;) + ggtitle(expression(&quot;Chlorophyll&quot;~italic(a)~&quot;&quot;)) + guides(color = F) + xlab(&quot;&quot;)+ ylab(expression(&quot;CHL (mg m&quot;^-3*&quot;)&quot;)) + scale_x_continuous(breaks = seq(1,52,10), labels = c(&quot;Jan.&quot;,&quot;Mar.&quot;,&quot;May&quot;,&quot;July&quot;,&quot;Oct.&quot;,&quot;Dec.&quot;), expand = c(0.01,0.01)) + scale_color_manual(values = c(&quot;#ef8a62&quot;,&quot;#2c7fb8&quot;,&quot;#a1d99b&quot;))+ theme_ts() MAB_pp &lt;- interp_chl_pp(epu = &quot;MAB&quot;, Variable = &quot;WEEKLY_PPD_MEDIAN&quot;) MAB_pp_weekly &lt;- ggplot(data = MAB_pp) + geom_line(aes(x = Time, y = LTM)) + geom_ribbon(aes(x = Time, ymin = sd.low, ymax = sd.high), alpha = 0.1, fill = &quot;grey1&quot;) + geom_line(aes(x = Time, y = Value), size = 1,color = &quot;#33a02c&quot;) + ggtitle(expression(&quot;Primary production&quot;)) + guides(color = F) + xlab(&quot;&quot;)+ ylab(expression(&quot;PP (gC m&quot;^-2*&quot; d&quot;^-1*&quot;)&quot;)) + scale_x_continuous(breaks = seq(1,52,10), labels = c(&quot;Jan.&quot;,&quot;Mar.&quot;,&quot;May&quot;,&quot;July&quot;,&quot;Oct.&quot;,&quot;Dec.&quot;), expand = c(0.01,0.01)) + scale_color_manual(values = c(&quot;#ef8a62&quot;,&quot;#2c7fb8&quot;,&quot;#a1d99b&quot;))+ theme_ts() #MAB_chl_weekly + MAB_pp_weekly + plot_layout(ncol = 1) MAB_chl_weekly Figure 9.2: Weekly chlorophyll concentrations in the Mid-Atlantic are shown by the colored line for 2018. The long-term mean is shown in black, and shading indicates +/- 1 sample SD. In the figure below, we show monthly primary productivity on an annual time step in the Mid-Atlantic Bight. Figure 9.3: Monthly primary production trends show the annual cycle (i.e. the peak during the summer months) and the changes over time for each month. References "],
["fishing-community-climate-vulnerability.html", "10 Fishing Community Climate Vulnerability 10.1 Methods", " 10 Fishing Community Climate Vulnerability Description: Community climate vulnerability Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Database pull with analysis Contributor(s): Lisa L. Colburn Data steward: Lisa L. Colburn Point of contact: Lisa L. Colburn Public availability statement: The fisheries data used for this analysis includes confidential information and is not available to the public. 10.1 Methods 10.1.1 Data sources The data used in community climate vulnerability analyses were derived from the following sources in partnership with the Atlantic Coastal Cooperative Statistics Program’s (ACCSP) Standard Atlantic Fisheries Information System (SAFIS). Database Name Description Cfdersyyyy The dealer data are transaction-level pricing at the level of the “market-category.” These data are primarily generated through mandatory reporting by federally-permitted fish dealers. The federal reporting is supplemented with data from non-federally-permitted (state-only) fish dealers. Data are currently reported electronically in partnership with ACCSP through SAFIS. Cfvessyyy A related database that contains permit information. In these databases, the variable “port” contains the post associated with the vessel. The variable “Statenm” refers to the state of the mailing address of the owner. 10.1.2 Data extraction create table cfders2011 as select * from connection to oracle (select port, state, year, dealnum, permit, nespp3, spplndlb, sppvalue from cfders2011 where permit &gt; 0 order by permit); create table cfvess11 as select * from connection to oracle (select permit, homeport, homest from CFDBS.cfvess11 where permit &gt; 0 order by permit); create table port_name as select * from connection to oracle (select port, portnm from port order by port); create table st_name as select * from connection to oracle (select state, stateabb from statenm order by state); Truncated SAS code: /*CREATE VARIABLES FOR TOTAL LANDINGS WEIGTH AND VALUE (SUM) BY PORT OF LANDING AND BY HOMEPORT*/ data landings_ports1; set landings_ports; run; proc sort; by port state; run; proc means noprint data = landings_ports1; by port state; var spplndlb sppvalue; id port state; output out = landport_totspp sum = L_Totlb L_Totval; run; proc sort; by port; run; data landings_ports2; set landings_ports; run; proc sort; by homeport homest; run; proc means noprint data = landings_ports2; by homeport homest; var spplndlb sppvalue; id homeport homest; output out = homeport_totspp sum = H_Totlb H_Totval; run; proc sort; by homeport; run; /*CREATE SPECIES VARIABLES*/ data landings_ports_NE_spp; set landings_ports; monklb = 0; monkval = 0; /*monkfish*/ bluelb = 0; blueval = 0; /*bluefish*/ .omitted. otherlb = 0; otherval = 0; /*other - everything else*/ run; data landings_ports_NE_spp2; set landings_ports_NE_spp; if nespp3 = 012 then do; monklb = spplndlb; monkval = sppvalue; end; ...ommitted. if nespp3 = 406 then do; spotlb = spplndlb; spotval = sppvalue; end; if nespp3 not in (012, 023, 033, 051, 081, 105, 112, 115, 116, 120, 121, 122, 123, 124, 125, 132, 147, 152, 153, 155, 159, 168, 194, 197, 212, 221, 240, 250, 269, 305, 329, 330, 335, 344, 345, 351, 352, 365, 366, 367, 368, 369, 370, 372, 373, 384, 415, 418, 432, 438, 443, 444, 445, 446, 447, 464, 466, 467, 468, 469, 470, 471, 472, 507, 508, 509, 512, 517, 700, 710, 711, 724, 727, 748, 754, 769, 774, 775, 781, 786, 789, 798, 799, 800, 801, 802, 805, 806, 899, 001, 090, 069, 107, 150, 173, 196, 334, 347, 349, 364, 371, 420, 422, 481, 484, 714, 776, 777, 823, 763, 736) then do; otherlb = spplndlb; otherval = sppvalue; end; run; /*SUM SPECIES LANDINGS BY PORT OF LANDING*/ proc sort; by port; proc means noprint data = landings_ports_NE_spp2; by port state; . omitted ... id port state; output out = spp_porlnd_NE sum = ; run; proc sort; by port; run; /*SUM SPECIES LANDINGS BY HOMEPORT*/ data spp_home; set landings_ports_NE_spp2; run; proc sort; by homeport homest; proc means noprint data = spp_home; by homeport homest; . species are counted.. id homeport homest; output out = spp_homep_NE sum = ; run proc sort; by homeport; run; /*MERGE TOTAL PERMITS AND TOTAL DEALERS BY PORT OF LANDING*/ data land_port_totperm2; set land_port_totperm; run; proc sort; by port; run; data lnd_port_permit; merge spp_porlnd_NE (IN=X) land_port_totperm2 (IN=Y); by port; if X=1; run;data land_port_totdeal2; set land_port_totdeal; run; proc sort; by port; run; data lnd_port_permit_deal; merge lnd_port_permit (IN=x) land_port_totdeal2 (IN=Y); by port; if X=1; run; /*MERGE WITH PORT NAME AND STATE ABBREVIATION*/ data lnd_port_permit_deal_nm; merge lnd_port_permit_deal (IN=X) port_name (IN=Y); by port; if X=1; run; data lnd_port_permit_deal_nm_st; merge lnd_port_permit_deal_nm (IN=x) st_name (IN=Y); proc sort; by port; if X=1; run; /*MERGE TOTAL PERMITS AND TOTAL DEALERS BY HOMEPORT*/ data home_port_totperm2; set home_port_totperm; run; proc sort; by homeport; run; data home_port_permit; merge spp_homep_NE (IN=X) home_port_totperm2 (IN=Y); by homeport; if X=1; run; data home_port_totdeal2; set home_port_totdeal; run; proc sort; by homeport; run; data home_port_permit_deal; merge home_port_permit (IN=x) home_port_totdeal2 (IN=Y); by homeport; if X=1; run; proc sort; by homeport; run; /*MERGE TOTAL LANDINGS BY PORT OF LANDING*/ data lnd_port_per_deal_nm_st_tspp; merge lnd_port_permit_deal_nm_st (IN=X) landport_totspp (IN=Y); by port; if X=1; run; /*MERGE TOTAL LANDINGS BY HOMEPORT*/ data home_port_per_deal_tspp; merge home_port_permit_deal (IN=X) homeport_totspp (IN=Y); by homeport; if X=1; run; data netana.port_landing11; set lnd_port_per_deal_nm_st_tspp; if state in (22, 32, 24, 42, 7, 35, 33, 8, 23, 49, 36); run; proc sort; by port state; run; data netana.homeport11; set home_port_per_deal_tspp; if homest in (&#39;ME&#39;, &#39;NH&#39;, &#39;MA&#39;, &#39;RI&#39;, &#39;CT&#39;, &#39;NY&#39;, &#39;NJ&#39;, &#39;DE&#39;, &#39;MD&#39;, &#39;VA&#39;, &#39;NC&#39;); run; proc sort; by homeport homest; run; 10.1.3 Data analysis The results described below were developed using the methodology described in Colburn et al. (2016). Mapping community climate vulnerability - The map was produced using two variables: total value landed in a community and community species vulnerability, defined below: Communities were grouped based on total value of landings into the following categories: 1 (&lt;$ 200,000), 2 ($200,000-$9,999,999), 3 ($10,000,000-$49, 999,999), and 4 ($50,000,000 and above). Only communities with a total value landed of $200,000 or more were selected for the mapping process. Community climate vulnerability is determined by the percent contribution of species classified as very high, high, moderate, or low climate vulnerability in a community. The percent contribution of species is calculated as following: % VH &amp; H = value of landing contributed by species classified as having very high or high climate change vulnerability/total value of landings * 100 % M = value of landing contributed by species classified as having moderate climate change vulnerability/total value of landings * 100 % L = value of landing contributed by species classified as having low climate change vulnerability/ total value of landings * 100 If a community received a dominant score (50% or more) for any of the above categories, % VH &amp;, %M, or %L, then the community received a respective community species vulnerability ranking of High, Moderate, or Low. For example, if 90% of the total value landed a community is contributed by species classified as having very high or high climate change vulnerability, then this community gets “Very High/High” community species vulnerability. In case of no dominant percentage identified, the community gets a “Mixed” community species vulnerability ranking. Pie charts - The pie charts were created using the NMFS landings data pulled from NEFSC databases in Woods Hole, MA. The percent contribution of each species was calculated by dividing the total value of landings in each port by each species’ landed value. Data was calculated and graphed in a pie chart in Excel and given the colors that represent High (red), Moderate (blue), Low (yellow) climate vulnerability. The “other” category consists of species with low landings and/or those that do not have a vulnerability ranking based on Hare et al. (2016). These species were aggregated and given the color gray. 10.1.4 Plotting Figure 10.1: Commercial species vulnerability to climate change in in New England fishing communities. References "],
["conceptual-models.html", "11 Conceptual Models 11.1 Methods", " 11 Conceptual Models Description: Conceptual models for the New England (Georges Bank and Gulf of Maine) and Mid-Atlantic regions of the Northeast US Large Marine Ecosystem Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018,2019), State of the Ecosystem - Mid-Atlantic (2018,2019) Indicator category: Synthesis of published information, Extensive analysis; not yet published Contributor(s): Sarah Gaichas, Patricia Clay, Geret DePiper, Gavin Fay, Michael Fogarty, Paula Fratantoni, Robert Gamble, Sean Lucey, Charles Perretti, Patricia Pinto da Silva, Vincent Saba, Laurel Smith, Jamie Tam, Steve Traynor, Robert Wildermuth Data steward: Sarah Gaichas, sarah.gaichas@noaa.gov Point of contact: Sarah Gaichas, sarah.gaichas@noaa.gov Public availability statement: All source data aside from confidential commercial fisheries data (relevant only to some components of the conceptual models) are available to the public (see Data Sources below). 11.1 Methods Conceptual models were constructed to facilitate multidisciplinary analysis and discussion of the linked social-ecological system for integrated ecosystem assessment. The overall process was to first identify the components of the model (focal groups, human activities, environmental drivers, and objectives), and then to document criteria for including groups and linkages and what the specific links were between the components. The prototype conceptual model used to design Northeast US conceptual models for each ecosystem production unit (EPU) was designed by the California Current IEA program. The California Current IEA developed an overview conceptual model for the Northern California Current Large Marine Ecosystem (NCC), with models for each focal ecosystem component that detailed the ecological, environmental, and human system linkages. Another set of conceptual models outlined habitat linkages. An inital conceptual model for Georges Bank and the Gulf of Maine was outlined at the 2015 ICES WGNARS meeting. It specified four categories: Large scale drivers, focal ecosystem components, human activities, and human well being. Strategic management objectives were included in the conceptual model, which had not been done in the NCC. Focal ecosystem components were defined as aggregate species groups that had associated US management objectives (outlined within WGNARS for IEAs, see DePiper et al. (2017)): groundfish, forage fish, fished invertebrates, living habitat, and protected species. These categories roughly align with Fishery Managment Plans (FMPs) for the New England Fishery Management Council. The Mid-Atlantic conceptual model was developed along similar lines, but the focal groups included demersals, forage fish, squids, medium pelagics, clams/quahogs, and protected species to better align with the Mid Atlantic Council’s FMPs. After the initial draft model was outlined, working groups were formed to develop three submodels following the CCE example: ecological, environmental, and human dimensions. The general approach was to specify what was being included in each group, what relationship was represented by a link between groups, what threshold of the relationship was used to determine whether a relationship was significant enough to be included (we did not want to model everything), the direction and uncertainty of the link, and documentation supporting the link between groups. This information was recorded in a spreadsheet. Submodels were then merged together by common components using the “merge” function in the (currently unavailable) desktop version of Mental Modeler (http://www.mentalmodeler.org/#home; Gray et al. (2013)). The process was applied to Georges Bank (GB), the Gulf of Maine (GOM), and the Mid-Atlantic Bight (MAB) Ecological Production Units. 11.1.1 Data sources 11.1.1.1 Ecological submodels Published food web (EMAX) models for each subregion (J.S. Link et al. 2006; Link et al. 2008), food habits data collected by NEFSC trawl surveys (Smith and Link 2010), and other literature sources (Smith et al. 2015) were consulted. Expert judgement was also used to adjust historical information to current conditions, and to include broad habitat linkages to Focal groups. 11.1.1.2 Environmental submodels Published literature on the primary environmental drivers (seasonal and interannual) in each EPU was consulted. Sources for Georges Bank included Backus and Bourne (1987) and Townsend et al. (2006). Sources for the Gulf of Maine included Smith (1983), Smith et al. (2001), Mupparapu and Brown (2002), Townsend et al. (2006), Smith et al. (2012), and Mountain (2012). Sources for the Mid Atlantic Bight included Houghton et al. (1982), Beardsley et al. (1985), Lentz (2003), Mountain (2003), Glenn et al. (2004), Sullivan, Cowen, and Steves (2005), Castelao et al. (2008), Shearman and Lentz (2009), Castelao, Glenn, and Schofield (2010), Gong, Kohut, and Glenn (2010), Gawarkiewicz et al. (2012), Forsyth, Andres, and Gawarkiewicz (2015), Fratantoni, Holzwarth-Davis, and Taylor (2015), Zhang and Gawarkiewicz (2015), Miller, Hare, and Alade (2016), and Lentz (2017). 11.1.1.3 Human dimensions submodels Fishery catch and bycatch information was drawn from multiple regional datasets, incuding the Greater Atlantic Regional Office Vessel Trip Reports &amp; Commercial Fisheries Dealer databases, Northeast Fishery Observer Program &amp; Northeast At-Sea Monitoring databases, Northeast Fishery Science Center Social Sciences Branch cost survey, and the Marine Recreational Informational Program database. Further synthesis of human welfare derived from fisheries was drawn from Färe, Kirkley, and Walden (2006), Walden et al. (2012), Lee and Thunberg (2013), Lee (2014), and Lee, Steinback, and Wallmo (2017). Bycatch of protected species was taken from Waring et al. (2015), with additional insights from Bisack and Magnusson (2014). The top 3 linkages were drawn for each node. For example, the top 3 recreational species for the Mid-Atlantic were used to draw linkages between the recreational fishery and species focal groups. A similar approach was used for relevant commercial fisheries in each region. Habitat-fishery linkages were drawn from unpublished reports, including: Mid-Atlantic Fishery Management Council. 2016. Amendment 16 to the Atlantic Mackerel, Squid, and Butterfish Fishery Management Plan: Measures to protect deep sea corals from Impacts of Fishing Gear. Environmental Assessment, Regulatory Impact Review, and Initial Regulatory Flexibility Analysis. Dover, DE. August, 2016. NOAA. 2016. Deep sea coral research and technology program 2016 Report to Congress. http://www.habitat.noaa.gov/protection/corals/deepseacorals.html retrieved February 8, 2017. New England Fishery Management Council. 2016. Habitat Omnibus Deep-Sea Coral Amendment: Draft. http://www.nefmc.org/library/omnibus-deep-sea-coral-amendment Retrieved Feb 8, 2017. Bachman et al. 2011. The Swept Area Seabed Impact (SASI) Model: A Tool for Analyzing the Effects of Fishing on Essential Fish Habitat. New England Fisheries Management Council Report. Newburyport, MA. Tourism and habitat linkages were drawn from unpublished reports, including: http://neers.org/RESOURCES/Bibliographies.html Great Bay (GoM) resources http://greatbay.org/about/publications.htm Meaney, C.R. and C. Demarest. 2006. Coastal Polution and New England Fisheries. Report for the New England Fisheries Management Council. Newburyport, MA. List of valuation studies, by subregion and/or state, can be found at http://www.oceaneconomics.org/nonmarket/valestim.asp. Published literature on human activities in each EPU was consulted. Sources for protected species and tourism links included Hoagland and Meeks (2000) and Lee (2010). Sources for links between environmental drivers and human activities included Adams (1973), Matzarakis and Freitas (2001), Scott, McBoyle, and Schwartzentruber (2004), Hess, Malilay, and Parkinson (2008), Colburn and Jepson (2012), Jepson and Colburn (2013), and Colburn et al. (2016). Sources for cultural practices and attachments links included Pauly (1997), McGoodwin (2001), St Martin (2001), Norris-Raynbird (2004), Pollnac et al. (2006), Clay and Olson (2007), Clay and Olson (2008), Everett and Aitchison (2008), Donkersloot (2010), Lord (2011), Halpern et al. (2012), Wynveen, Kyle, and Sutton (2012), Cortes-Vazquez and Zedalis (2013), Koehn, Reineman, and Kittinger (2013), Potschin and Haines-Young (2013), Reed et al. (2013), Urquhart and Acott (2013), Blasiak et al. (2014), Klain, Satterfield, and Chan (2014), Poe, Norman, and Levin (2014), Brown (2015), Donatuto and Poe (2015), Khakzad and Griffith (2016), Oberg et al. (2016), and Seara, Clay, and Colburn (2016). 11.1.2 Data extraction 11.1.2.1 Ecological submodels “Data” included model estimated quantities to determine whether inclusion thresholds were met for each potential link in the conceptual model. A matrix with diet composition for each modeled group is an input to the food web model. A matrix of mortalities caused by each predator and fishery on each modeled group is a direct ouput of a food web model (e.g. Ecopath). Food web model biomasss flows between species, fisheries, and detritus were summarized using algorithms implemented in visual basic by Kerim Aydin, NOAA NMFS Alaska Fisheries Science Center. Because EMAX model groups were aggregated across species, selected diet compositions for individual species were taken from the NEFSC food habits database using the FEAST program for selected species (example query below). These diet queries were consulted as supplemental information. Example FEAST sql script for Cod weighted diet on Georges Bank. Queries for different species are standardized by the FEAST application and would differ only in the svspp code. Select svspp,year,cruise6,stratum,station,catsex,pdid,pdgutw,pdlen,pdwgt,perpyw,pyamtw,COLLCAT,numlen,pyamtv from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; order by svspp,year,cruise6,stratum,station,pdid,COLLCAT Select distinct svspp,year,cruise6,stratum,station from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; order by svspp,year,cruise6,stratum,station Select distinct svspp,year,cruise6,stratum,station,catsex,catnum from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; order by svspp,year,cruise6,stratum,station Select distinct COLLCAT from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; order by COLLCAT Select distinct svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,pdgutw,pdwgt from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; order by svspp,year,cruise6,stratum,station,catsex,pdid Select svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT,sum(perpyw),sum(pyamtw),sum(pyamtv) from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; group by svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT order by svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT Select svspp,year,cruise6,stratum,station,COLLCAT,sum(pyamtv) sumpvol from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; group by svspp,year,cruise6,stratum,station,COLLCAT order by svspp,year,cruise6,stratum,station,COLLCAT Select svspp,year,cruise6,stratum,station, count(distinct pdid) nstom from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; group by svspp,year,cruise6,stratum,station,catsex order by svspp,year,cruise6,stratum,station Select svspp,year,cruise6,stratum,station,pdlen,numlen,count(distinct pdid) nstom from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and numlen is not null and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; group by svspp,year,cruise6,stratum,station,pdlen,numlen,catsex order by svspp,year,cruise6,stratum,station,pdlen Select svspp,year,cruise6,stratum,station,pdlen,COLLCAT,sum(pyamtv) sumpvol from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; group by svspp,year,cruise6,stratum,station,pdlen,COLLCAT order by svspp,year,cruise6,stratum,station,pdlen,COLLCAT Select distinct svspp,year,cruise6,stratum,station,pdid,pdlen from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and numlen is null and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; Select distinct year,cruise6,stratum,station,beglat,beglon from fhdbs.allfh_feast where pynam &lt;&gt; &#39;BLOWN&#39; and pynam &lt;&gt; &#39;PRESERVED&#39; and pynam &lt;&gt; &#39; &#39; and svspp=&#39;073&#39; and YEAR BETWEEN &#39;1973&#39; AND &#39;2016&#39; and GEOAREA=&#39;GB&#39; order by year,cruise6,stratum,station 11.1.2.2 Environmental submodels Information was synthesized entirely from published sources and expert knowledge; no additional data extraction was completed for the environmental submodels. 11.1.2.3 Human dimensions submodels Recreational fisheries data were extracted from the 2010-2014 MRIP datasets. Original data can be found here for each region (New England or Mid-Atlantic as defined by states). Commercial fishing data was developed as part of the State of the Ecosystem Report, including revenue and food production estimates, with data extraction metodology discussed in the relevant sections of the technical memo. In addition, the Northeast Regional Input/Output Model (Steinback and Thunberg 2006) was used as the basis for the strength of the employment linkages. 11.1.3 Data analysis 11.1.3.1 Ecological submodels Aggregated diet and mortality information was examined to determine the type of link, direction of link, and which links between which groups should be inclded in the conceptual models. Two types of ecological links were defined using food web models: prey links and predation/fishing mortality links. Prey links resulted in positve links between the prey group and the focal group, while predation/fishing mortality links resulted in negative links to the focal group to represent energy flows. The intent was to include only the most important linkages between focal groups and with other groups supporting or causing mortality on focal species groups. Therefore, threshold levels of diet and mortality were established (based on those that would select the top 1-3 prey and predators of each focal group): 10% to include a link (or add a linked group) in the model and 20% to include as a strong link. A Primary Production group was included in each model and linked to pelagic habitat to allow environmental effects on habitat to be connected to the ecologial submodel. Uncertainty for the inclusion of each link and for the magnitude of each link was qualitatively assessed and noted in the spreadsheet. Four habitat categories (Pelagic, Seafloor and Demersal, Nearshore, and Freshwater and Estuarine) were included in ecological submodels as placeholders to be developed further along with habitat-specific research. Expert opinion was used to include the strongest links between each habitat type and each Focal group (noting that across species and life stages, members of these aggregate groups likely occupy many if not all of the habitat types). Link direction and strength were not specified. Environmental drivers were designed to link to habitats, rather than directly to Focal groups, to represent each habitat’s important mediation function. EMAX model groups were aggregated to focal groups for the Georges Bank (GB), Gulf of Maine (GOM) and Mid-Atlantic Bight (MAB) conceptual models according to Table 11.1. “Linked groups” directly support or impact the Focal groups as described above. Table 11.1: Relationship between food web model groups and conceptual model focal groups Group Type Region Conceptual model group EMAX group(s) Notes Focal GB Commercial Fishery Fishery Focal GB Fished Inverts Megabenthos filterers Focal GB Forage Fish Sum of Small pelagics–commercial, other, anadromous, and squids Focal GB Groundfish Sum of Demersals–omnivores, benthivores, and piscivores Focal GB Protected Species Sum of Baleen Whales, Odontocetes, and Seabirds Pinnipeds not included in GB EMAX Linked GB Benthos Sum of Macrobenthos—polychaetes, crustaceans, molluscs, other and Megabenthos—other Linked GB Copepods and Micronecton Sum of Copepods–small and large, and Micronekton Linked GB Detritus and Bacteria Sum of Bacteria and Detritus-POC Linked GB Gelatinous zooplankton Gelatinous zooplankton Linked GB Primary Production Phytoplankton-Primary production Focal GOM Commercial Fishery Fishery Focal GOM Fished Inverts Megabenthos filterers Focal GOM Forage Fish Sum of Small pelagics–commercial, other, anadromous, and squids Focal GOM Groundfish Sum of Demersals–omnivores, benthivores, and piscivores Focal GOM Protected Species Sum of Baleen Whales, Odontocetes, Pinnipeds, and Seabirds Linked GOM Benthos Sum of Macrobenthos—polychaetes, crustaceans, molluscs, other and Megabenthos—other Linked GOM Copepods and Micronecton Sum of Copepods–small and large, and Micronekton Linked GOM Detritus and Bacteria Sum of Bacteria and Detritus-POC Linked GOM Gelatinous zooplankton Gelatinous zooplankton Linked GOM Primary Production Phytoplankton-Primary production Focal MAB Clams Quahogs Megabenthos filterers Focal MAB Commercial Fishery Fishery Focal MAB Demerals Sum of Demersals–omnivores, benthivores, and piscivores Focal MAB Forage Fish Sum of Small pelagics–commercial, other, and anadromous Focal MAB Medium Pelagics Medium pelagics Focal MAB Protected Species Sum of Baleen whales and Odontocetes Seabirds not included in MAB EMAX Focal MAB Squids Small pelagics–squids Linked MAB Benthos Sum of Macrobenthos—polychaetes, crustaceans, molluscs, other Linked MAB Copepods and Micronecton Sum of Copepods–small and large, and Micronekton Linked MAB Detritus and Bacteria Sum of Bacteria and Detritus-POC Linked MAB Gelatinous zooplankton Gelatinous zooplankton Linked MAB Primary Production Phytoplankton-Primary production Linked MAB Sharks Sum of Sharks—pelagic and coastal Ecological submodels were constructed and visualized in Mental Modeler (Fig. 11.1). Here, we show only the Gulf of Maine submodels as examples. Figure 11.1: Gulf of Maine Ecological submodel 11.1.3.2 Environmental submodels Environmental submodels were designed to link key oceanographic processes in each ecosystem production unit to the four general habitat categories (Pelagic, Seafloor and Demersal, Nearshore, and Freshwater and Estuarine) with emphasis on the most important physical processes in each ecosystem based on expert knowledge as supported by literature review. The basis of each submodel were environmental variables observable at management-relevant scales as identified by WGNARS: Surface and Bottom Water Temperature and Salinity, Freshwater Input, and Stratification (as well as sea ice timing and cover, which is not relevant to the northeast US shelf). Key drivers changing these observable variables and thus structuring habitat dynamics in each Ecological Production Units were added to the model using expert consensus. Environmental submodels were initially constructed and visualized in Mental Modeler (Fig. 11.2). Figure 11.2: Gulf of Maine Environmental submodel 11.1.3.3 Human dimensions submodels The top 3 species from each mode of recreational fishing (shoreside, private boat, party/charter) were used to assess the potential for missing links between the recreational fishing activity and biological focal components. Given the predominance of Mid-Atlantic groundfish in recreational fishing off New England (summer flounder, bluefish, striped bass), a Mid-Atlantic groundfish focal component was added to the Georges Bank EPU model. The magnitude of benefits generated from recreational fishing was scaled to reflect expert knowledge of target species, coupled with the MRIP data highlighted above. Scales were held consistent across the focal components within recreational fishing. No additional biological focal components were added to the commercial fishing activity, beyond what was developed in the ecological submodel. Benefits derived from commercial fishing were scaled to be consistent with the State of the Ecosystem revenue estimates, as modulated by expert knowledge and additional data sources. For example,the percentage of landings sold as food was used to map fishing activity to the commercial fishery food production objective, and the Northeast Regional Input/Output Model (Steinback and Thunberg 2006) was used to define the strength of the employment linkages. For profitability, expert knowledge was used to reweight revenue landings, based on ancillary cost data available (Das, Chhandita 2013; Das, Chhandita 2014). Human activities and objectives for the conceptual sub model are defined in DePiper et al. (2017). As shown in Figure 11.3, human dimensions submodels were also initially constructed and visualized in Mental Modeler. Figure 11.3: Gulf of Maine Human dimensions submodel 11.1.3.4 Merged models All links and groups from each submodel were preserved in the full merged model for each system. Mental modeler was used to merge the submodels. Full models were then re-drawn in Dia (http://dia-installer.de/) with color codes for each model component type for improved readability. Examples for each system are below. Figure 11.4: Georges Bank conceptual model Figure 11.5: Gulf of Maine conceptual model Figure 11.6: Mid-Atlantic Bight conceptual model 11.1.3.5 Communication tools The merged models were redrawn for use in communications with the public. These versions lead off the State of the Ecosystem reports for both Fishery Management Councils to provide an overview of linkages between environmental drivers, ecological, and human systems. Figure 11.7: New England conceptual model for public communication Figure 11.8: Mid-Atlantic conceptual model for public communication References "],
["fish-condition-indicator.html", "12 Fish Condition Indicator 12.1 Methods", " 12 Fish Condition Indicator Description: Relative condition Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018,2019), State of the Ecosystem - Mid-Atlantic (2018,2019) Indicator category: Database pull with analysis Contributor(s): Laurel Smith Data steward: Laurel Smith, laurel.smith@noaa.gov Point of contact: Laurel Smith, laurel.smith@noaa.gov Public availability statement: NEFSC survey data used in these analyses are available upon request (see BTS metadata for access procedures). Derived condition data are available here. 12.1 Methods Relative condition (Kn) was introduced by Cren (1951) as a way to remove the influence of length on condition, and Blackwell, Brown, and Willis (2000) noted that Kn may be useful in detecting prolonged physical stress on a fish populations. Relative condition is calculated as \\(Kn = W/W&#39;,\\) where W is the weight of an individual fish and W’ is the predicted length-specific mean weight for the fish population in a given region. “Here, relative condition was calculated for finfish stocks commonly caught on the Northeast Fisheries Science Center’s (NEFSC) autumn and spring bottom trawl surveys, from 1992-present. Where data allowed, predicted length-weight parameters were calculated for W’ by species, sex and season over the time period 1992-2012. When sample sizes of individual fish weights and lengths were too low, parameters were calculated for aggregated spring and fall survey data over the same time period. Fall survey relative condition was calculated by sex for those species that exhibited differences in growth between sexes and aggregated across sex for those that did not. 12.1.1 Data sources Individual fish lengths (to the nearest 0.5 cm) and weights (grams) were collected on the NEFSC bottom trawl surveys from 1992-present aboard RVs Albatross IV, Delaware II and the Henry B. Bigelow (see Survdat). A small number of outlier values were removed when calculating the length-weight parameters. 12.1.2 Data extraction Data were extracted from NEFSC’s survey database (SVDBS) using SQL. SQL query: SELECT cruise6,stratum,tow,station, year,month,day,time,beglat,beglon,setdepth, surftemp,bottemp, svspp,sex,length,age,maturity,indid,indwt,stom_volume,stom_wgt, expcatchwt, expcatchnum from connection to oracle (select b.cruise6,b.stratum,b.tow,b.station, s.est_year year,est_month month,est_day day, substr(est_time,1,2)||substr(est_time,4,2) time, round(substr(beglat,1,2) + (substr(beglat,3,7)/60),6) beglat, round(((substr(beglon,1,2) + (substr(beglon,3,7)/60)) * -1), 6) beglon, setdepth,surftemp, bottemp, b.svspp,sex,length,age,maturity,indid,indwt,stom_volume,stom_wgt, expcatchwt, expcatchnum from union_fscs_svbio b, union_fscs_svcat p, union_fscs_svsta s, svdbs_cruises c where season = &amp;sson and b.svspp in (&#39;013&#39;,&#39;015&#39;,&#39;023&#39;,&#39;026&#39;,&#39;028&#39;,&#39;032&#39;,&#39;072&#39;,&#39;073&#39;,&#39;074&#39;,&#39;075&#39;,&#39;076&#39;,&#39;077&#39;,&#39;078&#39;,&#39;102&#39;,&#39;103&#39;,&#39;104&#39;,&#39;105&#39;,&#39;106&#39;,&#39;107&#39;,&#39;108&#39;,&#39;121&#39;,&#39;131&#39;,&#39;135&#39;,&#39;141&#39;,&#39;143&#39;,&#39;145&#39;,&#39;155&#39;,&#39;164&#39;,&#39;193&#39;,&#39;197&#39;) and (b.cruise6=s.cruise6) and (c.cruise6=b.cruise6) and (p.cruise6=c.cruise6) and (p.stratum=b.stratum) and (b.stratum=s.stratum) and (p.station=b.station) and (b.station=s.station) and (p.svspp=b.svspp) and (p.tow=b.tow) and (b.tow=s.tow) ); %put &amp;sqlxmsg; %put &amp;sqlxrc; create view spp as select comname, svspp from connection to oracle (select comname, svspp from svspecies_list); %put &amp;sqlxmsg; %put &amp;sqlxrc; execute (commit) by oracle; 12.1.3 Data analysis The following growth curve was fit through individual fish lengths and weights from the NEFSC bottom trawl survey data from 1992-2012 to produce reference length-weight parameters: \\[\\textrm{Weight} = e^{Fall_{coef}} * \\textrm{Length}^{Fall_{exp}},\\] where length is in cm and weight is in kg. Fall survey data were used where sample sizes allowed for growth curve estimation, otherwise data from spring and fall seasons were combined. Individual fish lengths from NEFSC fall bottom trawl survey from 1992-2017 were then used to calculate predicted weights using the reference length-weight parameters. Relative condition (Kn) was calculated annually by species and sex (for sexually dimorphic species) by dividing individual fish weights by the predicted weight. The following R code was used in the analysis: # Length-weight parameter calculation: function (data, min.n = 25, min.range = 5, data.avail = NA, data.avail.bigelow = NA) { if(is.null(dim(data.avail))) data.avail &lt;- lw.data.availability(data, min.n, min.range) data.avail &lt;- data.avail[apply(data.avail[,2:5], 1, any),] if(is.null(dim(data.avail.bigelow)))data.avail.bigelow &lt;- lw.data.availability(data[data$data.source == &quot;Bigelow&quot;,], min.n, min.range) data.avail.bigelow &lt;- data.avail.bigelow[apply(data.avail.bigelow[,2:5], 1, any),] data.spp &lt;- as.numeric(rownames(data.avail[data.avail$sex.season == TRUE,])) lw.output &lt;- data.frame(matrix(ncol = 12)) names(lw.output) &lt;- c(&quot;species.name&quot;, &quot;species.code&quot;, &quot;source&quot;, &quot;sex&quot;, &quot;season&quot;, &quot;slope&quot;, &quot;slope.p&quot;, &quot;intercept&quot;, &quot;intercept.p&quot;, &quot;min.length&quot;, &quot;max.length&quot;, &quot;check.diff&quot;) for(sp in data.spp){ sp.data &lt;- lw.data[lw.data$species == sp,] sp.name &lt;- unique(as.character(species.names$scientific_name[species.names$svspp == sp])) print(sp.name) #All model print(&quot;Species Level&quot;) this.data &lt;- bigelow.test(sp.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], &quot;weight.log~length.log&quot;, &quot;species&quot;) ds &lt;- this.data[[2]] this.data &lt;- this.data[[1]] if(!is.null(dim(this.data))){ this.model &lt;- lm(weight.log~length.log, data = this.data) model.coefs &lt;- coef(this.model) model.summary &lt;- coef(summary(this.model)) length.range &lt;- range(sp.data$length) length.log &lt;- log(seq(length.range[1], length.range[2], by=.5)) species &lt;- rep(sp, length(length.log)) predict.length &lt;- data.frame(species, length.log) check.diffs &lt;- plot.lw(this.data, this.model, &quot;species&quot;, sp.name, predict.length) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, NA, NA, model.coefs[&quot;length.log&quot;], model.summary[&quot;length.log&quot;, &quot;Pr(&gt;|t|)&quot;], model.coefs[&quot;(Intercept)&quot;], model.summary[&quot;(Intercept)&quot;, &quot;Pr(&gt;|t|)&quot;], length.range[1], length.range[2], check.diffs)) } #Sex model print(&quot;Sex Level&quot;) this.data &lt;- sp.data[sp.data$sex &gt; 0,] model.definition &lt;- &quot;weight.log~length.log * factor(sex)&quot; this.data &lt;- bigelow.test(this.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, &quot;sex&quot;) ds &lt;- this.data[[2]] this.data &lt;- this.data[[1]] if(!is.null(dim(this.data))){ this.model &lt;- lm(weight.log~length.log * factor(sex), data = this.data) male.range &lt;- range(this.data$length[this.data$sex == 1]) female.range &lt;- range(this.data$length[this.data$sex == 2]) model.summary &lt;- coef(summary(this.model)) if(any(model.summary[grep(&quot;sex&quot;, rownames(model.summary)), &quot;Pr(&gt;|t|)&quot;] &lt;= .05)){ model.coefs &lt;- coef(this.model) length.log &lt;- rep(log(seq(length.range[1], length.range[2], by=.5)),2) sex &lt;- c(rep(1, length(length.log)/2), rep(2, length(length.log)/2)) predict.length &lt;- data.frame(length.log, sex) check.diffs &lt;- plot.lw(this.data, this.model, &quot;sex&quot;, sp.name, predict.length) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, 1, NA, model.coefs[&quot;length.log&quot;], model.summary[&quot;length.log&quot;, &quot;Pr(&gt;|t|)&quot;], model.coefs[&quot;(Intercept)&quot;], model.summary[&quot;(Intercept)&quot;, &quot;Pr(&gt;|t|)&quot;], male.range[1], male.range[2], check.diffs[1])) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, 2, NA, model.coefs[&quot;length.log&quot;] + model.coefs[&quot;length.log:factor(sex)2&quot;], model.summary[&quot;length.log:factor(sex)2&quot;, &quot;Pr(&gt;|t|)&quot;], model.coefs[&quot;(Intercept)&quot;] + model.coefs[&quot;factor(sex)2&quot;], model.summary[&quot;factor(sex)2&quot;, &quot;Pr(&gt;|t|)&quot;], female.range[1], female.range[2], check.diffs[2])) } else{ print(paste(&quot;Model parameters not significantly different for&quot;, model.definition)) } } #Season model model.definition &lt;- &quot;weight.log~length.log * factor(season)&quot; this.data &lt;- bigelow.test(sp.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, &quot;season&quot;) ds &lt;- this.data[[2]] this.data &lt;- this.data[[1]] if(!is.null(dim(this.data))){ this.model &lt;- lm(weight.log~length.log * factor(season), data = this.data) fall.range &lt;- range(this.data$length[this.data$season == &quot;FALL&quot;]) spring.range &lt;- range(this.data$length[this.data$season == &quot;SPRING&quot;]) model.summary &lt;- coef(summary(this.model)) if(any(model.summary[grep(&quot;season&quot;, rownames(model.summary)), &quot;Pr(&gt;|t|)&quot;] &lt;= .05)){ model.coefs &lt;- coef(this.model) length.log &lt;- rep(log(seq(length.range[1], length.range[2], by=.5)),2) season &lt;- c(rep(&quot;FALL&quot;, length(length.log)/2), rep(&quot;SPRING&quot;, length(length.log)/2)) predict.length &lt;- data.frame(length.log, season) check.diffs &lt;- plot.lw(this.data, this.model, &quot;season&quot;, sp.name, predict.length) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, NA, &quot;FALL&quot;, model.coefs[&quot;length.log&quot;], model.summary[&quot;length.log&quot;, &quot;Pr(&gt;|t|)&quot;], model.coefs[&quot;(Intercept)&quot;], model.summary[&quot;(Intercept)&quot;, &quot;Pr(&gt;|t|)&quot;], fall.range[1], fall.range[2], check.diffs[1])) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, NA, &quot;SPRING&quot;, model.coefs[&quot;length.log&quot;] + model.coefs[&quot;length.log:factor(season)SPRING&quot;], model.summary[&quot;length.log:factor(season)SPRING&quot;, &quot;Pr(&gt;|t|)&quot;], model.coefs[&quot;(Intercept)&quot;] + model.coefs[&quot;factor(season)SPRING&quot;], model.summary[&quot;factor(season)SPRING&quot;, &quot;Pr(&gt;|t|)&quot;], spring.range[1], spring.range[2], check.diffs[1])) } else{ print(paste(&quot;Model parameters not significantly different for&quot;, model.definition)) } } #Sex-Season model this.data &lt;- sp.data[sp.data$sex &gt; 0,] model.definition &lt;- &quot;weight.log~length.log * factor(sex) * factor(season)&quot; this.data &lt;- bigelow.test(this.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, c(&quot;sex&quot;,&quot;season&quot;)) ds &lt;- this.data[[2]] this.data &lt;- this.data[[1]] if(!is.null(dim(this.data))){ this.model &lt;- lm(weight.log~length.log * factor(sex) * factor(season), data = this.data) model.summary &lt;- coef(summary(this.model)) male.fall.range &lt;- range(this.data$length[this.data$season == &quot;FALL&quot; &amp; this.data$sex == 1]) male.spring.range &lt;- range(this.data$length[this.data$season == &quot;SPRING&quot; &amp; this.data$sex == 1]) female.fall.range &lt;- range(this.data$length[this.data$season == &quot;FALL&quot; &amp; this.data$sex == 2]) female.spring.range &lt;- range(this.data$length[this.data$season == &quot;SPRING&quot; &amp; this.data$sex == 2]) if(any(model.summary[grep(&quot;season&quot;, rownames(model.summary)), &quot;Pr(&gt;|t|)&quot;] &lt;= .05 | any(model.summary[grep(&quot;season&quot;, rownames(model.summary)), &quot;Pr(&gt;|t|)&quot;] &lt;= .05))){ model.coefs &lt;- coef(this.model) male.fall.int &lt;- model.coefs[&quot;(Intercept)&quot;] male.fall.int.p &lt;- model.summary[&quot;(Intercept)&quot;, &quot;Pr(&gt;|t|)&quot;] male.fall.slope &lt;- model.coefs[&quot;length.log&quot;] male.fall.slope.p &lt;- model.summary[&quot;length.log&quot;, &quot;Pr(&gt;|t|)&quot;] male.spring.int &lt;- male.fall.int + model.coefs[&quot;factor(season)SPRING&quot;] male.spring.int.p &lt;- model.summary[&quot;factor(season)SPRING&quot;, &quot;Pr(&gt;|t|)&quot;] male.spring.slope &lt;- male.fall.slope + model.coefs[&quot;length.log:factor(season)SPRING&quot;] male.spring.slope.p &lt;- model.summary[&quot;length.log:factor(season)SPRING&quot;, &quot;Pr(&gt;|t|)&quot;] female.fall.int &lt;- male.fall.int + model.coefs[&quot;factor(sex)2&quot;] female.fall.int.p &lt;- model.summary[&quot;factor(sex)2&quot;, &quot;Pr(&gt;|t|)&quot;] female.fall.slope &lt;- male.fall.slope + model.coefs[&quot;length.log:factor(sex)2&quot;] female.fall.slope.p &lt;- model.summary[&quot;length.log:factor(sex)2&quot;, &quot;Pr(&gt;|t|)&quot;] female.spring.int &lt;- male.spring.int + model.coefs[&quot;factor(sex)2&quot;] + model.coefs[&quot;factor(sex)2:factor(season)SPRING&quot;] female.spring.int.p &lt;- model.summary[&quot;factor(sex)2:factor(season)SPRING&quot;, &quot;Pr(&gt;|t|)&quot;] female.spring.slope &lt;- male.spring.slope + model.coefs[&quot;length.log:factor(sex)2&quot;] + model.coefs[&quot;length.log:factor(sex)2:factor(season)SPRING&quot;] female.spring.slope.p &lt;- model.summary[&quot;length.log:factor(sex)2:factor(season)SPRING&quot;, &quot;Pr(&gt;|t|)&quot;] length.log &lt;- rep(log(seq(length.range[1], length.range[2], by=.5)),4) sex &lt;- c(rep(&quot;1&quot;, length(length.log)/2), rep(&quot;2&quot;, length(length.log)/2)) season &lt;- rep(c(rep(&quot;FALL&quot;, length(length.log)/4), rep(&quot;SPRING&quot;, length(length.log)/4)),2) predict.length &lt;- data.frame(length.log, sex, season) check.diffs &lt;- plot.lw(this.data, this.model, c(&quot;sex&quot;, &quot;season&quot;), sp.name, predict.length) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, 1, &quot;FALL&quot;, male.fall.slope, male.fall.slope.p, male.fall.int, male.fall.int.p, male.fall.range[1], male.fall.range[2], check.diffs[1])) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, 1, &quot;SPRING&quot;, male.spring.slope, male.spring.slope.p, male.spring.int, male.spring.int.p, male.spring.range[1], male.spring.range[2],check.diffs[2])) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, 2, &quot;FALL&quot;, female.fall.slope, female.fall.slope.p, female.fall.int, female.fall.int.p, female.fall.range[1], female.fall.range[2],check.diffs[3])) lw.output &lt;- rbind(lw.output, c(sp.name, sp, ds, 2, &quot;SPRING&quot;, female.spring.slope, female.spring.slope.p, female.spring.int, female.spring.int.p, female.spring.range[1], female.spring.range[2],check.diffs[4])) } else{ print(paste(&quot;Model parameters not significantly different for&quot;, model.definition)) } } } lw.output &lt;- lw.output[!is.na(lw.output$species.code),] lw.output } #Relative Condition: proc import datafile = &quot;lw_parameters.csv&quot; out = LWparams dbms = csv replace; getnames = yes; run; data LWparams; set LWparams; if LW_SVSPP = 13 then svspp = &#39;013&#39;; if LW_SVSPP = 15 then svspp = &#39;015&#39;; if LW_SVSPP = 23 then svspp = &#39;023&#39;; if LW_SVSPP = 26 then svspp = &#39;026&#39;; if LW_SVSPP = 28 then svspp = &#39;028&#39;; if LW_SVSPP = 32 then svspp = &#39;032&#39;; if LW_SVSPP = 72 then svspp = &#39;072&#39;; if LW_SVSPP = 73 then svspp = &#39;073&#39;; if LW_SVSPP = 74 then svspp = &#39;074&#39;; if LW_SVSPP = 75 then svspp = &#39;075&#39;; if LW_SVSPP = 76 then svspp = &#39;076&#39;; if LW_SVSPP = 77 then svspp = &#39;077&#39;; if LW_SVSPP = 78 then svspp = &#39;078&#39;; if LW_SVSPP ge 100 then svspp = LW_SVSPP; if sexMF = &#39;M&#39; then sex = &#39;1&#39;; if sexMF = &#39;F&#39; then sex = &#39;2&#39;; if sexMF = &#39; &#39; then sex = &#39;0&#39;; if EXPONENT_FALL = . then EXPONENT_FALL=SEASONLESS_EXPONENT; if EXPONENT_SPRING = . then EXPONENT_SPRING=SEASONLESS_EXPONENT; if COEFFICIENT_FALL = . then COEFFICIENT_FALL=SEASONLESS_COEFFICIENT; if COEFFICIENT_SPRING = . then COEFFICIENT_SPRING=SEASONLESS_COEFFICIENT; proc sort data=LWparams; by svspp sex; proc sort data=lenwt; by svspp sex; data lwdatpar (keep =cruise6 stratum tow station year month day time beglat beglon setdepth surftemp bottemp svspp sex length age maturity indid indwt stom_volume stom_wgt expcatchwt expcatchnum COEFFICIENT_SPRING EXPONENT_SPRING COEFFICIENT_FALL EXPONENT_FALL SEASONLESS_COEFFICIENT SEASONLESS_EXPONENT); merge lenwt (in=d) LWparams (in=p); by svspp sex; data sortlw; set lwdatpar; proc sort; by svspp sex year; data lwdata; set sortlw; if indwt = . then delete; if length = . then delete; if indwt &gt;0; svspp1 = svspp*1; indwtg=indwt*1000.0; cond=indwtg/(length**3); if EXPONENT_FALL gt 0 then predwt = (exp(COEFFICIENT_FALL))*length**EXPONENT_FALL; if EXPONENT_FALL = . then predwt = (exp(SEASONLESS_COEFFICIENT))*length**SEASONLESS_EXPONENT; if EXPONENT_FALL gt 0 then predwtPK = exp(COEFFICIENT_FALL+(EXPONENT_FALL*log(length))); if EXPONENT_FALL = . then predwtPK = exp(SEASONLESS_COEFFICIENT+(SEASONLESS_EXPONENT*log(length))); ***Relative condition; RelWt = indwt/predwt*100; proc sort data=lwdata; by svspp1 sex year; run; 12.1.4 Plotting Figure 12.1: Normalized condition factors of managed species in the Northeast Large Marine Ecosystem. References "],
["epu.html", "13 Ecological Production Units 13.1 Methods", " 13 Ecological Production Units Description: Ecological Production Units Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Extensive analysis, not yet published Contributor(s): Robert Gamble Data steward: NA Point of contact: Robert Gamble, robert.gamble@noaa.gov Public availability statement: Ecological production unit (EPU) shapefiles are available here. More information about source data used to derive EPUs can be found here. 13.1 Methods To define ecological production units, we assembled a set of physiographic, oceanographic and biotic variables on the Northeast U.S. Continental Shelf, an area of approximately 264,000 km within the 200 m isobath. The physiographic and hydrographic variables selected have been extensively used in previous analyses of oceanic provinces and regions (e.g Roff and Taylor 2000). Primary production estimates have also been widely employed for this purpose in conjunction with physical variables (Longhurst 2007) to define ecological provinces throughout the world ocean. We did not include information on higher trophic levels or fishing patterns in our. The biomass and production of higher trophic levels in this region has been sharply perturbed by fishing and other anthropogenic influences. Similarly, fishing patterns are affected by regulatory change, market and economic factors and other external influences. Because these malleable patterns of change are often unconnected with underlying productivity, we excluded factors directly related to fishing practices. The physiographic variables considered in this analysis are listed in Table 13.1. They include bathymetry and surficial sediments. The physical oceanographic and hydrographic measurements include sea surface temperature, annual temperature span, and temperature gradient water derived from satellite observations for the period 1998 to 2007. 13.1.1 Data sources Shipboard observations for surface and bottom water temperature and salinity in surveys conducted in spring and fall. Daily sea surface temperature (SST, °C) measurements at 4 km resolution were derived from nighttime scenes composited from the AVHRR sensor on NOAA’s polar-orbiting satellites and from NASA’s MODIS TERRA and MODIS AQUA sensors. We extracted information for the annual mean SST, temperature span, and temperature gradients from these sources. The latter metric provides information on frontal zone locations. Table 13.1: Variables used in derivation of Ecological Production Units. Variables Sampling Method Units Surficial Sediments Benthic Grab Krumbian Scale Sea Surface Temperature Satellite Imagery (4km grid) &amp;deg;C annual average Sea Surface Temperature Satellite Imagery (4km grid) dimensionless Sea Surface Temperature Satellite Imagery (4km grid) &amp;deg;C annual average Surface Temperature Shipboard hydrography (point) &amp;deg;C (Spring and Fall) Bottom Temperature Shipboard hydrography (point) &amp;deg;C (Spring and Fall) Surface Salinity Shipboard hydrography (point) psu (Spring and Fall) Bottom Salinity Shipboard hydrography (point) psu (Spring and Fall) Stratification Shipboard hydrography (point) Sigma-t units (Spring and Fall) Chlorophyll-a Satellite Imagery (1.25 km grid) mg/C/m3 (annual average) Chlorophyll-a gradient Satellite Imagery (1.25 km grid) dimensionless Chlorophyll-a span Satellite Imagery (1.25 km grid) mg/C/m3 (annual average) Primary Production Satellite Imagery (1.25 km grid) gC/m3/year (cumulative) Primary Production gradient Satellite Imagery (1.25 km grid) dimensionless Primary Production span Satellite Imagery (1.25 km grid) gC/m3/year (cumulative) The biotic measurements included satellite-derived estimates of chlorophyll a (CHLa) mean concentration, annual span, and CHLa gradients and related measures of primary production. Daily merged SeaWiFS/MODIS-Aqua CHLa (CHL, mg m-3) and SeaiWiFS photosynthetically available radiation (PAR, Einsteins m-2 d-1) scenes at 1.25 km resolution were obtained from NASA Ocean Biology Processing Group. 13.1.2 Data extraction NA 13.1.3 Data analysis In all cases, we standardized the data to common spatial units by taking annual means of each observation type within spatial units of 10’ latitude by 10’ longitude to account for the disparate spatial and temporal scales at which these observations are taken. There are over 1000 spatial cells in this analysis. Shipboard sampling used to obtain direct hydrographic measurements is constrained by a minimum sampling depth of 27 m specified on the basis of prescribed safe operating procedures. As a result nearshore waters are not fully represented in our initial specifications of ecological production units. The size of the spatial units employed further reflects a compromise between retaining spatial detail and minimizing the need for spatial interpolation of some data sets. For shipboard data sets characterized by relatively coarse spatial resolution, where necessary, we first constructed an interpolated map using an inverse distance weighting function before including it in the analysis. Although alternative interpolation schemes based on geostatistical approaches are possible, we considered the inverse distance weighting function to be both tractable and robust for this application. We first employed a spatial principal components analysis (PCA; e.g. Pielou 1984; Legendre and Legendre 1998) to examine the multivariate structure of the data and to account for any inter-correlations among the variables to be used in subsequent analysis. The variables included in the analysis exhibited generally skewed distributions and we therefore transformed each to natural logarithms prior to analysis. The PCA was performed on the correlation matrix of the transformed observations. We selected the eigenvectors associated with eigenvalues of the dispersion matrix with scores greater than 1.0 (the Kaiser-Guttman criterion; Legendre and Legendre 1998) for all subsequent analysis. These eigenvectors represent orthogonal linear combinations of the original variables used in the analysis. We delineated ecological subunits by applying a disjoint cluster based on Euclidean distances using the K-means procedure (Legendre and Legendre 1998) on the principal component scores The use of non-independent variables can strongly influence the results of classification analyses of this type (Pielou 1984), hence the interest in using the PCA results in the cluster. The eigenvectors were represented as standard normal deviates. We used a Pseudo-F Statistic described by Milligan and Cooper (1985) to objectively define the number of clusters to use in the analysis. The general approach employed is similar to that of Host et al. (1996) for the development of regional ecosystem classifications for terrestrial systems. After the analyses were done, we next considered options for interpolation of nearshore boundaries resulting from depth-related constraints on shipboard observations. For this, we relied on information from satellite imagery. For the missing nearshore areas in the Gulf of Maine and Mid-Atlantic Bight, the satellite information for chlorophyll concentration and sea surface temperature indicated a direct extension from adjacent observations. For the Nantucket Shoals region south of Cape Cod, similarities in tidal mixing patterns reflected in chlorophyll and temperature observations indicated an affinity with Georges Bank and the boundaries were changed accordingly. Finally, we next considered consolidation of ecological subareas so that nearshore regions are considered to be special zones nested within the adjacent shelf regions. Similar consideration led to nesting the continental slope regions within adjacent shelf regions in the Mid-Atlantic and Georges Bank regions. This led to four major units: Mid-Atlantic Bight, Georges Bank, Western-Central Gulf of Maine (simply “Gulf of Maine” in the SOE), and Scotian Shelf-Eastern Gulf of Maine. As the State of the Ecosystem reports are specific to FMC managed regions, the Scotian Shelf-Eastern Gulf of Maine EPU is not considered in SOE indicator analyses. Figure 13.1: Map of the four Ecological Production Units, including the Mid-Atlantic Bight (light blue), Georges Bank (red), Western-Central Gulf of Maine (or Gulf of Maine; green), and Scotian Shelf-Eastern Gulf of Maine (dark blue) 13.1.4 Data processing Shapefiles were converted to sf objects for inclusion in the ecodata R package using the following R code. # A function to process and make ecological production unit (EPU) shapefiles available in the ecodata package. # Read more about the delineation of EPUs at https://noaa-edab.github.io/tech-doc/epu.html library(sf) library(rgdal) library(raster) library(rnaturalearth) gis.dir &lt;- here::here(&#39;inst&#39;,&#39;extdata&#39;,&#39;gis&#39;) crs &lt;- &quot;+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77+x_0=0 +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; get_epu_sf &lt;- function(save_clean){ epu_shp &lt;- readOGR(file.path(gis.dir, &quot;EPU_Extended.shp&quot;), verbose = F) crs(epu_shp) &lt;- crs epu_sf &lt;- as(epu_shp, &quot;sf&quot;) if (save_clean){ usethis::use_data(epu_sf) } else { return(epu_sf) } } References "],
["gulf-stream-index.html", "14 Gulf Stream Index 14.1 Methods", " 14 Gulf Stream Index Description: Annual time series of the Gulf Stream index Indicator category: Published method Found in: State of the Ecosystem - New England (2019) Contributor(s): Terry Joyce, Rong Zhang Data steward: Vincent Saba, vincent.saba@noaa.gov Point of contact: Vincent Saba, vincent.saba@noaa.gov Public availability statement: Source data are publicly available 14.1 Methods Summarized from Joyce et al. (2019), ocean temperature data from NOAA NODC were sorted by latitude, longitude, and time using a resolution of 1° of longitude, latitude, and 3 months of time, respectively, with a Gaussian squared weighting from the selected desired point in a window twice the size of the desired resolution. Editing was used to reject duplicate samples and 3\\(\\sigma\\) outliers from each selected sample point prior to performing the weighting and averaging; the latter was only carried out when there were at least three data points in the selected interval for each sample point. Typically, 50 or more data values were available. The resulting temperature field was therefore smoothed. Data along the Gulf Stream north wall at nine data points were used to assemble a spatial/temporal sampling of the temperature at 200m data along the north wall from 75°W to 55°W. The leading mode of temperature variability of the Gulf Stream is equivalent to a north‐south shift of 50–100 km, which is zonally of one sign and amounts to 50% of the seasonal‐interannual variance between 75°W and 55°W. The temporal behavior of this mode (PC1) shows the temporal shift of the Gulf Stream path with a dominant approximately 8‐ to 10‐year periodicity over much of the period. 14.1.1 Data sources Ocean temperatures at 200 m are available at https://www.nodc.noaa.gov/OC5/3M_HEAT_CONTENT/. 14.1.2 Data analysis For detailed analytical methods, see Joyce et al. (2019). 14.1.3 Data Processing The Gulf Stream index data set was formatted for inclusion in the ecodata R package with the following code. # Processing for Gulf Stream Index data # GSI = degrees latitude above the average Gulf Stream position based # on ocean temperature at 200m (15 C) depth between 55W to 75W. library(dplyr) library(tidyr) library(lubridate) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_gsi &lt;- function(save_clean = F){ gsi &lt;- read.csv(file.path(raw.dir, &quot;GSI.csv&quot;)) %&gt;% dplyr::rename(Time = Year, Value = GSI) %&gt;% mutate(Var = &quot;gulf stream index&quot;, Units = &quot;latitude anomaly&quot;, EPU = &quot;All&quot;) if (save_clean){ usethis::use_data(gsi, overwrite = T) } else { return(gsi) } } References "],
["harbor-porpoise-bycatch.html", "15 Harbor Porpoise Bycatch 15.1 Methods", " 15 Harbor Porpoise Bycatch Description: Harbor Porpoise Indicator Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Synthesis of published information; Published methods Contributor(s): Christopher D. Orphandies Data steward: Chris Orphanides, chris.orphanides@noaa.gov Point of contact: Chris Orphanides, chris.orphanides@noaa.gov Public availability statement: Source data are available in public stock assessment reports (2018 report in-press). Derived data as shown in the 2018 SOE reports are available here 15.1 Methods 15.1.1 Data sources Reported harbor porpoise bycatch estimates and potential biological removal levels can be found in publicly available documents; detailed here. The most recent bycatch estimates for 2016 were taken from the 2018 stock assessment (in-press). More detailed documentation as to the methods employed can be found in NOAA Fisheries Northeast Fisheries Science Center (NEFSC) Center Reference Documents (CRDs) found on the NEFSC publications page. The document for the 2016 estimates (CRD 19-04) is available here. Additional methodological details are available for previous year’s estimates and are documented in numerous published CRDs: CRD 17-18, CRD-16-05, CRD 15-15, CRD 14-02, CRD 13-13, CRD 11-08, CRD 10-10, CRD 07-20, CRD 06-13, CRD 03-18, CRD 01-15, and CRD 99-17. 15.1.2 Data extraction Annual gillnet bycatch estimates are documented in a CRD (see sources above). These feed into the Stock Assessment Reports which report both the annual bycatch estimate and the mean 5-year estimate. The 5-year estimate is the one used for management purposes, so that is the one provided for the SOE plot. 15.1.3 Data analysis Bycatch estimates as found in stock assessment reports were plotted along with confidence intervals. The confidence intervals were calculated from published CVs assuming a normal distribution (\\(\\sigma = \\mu CV\\); \\(CI = \\bar{x} \\pm \\sigma * 1.96\\)). Data were analyzed and formatted for inclusion in the ecodata R package using the following R code. # Process harbor porpoise bycatch estimates # Time series figure is 5-yr running mean for harbor porpoise bycatch estimates for the Northeast US across all fisheries. library(dplyr) library(tidyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) #HP bycatch time series estimates------------------------------------------------------ get_harborporpoise &lt;- function(save_clean = F){ d &lt;- read.csv(file.path(raw.dir,&quot;1994-2017_5yr_hp_est.csv&quot;)) #Create confidence intervals var1nnum &lt;- log(1+d$CV^2) c &lt;- exp(1.96 * sqrt(var1nnum)) d$up95ci &lt;- d$EST * c d$lo95ci &lt;- d$EST / c harborporpoise &lt;- d %&gt;% dplyr::rename(Time = YEAR) %&gt;% gather(., Var, Value, -Time) %&gt;% mutate(Units = &quot;N&quot;, EPU = &quot;All&quot;, Var, Var = plyr::mapvalues(Var, from = c(&quot;EST&quot;,&quot;CV&quot;,&quot;PBR&quot;,&quot;up95ci&quot;,&quot;lo95ci&quot;), to = c(&quot;harbor porpoise bycatch estimate&quot;, &quot;harbor porpoise bycatch cv&quot;, &quot;harbor porpoise bycatch pbr&quot;, &quot;harbor porpoise bycatch up95ci&quot;, &quot;harbor porpoise bycatch lo95ci&quot;))) %&gt;% as.data.frame() if (save_clean){ usethis::use_data(harborporpoise, overwrite = R) } else { return(harborporpoise) } } 15.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mar = c(4, 6, 2, 6)) soe.plot(SOE.data, &#39;Time&#39;, &quot;Harbor porpoise bycatch estimates&quot;, rel.y.num = 1.2, end.start = 2007, full.trend = F, point.cex = 1, ymax = F, y.upper = 2500, mean_line = F, x.label = &#39;Year&#39;, y.label = &#39;Bycatch, n&#39;, rel.y.text = 1) legend(2000, 2250, legend = &quot;Potential Biological Removal&quot;, col = adjustcolor(&quot;red&quot;, .5), lwd = 3, bty = &quot;n&quot;, cex = 0.9) #credible intervals and PBI lw_CI &lt;- SOE.data[Var == &#39;Harbor porpoise bycatch 2.5 CI&#39;, list(Time, Value)] up_CI &lt;- SOE.data[Var == &#39;Harbor porpoise bycatch 97.5 CI&#39;, list(Time, Value)] pbi &lt;- SOE.data[Var == &#39;Harbor porpoise potential biological removal&#39;, list(Time, Value)] points(pbi, type = &quot;l&quot;, lty = 1, col = adjustcolor(&quot;red&quot;, .5), lwd = 3) points(lw_CI, type = &quot;l&quot;, lty = 2, col = adjustcolor(&quot;darkorange&quot;, .9), lwd = 2.5) points(up_CI, type = &quot;l&quot;, lty = 2, col = adjustcolor(&quot;darkorange&quot;, .9), lwd = 2.5) Figure 15.1: Harbor porpoise bycatch estimated shown with Potential Biological Removal (red) and confidence intervals (orange). "],
["ichthyoplankton-diversity.html", "16 Ichthyoplankton Diversity 16.1 Methods", " 16 Ichthyoplankton Diversity Description: NOAA NEFSC Oceans and Climate branch public ichthyoplankton dataset Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Database pull with analysis Contributor(s): Harvey J. Walsh Data steward: Harvey Walsh, harvey.walsh@noaa.gov Point of contact: Harvey Walsh, harvey.walsh@noaa.gov Public availability statement: Source data are available to the public here. Derived data for this indicator are available here. 16.1 Methods Data from the NOAA NEFSC Oceans and Climate branch public dataset were used to examine changes in diversity of abundance among 45 ichthyoplankton taxa. The 45 taxa were established (Walsh et al. 2015), and include the most abundant taxa from the 1970s to present that represent consistency in the identification of larvae. 16.1.1 Data sources Multi-species plankton surveys cover the entire Northeast US shelf from Cape Hatteras, North Carolina, to Cape Sable, Nova Scotia, four to six times per year. A random-stratified design based on the NEFSC bottom trawl survey design (Azarovitz 1981) is used to collect samples from 47 strata. The number of strata is lower than the trawl survey as many of the narrow inshore and shelf-break strata are combined in the EcoMon design. The area encompassed by each stratum determined the number of samples in each stratum. Samples were collected both day and night using a 61 cm bongo net. Net tow speed was 1.5 knots and maximum sample depth was 200 m. Double oblique tows were a minimum of 5 mintues in duration, and fished from the surface to within 5 m of the seabed or to a maximum depth of 200 m. The volume filtered of all collections was measured with mechanical flowmeters mounted across the mouth of each net. Processing of most samples was conducted at the Morski Instytut Rybacki (MIR) in Szczecin, Poland; the remaining samples were processed at the NEFSC or the Atlantic Reference Center, St Andrews, Canada. Larvae were identified to the lowest possible taxa and enumerated for each sample. Taxon abundance for each station was standardized to number under 10 m-2 sea surface. 16.1.2 Data extraction Data retrieved from NOAA NEFSC Oceans and Climate branch public dataset (Filename: “EcoMon_Plankton_Data_v3_0.xlsx”, File Date: 10/20/2016). 16.1.3 Data analysis All detailed data processing steps are not currently included in this document, but general steps are outlined. Data were grouped into seasons: spring = February, March, April and fall = September, October, November. Stratified weighted mean abundance was calculated for each taxon for each year and season across all plankton strata (n = 47) for 17 years (1999 to 2015). Shannon Diversity Index and count of positive taxon was calculated for each season and year. MATLAB code used to calculate diversity indices: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Calculates Shannon Diversity Index of Ichthyoplankton data % % Input: excel file of ichthyoplankton data % % USES: index_SaW.m % % last modified: 03August2018, HJW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %% Data retrieved from NOAA NEFSC Oceans and Climate branch public dataset % ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/ % Filename: EcoMon_Plankton_Data_v3_0.xlsx % File Date: 10/20/2016 % Data processing - not included in this file % Data grouped into seasons: spring = Feb to Apr, fall = Sept to Nov % Stratified weighted mean abundance was calculated for each taxon for each year and season % Abundance across all plankton strata (n = 47) for 17 years (1999 to 2015) %% Import aggregated data from spreadsheet % Script for importing data from the following spreadsheet: % % Workbook: /Users/hwalsh/NEFSC larval samples/CombinedData/SOE_Diversity/NEFSCIchthyoplanktonAbundance.xlsx % Worksheet: Sheet1 %% Output Data % SprSW = Spring Shannon Diversity Index % SprCount = Spring count of positive ichthyoplankton taxa (Max = 45) % FallSW = Fall Shannon Diversity Index % FallCount = Fall count of positive ichthyoplankton taxa (Max = 45) %% Import the data [~, ~, raw] = xlsread(&#39;/Users/hwalsh/NEFSC larval samples/CombinedData/SOE_Diversity/NEFSCIchthyoplanktonAbundance.xlsx&#39;,&#39;Sheet1&#39;); raw = raw(2:end,7:end); %% Create output variable data = reshape([raw{:}],size(raw)); %% Create table NEFSCIchthyoplanktonAbundance = table; %% Allocate imported array to column variable names NEFSCIchthyoplanktonAbundance.Brevoortiatyrannus = data(:,1); NEFSCIchthyoplanktonAbundance.Clupeaharengus = data(:,2); NEFSCIchthyoplanktonAbundance.Cyclothonespp = data(:,3); NEFSCIchthyoplanktonAbundance.Diaphusspp = data(:,4); NEFSCIchthyoplanktonAbundance.Ceratoscopelusmaderensis = data(:,5); NEFSCIchthyoplanktonAbundance.Benthosemaspp = data(:,6); NEFSCIchthyoplanktonAbundance.Urophycisspp = data(:,7); NEFSCIchthyoplanktonAbundance.Enchelyopuscimbrius = data(:,8); NEFSCIchthyoplanktonAbundance.Gadusmorhua = data(:,9); NEFSCIchthyoplanktonAbundance.Melanogrammusaeglefinus = data(:,10); NEFSCIchthyoplanktonAbundance.Pollachiusvirens = data(:,11); NEFSCIchthyoplanktonAbundance.Merlucciusalbidus = data(:,12); NEFSCIchthyoplanktonAbundance.Merlucciusbilinearis = data(:,13); NEFSCIchthyoplanktonAbundance.Centropristisstriata = data(:,14); NEFSCIchthyoplanktonAbundance.Pomatomussaltatrix = data(:,15); NEFSCIchthyoplanktonAbundance.Cynoscionregalis = data(:,16); NEFSCIchthyoplanktonAbundance.Leiostomusxanthurus = data(:,17); NEFSCIchthyoplanktonAbundance.Menticirrhusspp = data(:,18); NEFSCIchthyoplanktonAbundance.Micropogoniasundulatus = data(:,19); NEFSCIchthyoplanktonAbundance.Tautogolabrusadspersus = data(:,20); NEFSCIchthyoplanktonAbundance.Tautogaonitis = data(:,21); NEFSCIchthyoplanktonAbundance.Auxisspp = data(:,22); NEFSCIchthyoplanktonAbundance.Scomberscombrus = data(:,23); NEFSCIchthyoplanktonAbundance.Peprilusspp = data(:,24); NEFSCIchthyoplanktonAbundance.Sebastesspp = data(:,25); NEFSCIchthyoplanktonAbundance.Prionotusspp = data(:,26); NEFSCIchthyoplanktonAbundance.Myoxocephalusaenaeus = data(:,27); NEFSCIchthyoplanktonAbundance.Myoxocephalusoctodecemspinosus = data(:,28); NEFSCIchthyoplanktonAbundance.Ammodytesspp = data(:,29); NEFSCIchthyoplanktonAbundance.Pholisgunnellus = data(:,30); NEFSCIchthyoplanktonAbundance.Ulvariasubbifurcata = data(:,31); NEFSCIchthyoplanktonAbundance.Anarhichasspp = data(:,32); NEFSCIchthyoplanktonAbundance.Citharichthysarctifrons = data(:,33); NEFSCIchthyoplanktonAbundance.Etropusspp = data(:,34); NEFSCIchthyoplanktonAbundance.Syaciumspp = data(:,35); NEFSCIchthyoplanktonAbundance.Bothusspp = data(:,36); NEFSCIchthyoplanktonAbundance.Hippoglossinaoblonga = data(:,37); NEFSCIchthyoplanktonAbundance.Paralichthysdentatus = data(:,38); NEFSCIchthyoplanktonAbundance.Pseudopleuronectesamericanus = data(:,39); NEFSCIchthyoplanktonAbundance.Hippoglossoidesplatessoides = data(:,40); NEFSCIchthyoplanktonAbundance.Limandaferruginea = data(:,41); NEFSCIchthyoplanktonAbundance.Glyptocephaluscynoglossus = data(:,42); NEFSCIchthyoplanktonAbundance.Scophthalmusaquosus = data(:,43); NEFSCIchthyoplanktonAbundance.Symphurusspp = data(:,44); NEFSCIchthyoplanktonAbundance.Lophiusamericanus = data(:,45); %% Clear temporary variables clearvars data raw; %% Spearate Spring (Spr) and Fall data Spr=table2array(NEFSCIchthyoplanktonAbundance(1:17,:))&#39;; Fall=table2array(NEFSCIchthyoplanktonAbundance(18:34,:))&#39;; %% Shannon-Wiener index [SprSW]=index_SaW(Spr,exp(1)); [FallSW]=index_SaW(Fall,exp(1)); %% Count of number taxa per year SprCount=zeros(1,length(SprSW)); for ii=1:length(Spr) for yy=1:length(SprCount) if Spr(ii,yy)&gt;0 SprCount(1,yy)=SprCount(1,yy)+1; end end end FallCount=zeros(1,length(FallSW)); for ii=1:length(Fall) for yy=1:length(FallCount) if Fall(ii,yy)&gt;0 FallCount(1,yy)=FallCount(1,yy)+1; end end end clear ii yy 16.1.4 Data processing Ichthyoplankton diversity data sets were formatted for inclusion in the ecodata R package using the following R code. #Ichthyoplankton species counts, diversity, and abundance library(dplyr) library(tidyr) library(readxl) library(stringr) raw.dir &lt;- here::here(&quot;data-raw&quot;) ichthyo_spec_counts &lt;- read_excel(file.path(raw.dir,&quot;NEFSCIchthyoplanktonSpeciesCount_v3_3.xlsx&quot;)) %&gt;% dplyr::select(-Source) %&gt;% dplyr::rename(Time = Year, EPU = Region) %&gt;% mutate(Var = paste(Season,str_replace_all(Var, &quot;_&quot;, &quot; &quot;)), EPU = ifelse(EPU == &quot;all&quot;, &quot;All&quot;, EPU)) %&gt;% dplyr::select(-Season) %&gt;% mutate(Value = as.numeric(Value)) ichthyo_diversity &lt;- read_excel(file.path(raw.dir,&quot;NEFSCIchthyoplanktonDiversity_v3_3.xlsx&quot;)) %&gt;% dplyr::select(-Source) %&gt;% dplyr::rename(Time = Year, EPU = Region) %&gt;% mutate(Var = paste(Season,str_replace_all(Var, &quot;_&quot;, &quot; &quot;)), EPU = ifelse(EPU == &quot;all&quot;, &quot;All&quot;, EPU)) %&gt;% dplyr::select(-Season) %&gt;% mutate(Value = as.numeric(Value)) %&gt;% rbind(.,ichthyo_spec_counts) usethis::use_data(ichthyo_diversity, overwrite = T) # ichthyo_abundance &lt;- read_excel(file.path(raw.dir,&quot;NEFSCIchthyoplanktonAbundance_v3_3.xlsx&quot;)) %&gt;% # dplyr::select(-Source) %&gt;% # dplyr::rename(Time = Year, # EPU = Region) %&gt;% # mutate(Var = paste(Season,str_replace_all(Var, &quot;_&quot;, &quot; &quot;)), # EPU = ifelse(EPU == &quot;all&quot;, &quot;All&quot;, # EPU)) %&gt;% # tidyr::gather(.,Var,Value,-Time,-Season,-Var,-Units,-EPU) %&gt;% # unite(.,Var, c(&quot;Season&quot;,&quot;Var&quot;), sep = &quot; &quot;) %&gt;% # mutate(Value = as.numeric(Value)) # # usethis::use_data(ichthyo_abundance, overwrite = T) 16.1.5 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4)) soe.plot(SOE.data, &quot;Time&quot;, &quot;Spring_Ich_Shannon Diversity Index&quot;, stacked = &quot;A&quot;, rel.y.num = 1.1, end.start = 2007, full.trend = F, cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Fall_Ich_Shannon Diversity Index&quot;, stacked = &quot;B&quot;, rel.y.num = 1.1, end.start = 2007, full.trend = F, cex.stacked = 1.5) soe.stacked.axis(&quot;Year&quot;, &quot;Shannon Index&quot;, y.line = 2.5) Figure 16.1: Ichthyoplankton Shannon diversity in the spring (A) and fall (B) in the Northeast Large Marine Ecosystem. References "],
["comdat.html", "17 Commercial Landings Data 17.1 Methods", " 17 Commercial Landings Data Description: Commercial landings data pull Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Database pull Contributor(s): Sean Lucey Data steward: Sean Lucey, Sean.Lucey@noaa.gov Point of contact: Sean Lucey, Sean.Lucey@noaa.gov Public availability statement: Raw data are not publically available due to confidentiality of individual fishery participants. Derived indicator outputs are available here. 17.1 Methods Fisheries dependent data for the Northeast Shelf extend back several decades. Data from the 1960s on are housed in the Commercial database (CFDBS) of the Northeast Fisheries Science Center which contains the commercial fisheries dealer purchase records (weigh-outs) collected by NMFS Statistical Reporting Specialists and state agencies from Maine to Virginia. The data format has changed slightly over the time series with three distinct time frames as noted in Table 17.1 below. Table 17.1: Data formats Table Years WOLANDS 1964 - 1981 WODETS 1982 - 1993 CFDETS_AA &gt; 1994 Comlands is an R database pull that consolidates the landings records from 1964 on and attempts to associate them with NAFO statistical areas (Figure 17.1). The script is divided into three sections. The first pulls domestic landings data from the yearly landings tables and merges them into a single data source. The second section applies an algorithm to associate landings that are not allocated to a statistical area using similar characteristics of the trip to trips with known areas. The final section pulls foreign landings from the NAFO website and rectifies species and gear codes so they can be merged along with domestic landings. Figure 17.1: Map of the North Atlantic Fisheries Organization (NAFO) Statistical Areas. Colors represent the Ecological Production Unit (EPU) with which the statistical area is associated. During the first section, the Comlands script pulls the temporal and spatial information as well as vessel and gear characteristics associated with the landings in addition to the weight, value, and utilization code of each species in the landings record. The script includes a toggle to use landed weights as opposed to live weights. For all but shellfish species, live weights are used for the State of the Ecosystem report. Due to the volume of data contained within each yearly landings table, landings are aggregated by species, utilization code, and area as well as by month, gear, and tonnage class. All weights are then converted from pounds to metric tons. Landings values are also adjusted for inflation using the Producer Price Index by Commodity for Processed Foods and Feeds: Unprocessed and Packaged Fish. Inflation is based on January of the terminal year of the data pull ensuring that all values are in current dollar prices. Table 17.2: Gear types Major gear Otter Trawls Scallop Dredges Other Dredges Gillnets Longlines Seines Pots/Traps Midwater Other Several species have additional steps after the data is pulled from CFDBS. Skates are typically landed as a species complex. In order to segregate the catch into species, the ratio of individual skate species in the NEFSC bottom trawl survey is used to disaggregate the landings. A similar algorithm is used to separate silver and offshore hake which can be mistaken for one another. Finally, Atlantic herring landings are pulled from a separate database as the most accurate weights are housed by the State of Maine. Comlands pulls from the State database and replaces the less accurate numbers from the federal database. The majority of landings data are associated with a NAFO Statistical Area. For those that are not, Comlands attempts to assign them to an area using similar characteristics of trips where the area is known. To simplify this task, landings data are further aggregated into quarter and half year, small and large vessels, and eight major gear categories (Table 17.2). Landings are then proportioned to areas that meet similar characteristics based on the proportion of landings in each area by that temporal/vessel/gear combination. If a given attribute is unknown, the algorithm attempts to assign it one, once again based on matched characteristics of known trips. Statistical areas are then assigned to their respective Ecological Production Unit (Table 17.3). Table 17.3: Statistical areas making up each EPU EPU Stat Areas Gulf of Maine 500, 510, 512, 513, 514, 515 Georges Bank 521, 522, 523, 524, 525, 526, 551, 552, 561, 562 Mid-Atlantic 537, 539, 600, 612, 613, 614, 615, 616, 621, 622, 625, 626, 631, 632 The final step of Comlands is to pull the foreign landings from the NAFO database. US landings are removed from this extraction so as not to be double counted. NAFO codes and CFDBS codes differ so the script rectifies those codes to ensure that the data is seamlessly merged into the domestic landings. Foreign landings are flagged so that they can be removed if so desired. 17.1.1 Data sources Comland is a database query of the NEFSC commercial fishery database (CFDBS). More information about the CFDBS is available here. 17.1.2 Data extraction R code used in the extraction process described above: #Comland.r #Version now controlled by git - originally part of comcatch.r #Grab commercial landings data from US and Foreign countries (NAFO) #Need to fix menhaden data #SML #Requires the following files: # data.dir.2\\\\Comland_skates_hakes.R # data.dir\\\\Menhaden.csv # data.dir.3\\\\SS_NAFO_21A.csv # data.dir.3\\\\species.txt #User parameters if(Sys.info()[&#39;sysname&#39;]==&quot;Windows&quot;){ data.dir &lt;- &quot;L:\\\\EcoAP\\\\Data\\\\Commercial&quot; data.dir.2 &lt;- &quot;L:\\\\Rworkspace\\\\RCom&quot; data.dir.3 &lt;- &quot;L:\\\\EcoAP\\\\Data\\\\NAFO&quot; out.dir &lt;- &quot;L:\\\\EcoAP\\\\Data\\\\Commercial&quot; memory.limit(4000) channel &lt;- odbcDriverConnect() } if(Sys.info()[&#39;sysname&#39;]==&quot;Linux&quot;){ data.dir &lt;- &quot;/home/slucey/slucey/EcoAP/Data/Commercial&quot; data.dir.2 &lt;- &quot;/home/slucey/slucey/Rworkspace/RCom&quot; data.dir.3 &lt;- &quot;/home/slucey/slucey/EcoAP/Data/NAFO&quot; out.dir &lt;- &quot;/home/slucey/slucey/EcoAP/Data/Commercial&quot; uid &lt;- &#39;slucey&#39; cat(&quot;Oracle Password: &quot;) pwd &lt;- scan(stdin(), character(), n = 1) } landed &lt;- &#39;y&#39; #use landed weight for scallops and clams instead of live weight foreign &lt;- &#39;y&#39; #Mark foreign landings and keep seperate adjust.ppi &lt;- &#39;y&#39; #Adjust value for inflation use.existing &lt;- &#39;n&#39; #use raw data from a previous run - saves time sum.by &lt;- &#39;EPU&#39; #Variable to sum landings by [EPU, stat.area] #Final year of query endyear &lt;- 2016 #If adjusting for inflation refyear &lt;- 2016 refmonth &lt;- 1 #------------------------------------------------------------------------------- #Required packages library(RODBC); library(data.table); library(rgdal) #------------------------------------------------------------------------------- #User created functions #Convert NA&#39;s to zeros na.zero &lt;- function(x){ for(i in 1:length(x[1, ])){ if(length(which(is.na(x[, i]))) &gt; 0){ x[which(is.na(x[, i])), i] &lt;- 0} } return(x) } #------------------------------------------------------------------------------- #Connect to database if(Sys.info()[&#39;sysname&#39;] == &quot;Windows&quot;)channel &lt;- odbcDriverConnect() if(Sys.info()[&#39;sysname&#39;] == &quot;Linux&quot;) channel &lt;- odbcConnect(&#39;sole&#39;, uid, pwd) if(use.existing == &#39;n&#39;){ #Landings tables &lt;- c(paste0(&#39;WOLANDS&#39;, 64:81), paste0(&#39;WODETS&#39;, 82:93), paste0(&#39;CFDETS&#39;, 1994:endyear, &#39;AA&#39;)) #Generate one table comland &lt;- c() for(i in 1:length(tables)){ landings.qry &lt;- paste(&quot;select year, month, negear, toncl1, nespp3, nespp4, area, spplivlb, spplndlb, sppvalue, utilcd from&quot;, tables[i]) comland.yr &lt;- as.data.table(sqlQuery(channel, landings.qry)) setkey(comland.yr, YEAR, MONTH, NEGEAR, TONCL1, NESPP3, NESPP4, AREA, UTILCD) if(landed == &#39;y&#39;) comland.yr[NESPP3 %in% 743:800, SPPLIVLB := SPPLNDLB] #Sum landings and value #landings comland.yr[, V1 := sum(SPPLIVLB), by = key(comland.yr)] #value #Fix null values comland.yr[is.na(SPPVALUE), SPPVALUE := 0] comland.yr[, V2 := sum(SPPVALUE), by = key(comland.yr)] #Remove extra rows/columns comland.yr &lt;- unique(comland.yr, by = key(comland.yr)) comland.yr[, c(&#39;SPPLIVLB&#39;, &#39;SPPLNDLB&#39;, &#39;SPPVALUE&#39;) := NULL] #Rename summed columns setnames(comland.yr, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVLB&#39;, &#39;SPPVALUE&#39;)) comland &lt;- rbindlist(list(comland, comland.yr)) } if(landed == &#39;n&#39;) save(comland, file = file.path(out.dir, &quot;comland_raw_US.RData&quot;)) #Last run 8/31/16 if(landed == &#39;y&#39;) save(comland, file = file.path(out.dir, &quot;comland_raw_US_meatwt.RData&quot;)) #Last run 1/25/18 } if(use.existing == &#39;y&#39;){ if(landed == &#39;n&#39;) load(file = file.path(out.dir, &quot;comland_raw_US.RData&quot;)) if(landed == &#39;y&#39;) load(file = file.path(out.dir, &quot;comland_raw_US_meatwt.RData&quot;)) } #------------------------------------------------------------------------------- #Convert from lbs to metric tons comland[, SPPLIVMT := SPPLIVLB * 0.00045359237] comland[, SPPLIVLB := NULL] #fix years comland[YEAR &lt; 100, YEAR := YEAR + 1900L] if(adjust.ppi == &#39;y&#39;){ #Adjust SPPVALUE for inflation temp &lt;- tempfile() download.file(&quot;http://download.bls.gov/pub/time.series/wp/wp.data.3.ProcessedFoods&quot;, temp) inflate &lt;- as.data.table(read.delim(temp)) unlink(temp) inflate[, series_id := gsub(&quot; &quot;, &quot;&quot;, inflate[, series_id])] deflate &lt;- inflate[series_id == &quot;WPU0223&quot;, ] deflate[, MONTH := as.numeric(substr(period, 2, 3))] setnames(deflate, c(&#39;year&#39;, &#39;value&#39;), c(&#39;YEAR&#39;, &#39;PPI&#39;)) deflate &lt;- deflate[, list(YEAR, MONTH, PPI)] #Set yearly deflator to 0 instead of 13 to match unknown month designation deflate[MONTH == 13, MONTH := 0] deflate.base &lt;- deflate[YEAR == refyear &amp; MONTH == refmonth, PPI] comland &lt;- merge(comland, deflate, by = c(&#39;YEAR&#39;, &#39;MONTH&#39;), all.x = T) comland[, SPPVALUE := round((SPPVALUE * deflate.base) / PPI)] #Remove extra column comland[, PPI := NULL] } #Remove market categories of parts comland &lt;- comland[!NESPP4 %in% c(119, 123, 125, 127, 812, 819, 828, 829, 1731, 2351, 2690, 2699, 3472, as.numeric(paste(348:359, 8, sep = &#39;&#39;)), 3868, as.numeric(paste(469:471, 4, sep = &#39;&#39;)), as.numeric(paste(480:499, 8, sep =&#39;&#39;)), 5018, 5039, 5261, 5265), ] #Generate NESPP3 and MKTCAT in comland data comland[NESPP4 &lt; 100, MKTCAT := as.numeric(substring(NESPP4, 2, 2))] comland[NESPP4 &gt; 99 &amp; NESPP4 &lt; 1000, MKTCAT := as.numeric(substring(NESPP4, 3, 3))] comland[NESPP4 &gt; 999, MKTCAT := as.numeric(substring(NESPP4, 4, 4))] #drop NESPP4 comland[, NESPP4 := NULL] #Deal with Hakes and Skates------------------------------------------------------------------ source(file.path(data.dir.2, &#39;Comland_skates_hakes.R&#39;)) #get little skates and winter skates from skates(ns) - use survey in half years #Generate Half year variable in comland comland.skates &lt;- comland[NESPP3 == 365, ] comland.skates[MONTH %in% 1:6, Half := 1] comland.skates[MONTH %in% 7:12, Half := 2] setkey(skate.hake.us, YEAR, Half, AREA) comland.skates &lt;- merge(comland.skates, skate.hake.us, by = key(skate.hake.us), all.x = T) comland.skates[, little := little.per * SPPLIVMT] comland.skates[, little.value := round(little.per * SPPVALUE)] comland.skates[is.na(little), little := 0] comland.skates[is.na(little.value), little.value := 0] comland.skates[, winter := winter.per * SPPLIVMT] comland.skates[, winter.value := round(winter.per * SPPVALUE)] comland.skates[is.na(winter), winter := 0] comland.skates[is.na(winter.value), winter.value := 0] comland.skates[, other.skate := SPPLIVMT - (little + winter)] comland.skates[, other.skate.value := SPPVALUE - (little.value + winter.value)] #Little (366), winter (367), skates(ns) (365) #put skates in comland format to merge back little &lt;- comland.skates[, list(YEAR, Half, AREA, MONTH, NEGEAR, TONCL1, NESPP3, UTILCD, MKTCAT, little, little.value)] little[, NESPP3 := 366] setnames(little, c(&#39;little&#39;, &#39;little.value&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) little &lt;- little[SPPLIVMT &gt; 0, ] winter &lt;- comland.skates[, list(YEAR, Half, AREA, MONTH, NEGEAR, TONCL1, NESPP3, UTILCD, MKTCAT, winter, winter.value)] winter[, NESPP3 := 367] setnames(winter, c(&#39;winter&#39;, &#39;winter.value&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) winter &lt;- winter[SPPLIVMT &gt; 0, ] other &lt;- comland.skates[, list(YEAR, Half, AREA, MONTH, NEGEAR, TONCL1, NESPP3, UTILCD, MKTCAT, other.skate, other.skate.value)] other[, NESPP3 := 365] setnames(other, c(&#39;other.skate&#39;, &#39;other.skate.value&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) other &lt;- other[SPPLIVMT &gt; 0, ] #merge all three and reformat for comland skates.add.back &lt;- rbindlist(list(little, winter, other)) skates.add.back[, Half := NULL] setcolorder(skates.add.back, names(comland)) comland &lt;- rbindlist(list(comland[NESPP3 != 365, ], skates.add.back)) #get silver hake from mixed hakes - use survey in half years #Generate Half year variable in comland comland.hakes &lt;- comland[NESPP3 == 507, ] comland.hakes[MONTH %in% 1:6, Half := 1] comland.hakes[MONTH %in% 7:12, Half := 2] comland.hakes &lt;- merge(comland.hakes, skate.hake.us, by = key(skate.hake.us), all.x = T) comland.hakes[, silver := silver.per * SPPLIVMT] comland.hakes[, silver.value := round(silver.per * SPPVALUE)] comland.hakes[is.na(silver), silver := 0] comland.hakes[is.na(silver.value), silver.value := 0] comland.hakes[, off.hake := SPPLIVMT - silver] comland.hakes[, off.hake.value := SPPVALUE - silver.value] #Silver hake (509), mix hakes (507) #put hakes in comland format to merge back silver &lt;- comland.hakes[, list(YEAR, Half, AREA, MONTH, NEGEAR, TONCL1, NESPP3, UTILCD, MKTCAT, silver, silver.value)] silver[, NESPP3 := 509] setnames(silver, c(&#39;silver&#39;, &#39;silver.value&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) silver &lt;- silver[SPPLIVMT &gt; 0, ] offshore &lt;- comland.hakes[, list(YEAR, Half, AREA, MONTH, NEGEAR, TONCL1, NESPP3, UTILCD, MKTCAT, off.hake, off.hake.value)] offshore[, NESPP3 := 507] setnames(offshore, c(&#39;off.hake&#39;, &#39;off.hake.value&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) offshore &lt;- offshore[SPPLIVMT &gt; 0, ] #merge both and reformat for comland hakes.add.back &lt;- rbindlist(list(silver, offshore)) hakes.add.back[, Half := NULL] setcolorder(hakes.add.back, names(comland)) comland &lt;- rbindlist(list(comland[NESPP3 != 507, ], hakes.add.back)) #Herring--------------------------------------------------------------------------------- #Herring data is housed by the state of Maine. herr.qry &lt;- &quot;select year, month, stock_area, negear, gearname, keptmt, discmt from maine_herring_catch&quot; herr.catch &lt;- as.data.table(sqlQuery(channel, herr.qry)) setkey(herr.catch, YEAR, MONTH, STOCK_AREA, NEGEAR) herring &lt;- herr.catch[, list(sum(KEPTMT), sum(DISCMT)), by = key(herr.catch)] setnames(herring, c(&#39;STOCK_AREA&#39;, &#39;V1&#39;, &#39;V2&#39;), c(&#39;AREA&#39;, &#39;SPPLIVMT&#39;, &#39;DISCMT&#39;)) #Using averages from comland to fill in categories herring[, MKTCAT := 5] herring[, TONCL1 := 2] herring[, UTILCD := 0] #compute price/utilization from CF tables herring.comland &lt;- comland[NESPP3 == 168, ] #Price from comland herring.price &lt;- herring.comland[, (sum(SPPVALUE) / sum(SPPLIVMT)), by = c(&#39;YEAR&#39;, &#39;MONTH&#39;)] setnames(herring.price, &#39;V1&#39;, &#39;price&#39;) herring &lt;- merge(herring, herring.price, by = c(&#39;YEAR&#39;, &#39;MONTH&#39;), all.x = T) #Use 1964 prices for &lt; 1964 herring[YEAR &lt; 1964, price := mean(herring[YEAR == 1964, price])] #Calculate SPPVALUE from price herring[, SPPVALUE := round(price * SPPLIVMT)] #Utilization from comland herring.util &lt;- herring.comland[, sum(SPPLIVMT), by = c(&#39;YEAR&#39;, &#39;MONTH&#39;, &#39;UTILCD&#39;)] setnames(herring.util, &#39;V1&#39;, &#39;SPPLIVMT&#39;) herring.util[, SPPLIVMT.ALL := sum(SPPLIVMT), by = c(&#39;YEAR&#39;, &#39;MONTH&#39;)] herring.util[, Prop := SPPLIVMT/SPPLIVMT.ALL] setorder(herring.util, YEAR, MONTH, Prop) herring.util[, cum.prop := cumsum(Prop), by = c(&#39;YEAR&#39;, &#39;MONTH&#39;)] #Apply proportions to Maine data set #Not pulled all the time - current through 2017 herring[, Total := sum(SPPLIVMT), by = c(&#39;YEAR&#39;, &#39;MONTH&#39;)] herring[, Prop := SPPLIVMT / Total] setorder(herring, YEAR, MONTH, Prop) herring[, cum.prop := cumsum(Prop), by = c(&#39;YEAR&#39;, &#39;MONTH&#39;)] for(iyear in unique(herring.util[, YEAR])){ for(imonth in unique(herring.util[YEAR == iyear, MONTH])){ cum.prop.low &lt;- 0 for(iutil in herring.util[YEAR == iyear &amp; MONTH == imonth, UTILCD]){ cum.prop.high &lt;- herring.util[YEAR == iyear &amp; MONTH == imonth &amp; UTILCD == iutil, cum.prop] herring[YEAR == iyear &amp; MONTH == imonth &amp; cum.prop &lt;= cum.prop.high &amp; cum.prop &gt; cum.prop.low, UTILCD := iutil] cum.prop.low &lt;- cum.prop.high } } } #fix column headings herring[, c(&#39;Total&#39;, &#39;Prop&#39;, &#39;cum.prop&#39;, &#39;price&#39;, &#39;DISCMT&#39;) := NULL] herring[, NESPP3 := 168] setcolorder(herring, names(comland)) #remove herring from data pull and add in Maine numbers comland &lt;- rbindlist(list(comland[NESPP3 != 168, ], herring)) #Menhaden------------------------------------------------------------------------------------ ##fix menhaden records - data from Tom Miller/ Andre Bouchheister #menhaden &lt;- as.data.table(read.csv(paste(data.dir, &quot;Menhaden.csv&quot;, sep = &#39;&#39;))) #menhaden.mab &lt;- menhaden[, MA.Total + CB.Total, by = Year] ##file metric is 1000s of lbs - convert to mt #menhaden.mab[, SPPLIVMT := (V1 * 1000) * 0.00045359237] #menhaden.mab[, V1 := NULL] # #menhaden.gom &lt;- menhaden[, list(Year, NE.Total)] #menhaden.gom[, SPPLIVMT := (NE.Total * 1000) * 0.00045359237] #menhaden.gom[, NE.Total := NULL] #save(comland, file = paste(out.dir, &quot;Comland_unkA.RData&quot;, sep = &#39;&#39;)) #Deal with unknowns------------------------------------------------------------------------- comland[NEGEAR == 999, NEGEAR := 0] comland[is.na(TONCL1), TONCL1 := 0] comland[is.na(AREA), AREA := as.factor(0)] comland[AREA == 999, AREA := as.factor(0)] comland[is.na(MKTCAT), MKTCAT := 0] comland[is.na(UTILCD), UTILCD := 0] #1 - drop unknown species/landings comland &lt;- comland[NESPP3 != 0 &amp; SPPLIVMT != 0, ] #Sumarry tables #missing area #known.area &lt;- comland[AREA != 0, sum(SPPLIVMT), by = NESPP3] #unknown.area &lt;- comland[AREA == 0, sum(SPPLIVMT), by = NESPP3] #setnames(known.area, &quot;V1&quot;, &quot;AREA.MT.known&quot;) #setnames(unknown.area, &quot;V1&quot;, &quot;AREA.MT.unknown&quot;) #missing.table &lt;- merge(known.area, unknown.area, by = &#39;NESPP3&#39;, all = T) # #missing.table[is.na(AREA.MT.known), AREA.MT.known := 0] #missing.table[is.na(AREA.MT.unknown), AREA.MT.unknown := 0] #missing.table[, AREA.Ratio := AREA.MT.unknown / AREA.MT.known] # ##missing month #known.month &lt;- comland[MONTH != 0, sum(SPPLIVMT), by = NESPP3] #unknown.month &lt;- comland[MONTH == 0, sum(SPPLIVMT), by = NESPP3] #setnames(known.month, &quot;V1&quot;, &quot;MONTH.MT.known&quot;) #setnames(unknown.month, &quot;V1&quot;, &quot;MONTH.MT.unknown&quot;) #missing.table &lt;- merge(missing.table, known.month, by = &#39;NESPP3&#39;, all = T) #missing.table &lt;- merge(missing.table, unknown.month, by = &#39;NESPP3&#39;, all = T) # #missing.table[is.na(MONTH.MT.known), MONTH.MT.known := 0] #missing.table[is.na(MONTH.MT.unknown), MONTH.MT.unknown := 0] #missing.table[, MONTH.Ratio := MONTH.MT.unknown / MONTH.MT.known] # ##missing gear #known.gear &lt;- comland[NEGEAR != 0, sum(SPPLIVMT), by = NESPP3] #unknown.gear &lt;- comland[NEGEAR == 0, sum(SPPLIVMT), by = NESPP3] #setnames(known.gear, &quot;V1&quot;, &quot;GEAR.MT.known&quot;) #setnames(unknown.gear, &quot;V1&quot;, &quot;GEAR.MT.unknown&quot;) #missing.table &lt;- merge(missing.table, known.gear, by = &#39;NESPP3&#39;, all = T) #missing.table &lt;- merge(missing.table, unknown.gear, by = &#39;NESPP3&#39;, all = T) # #missing.table[is.na(GEAR.MT.known), GEAR.MT.known := 0] #missing.table[is.na(GEAR.MT.unknown), GEAR.MT.unknown := 0] #missing.table[, GEAR.Ratio := GEAR.MT.unknown / GEAR.MT.known] # ##missing tonnage class #known.tc &lt;- comland[TONCL1 != 0, sum(SPPLIVMT), by = NESPP3] #unknown.tc &lt;- comland[TONCL1 == 0, sum(SPPLIVMT), by = NESPP3] #setnames(known.tc, &quot;V1&quot;, &quot;TC.MT.known&quot;) #setnames(unknown.tc, &quot;V1&quot;, &quot;TC.MT.unknown&quot;) #missing.table &lt;- merge(missing.table, known.tc, by = &#39;NESPP3&#39;, all = T) #missing.table &lt;- merge(missing.table, unknown.tc, by = &#39;NESPP3&#39;, all = T) # #missing.table[is.na(TC.MT.known), TC.MT.known := 0] #missing.table[is.na(TC.MT.unknown), TC.MT.unknown := 0] #missing.table[, TC.Ratio := TC.MT.unknown / TC.MT.known] # #write.csv(missing.table, paste(out.dir, &quot;\\\\Missing_table.csv&quot;, sep = &#39;&#39;), row.names = F) # #2 - aggregate by quarter year, half year, major gear, and small/large TC comland[MONTH %in% 1:3, QY := 1] comland[MONTH %in% 4:6, QY := 2] comland[MONTH %in% 7:9, QY := 3] comland[MONTH %in% 10:12, QY := 4] comland[MONTH == 0, QY := 0] comland[MONTH %in% 1:6, HY := 1] comland[MONTH %in% 7:12, HY := 2] comland[MONTH == 0, HY := 0] otter &lt;- 50:59 dredge.sc &lt;- 131:132 pot &lt;- c(189:190, 200:219, 300, 301) longline &lt;- c(10, 40) seine &lt;- c(70:79, 120:129, 360) gillnet &lt;- c(100:119, 500, 510, 520) midwater &lt;- c(170, 370) dredge.o &lt;- c(281, 282, 380:400) comland[NEGEAR %in% otter, GEAR := &#39;otter&#39;] comland[NEGEAR %in% dredge.sc, GEAR := &#39;dredge.sc&#39;] comland[NEGEAR %in% pot, GEAR := &#39;pot&#39;] comland[NEGEAR %in% longline, GEAR := &#39;longline&#39;] comland[NEGEAR %in% seine, GEAR := &#39;seine&#39;] comland[NEGEAR %in% gillnet, GEAR := &#39;gillnet&#39;] comland[NEGEAR %in% midwater, GEAR := &#39;midwater&#39;] comland[NEGEAR %in% dredge.o, GEAR := &#39;dredge.o&#39;] comland[NEGEAR == 0, GEAR := &#39;unknown&#39;] comland[is.na(GEAR), GEAR := &#39;other&#39;] comland[, GEAR := as.factor(GEAR)] comland[TONCL1 %in% 1:3, SIZE := &#39;small&#39;] comland[TONCL1 &gt; 3, SIZE := &#39;large&#39;] comland[TONCL1 == 0, SIZE := &#39;unknown&#39;] comland[, SIZE := as.factor(SIZE)] setkey(comland, YEAR, QY, HY, GEAR, SIZE, AREA, NESPP3, UTILCD) comland.agg &lt;- comland[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland)] setnames(comland.agg, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #3 - Use proportions of known catch to assign unknown catch #3.A QY/HY------------------------------------------------------------------------------ unk.month &lt;- comland.agg[QY == 0, ] k.month &lt;- comland.agg[QY != 0, ] #3.A.1 - All match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;AREA&#39;) unk.month.all &lt;- unk.month[GEAR != &#39;unknown&#39;] unk.month.all &lt;- unk.month.all[SIZE != &#39;unknown&#39;, ] unk.month.all &lt;- unk.month.all[AREA != 0, ] k.month.all &lt;- k.month[GEAR != &#39;unknown&#39;, ] k.month.all &lt;- k.month.all[SIZE != &#39;unknown&#39;, ] k.month.all &lt;- k.month.all[AREA != 0, ] setkeyv(unk.month.all, match.key) setkeyv(k.month.all, match.key) month.all &lt;- k.month.all[unk.month.all] #No match - need to match with larger aggregation no.match &lt;- month.all[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match, YEAR, NESPP3, AREA, GEAR) setkeyv(k.month.all, key(no.match)) month.all.2 &lt;- k.month.all[no.match] no.match.2 &lt;- month.all.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match.2, YEAR, NESPP3, AREA) setkeyv(k.month.all, key(no.match.2)) month.all.3 &lt;- k.month.all[no.match.2] no.match.3 &lt;- month.all.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match.3, YEAR, NESPP3) setkeyv(k.month.all, key(no.match.3)) month.all.4 &lt;- k.month.all[no.match.3] no.match.4 &lt;- month.all.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.AREA&#39;, &#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.4[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] #Merge all together and proportion catch to known months month.all &lt;- month.all [!is.na(SPPLIVMT), ] month.all.2 &lt;- month.all.2[!is.na(SPPLIVMT), ] month.all.2[, SIZE := i.SIZE] month.all.2[, i.SIZE := NULL] setcolorder(month.all.2, names(month.all)) month.all.3 &lt;- month.all.3[!is.na(SPPLIVMT), ] month.all.3[, GEAR := i.GEAR] month.all.3[, SIZE := i.SIZE] month.all.3[, i.GEAR := NULL] month.all.3[, i.SIZE := NULL] setcolorder(month.all.3, names(month.all)) month.all.4 &lt;- month.all.4[!is.na(SPPLIVMT), ] month.all.4[, AREA := i.AREA] month.all.4[, GEAR := i.GEAR] month.all.4[, SIZE := i.SIZE] month.all.4[, i.AREA := NULL] month.all.4[, i.GEAR := NULL] month.all.4[, i.SIZE := NULL] setcolorder(month.all.4, names(month.all)) month.all &lt;- rbindlist(list(month.all, month.all.2, month.all.3, month.all.4)) month.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.all[, unk := i.SPPLIVMT * prop] month.all[, unk2 := i.SPPVALUE * prop] month.all[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.all, c(&#39;unk&#39;, &#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setcolorder(no.match.4, names(month.all)) month.solved &lt;- rbindlist(list(month.all, no.match.4)) rm(list = c(ls(pattern = &#39;month.all&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.2 - GEAR/SIZE match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;) unk.month.g.s &lt;- unk.month[GEAR != &#39;unknown&#39;] unk.month.g.s &lt;- unk.month.g.s[SIZE != &#39;unknown&#39;, ] unk.month.g.s &lt;- unk.month.g.s[AREA == 0, ] unk.month.g.s &lt;- unk.month.g.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.g.s, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.g.s &lt;- k.month[GEAR != &#39;unknown&#39;, ] k.month.g.s &lt;- k.month.g.s[SIZE != &#39;unknown&#39;, ] k.month.g.s &lt;- k.month.g.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.g.s, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.g.s, match.key) setkeyv(k.month.g.s, match.key) month.g.s &lt;- k.month.g.s[unk.month.g.s] #No match - need to match with larger aggregation no.match &lt;- month.g.s[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match, YEAR, NESPP3, GEAR) setkeyv(k.month.g.s, key(no.match)) month.g.s.2 &lt;- k.month.g.s[no.match] no.match.2 &lt;- month.g.s.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match.2, YEAR, NESPP3) setkeyv(k.month.g.s, key(no.match.2)) month.g.s.3 &lt;- k.month.g.s[no.match.2] no.match.3 &lt;- month.g.s.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.3[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match.3[, AREA := 0] #Merge all together and proportion catch to known months month.g.s &lt;- month.g.s [!is.na(SPPLIVMT), ] month.g.s.2 &lt;- month.g.s.2[!is.na(SPPLIVMT), ] if(nrow(month.g.s.2) &gt; 0){ month.g.s.2[, SIZE := i.SIZE] month.g.s.2[, i.SIZE := NULL] setcolorder(month.g.s.2, names(month.g.s)) month.g.s &lt;- rbindlist(list(month.g.s, month.g.s.2)) } month.g.s.3 &lt;- month.g.s.3[!is.na(SPPLIVMT), ] if(nrow(month.g.s.3) &gt; 0){ month.g.s.3[, GEAR := i.GEAR] month.g.s.3[, SIZE := i.SIZE] month.g.s.3[, i.GEAR := NULL] month.g.s.3[, i.SIZE := NULL] setcolorder(month.g.s.3, names(month.g.s)) month.g.s &lt;- rbindlist(list(month.g.s, month.g.s.3)) } month.g.s[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.g.s[, unk := i.SPPLIVMT * prop] month.g.s[, unk2 := i.SPPVALUE * prop] month.g.s[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.g.s, c(&#39;unk&#39;, &#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.g.s[, AREA := 0] setcolorder(month.g.s, names(month.solved)) setcolorder(no.match.3, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.g.s, no.match.3)) rm(list = c(ls(pattern = &#39;month.g.s&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.3 - AREA/GEAR match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;GEAR&#39;, &#39;AREA&#39;) unk.month.a.g &lt;- unk.month[GEAR != &#39;unknown&#39;] unk.month.a.g &lt;- unk.month.a.g[SIZE == &#39;unknown&#39;, ] unk.month.a.g &lt;- unk.month.a.g[AREA != 0, ] unk.month.a.g &lt;- unk.month.a.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.a.g, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.a.g &lt;- k.month[GEAR != &#39;unknown&#39;, ] k.month.a.g &lt;- k.month.a.g[AREA != 0, ] k.month.a.g &lt;- k.month.a.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.a.g, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.a.g, match.key) setkeyv(k.month.a.g, match.key) month.a.g &lt;- k.month.a.g[unk.month.a.g] #No match - need to match with larger aggregation no.match &lt;- month.a.g[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match, YEAR, NESPP3, AREA) setkeyv(k.month.a.g, key(no.match)) month.a.g.2 &lt;- k.month.a.g[no.match] no.match.2 &lt;- month.a.g.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match.2, YEAR, NESPP3) setkeyv(k.month.a.g, key(no.match.2)) month.a.g.3 &lt;- k.month.a.g[no.match.2] no.match.3 &lt;- month.a.g.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.AREA&#39;, &#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.3[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match.3[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] #Merge all together and proportion catch to known months month.a.g &lt;- month.a.g [!is.na(SPPLIVMT), ] month.a.g.2 &lt;- month.a.g.2[!is.na(SPPLIVMT), ] if(nrow(month.a.g.2) &gt; 0){ month.a.g.2[, GEAR := i.GEAR] month.a.g.2[, i.GEAR := NULL] setcolorder(month.a.g.2, names(month.a.g)) month.a.g &lt;- rbindlist(list(month.a.g, month.a.g.2)) } month.a.g.3 &lt;- month.a.g.3[!is.na(SPPLIVMT), ] if(nrow(month.a.g.3) &gt; 0){ month.a.g.3[, AREA := i.AREA] month.a.g.3[, GEAR := i.GEAR] month.a.g.3[, i.AREA := NULL] month.a.g.3[, i.GEAR := NULL] setcolorder(month.a.g.3, names(month.a.g)) month.a.g &lt;- rbindlist(list(month.a.g, month.a.g.3)) } month.a.g[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.a.g[, unk := i.SPPLIVMT * prop] month.a.g[, unk2 := i.SPPVALUE * prop] month.a.g[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.a.g, c(&#39;unk&#39;, &#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.a.g[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] setcolorder(month.a.g, names(month.solved)) setcolorder(no.match.3, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.a.g, no.match.3)) rm(list = c(ls(pattern = &#39;month.a.g&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.4 - AREA/TC match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;SIZE&#39;, &#39;AREA&#39;) unk.month.a.s &lt;- unk.month[GEAR == &#39;unknown&#39;] unk.month.a.s &lt;- unk.month.a.s[SIZE != &#39;unknown&#39;, ] unk.month.a.s &lt;- unk.month.a.s[AREA != 0, ] unk.month.a.s &lt;- unk.month.a.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.a.s, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.a.s &lt;- k.month[SIZE != &#39;unknown&#39;, ] k.month.a.s &lt;- k.month.a.s[AREA != 0, ] k.month.a.s &lt;- k.month.a.s[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.a.s, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.a.s, match.key) setkeyv(k.month.a.s, match.key) month.a.s &lt;- k.month.a.s[unk.month.a.s] #No match - need to match with larger aggregation no.match &lt;- month.a.s[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match, YEAR, NESPP3, AREA) setkeyv(k.month.a.s, key(no.match)) month.a.s.2 &lt;- k.month.a.s[no.match] no.match.2 &lt;- month.a.s.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match.2, YEAR, NESPP3) setkeyv(k.month.a.s, key(no.match.2)) month.a.s.3 &lt;- k.month.a.s[no.match.2] no.match.3 &lt;- month.a.s.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;AREA&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.AREA&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.3[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match.3[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] #Merge all together and proportion catch to known months month.a.s &lt;- month.a.s [!is.na(SPPLIVMT), ] month.a.s.2 &lt;- month.a.s.2[!is.na(SPPLIVMT), ] if(nrow(month.a.s.2) &gt; 0){ month.a.s.2[, SIZE := i.SIZE] month.a.s.2[, i.SIZE := NULL] setcolorder(month.a.s.2, names(month.a.s)) month.a.s &lt;- rbindlist(list(month.a.s, month.a.s.2)) } month.a.s.3 &lt;- month.a.s.3[!is.na(SPPLIVMT), ] if(nrow(month.a.s.3) &gt; 0){ month.a.s.3[, AREA := i.AREA] month.a.s.3[, SIZE := i.SIZE] month.a.s.3[, i.AREA := NULL] month.a.s.3[, i.SIZE := NULL] setcolorder(month.a.s.3, names(month.a.s)) month.a.s &lt;- rbindlist(list(month.a.s, month.a.s.3)) } month.a.s[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.a.s[, unk := i.SPPLIVMT * prop] month.a.s[, unk2 := i.SPPVALUE * prop] month.a.s[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.a.s, c(&#39;unk&#39;, &#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.a.s[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] setcolorder(month.a.s, names(month.solved)) setcolorder(no.match.3, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.a.s, no.match.3)) rm(list = c(ls(pattern = &#39;month.a.s&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.5 - SIZE match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;SIZE&#39;) unk.month.si &lt;- unk.month[GEAR == &#39;unknown&#39;] unk.month.si &lt;- unk.month.si[SIZE != &#39;unknown&#39;, ] unk.month.si &lt;- unk.month.si[AREA == 0, ] unk.month.si &lt;- unk.month.si[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.si, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.si &lt;- k.month[SIZE != &#39;unknown&#39;, ] k.month.si &lt;- k.month.si[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.si, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.si, match.key) setkeyv(k.month.si, match.key) month.si &lt;- k.month.si[unk.month.si] #No match - need to match with larger aggregation no.match &lt;- month.si[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match, YEAR, NESPP3) setkeyv(k.month.si, key(no.match)) month.si.2 &lt;- k.month.si[no.match] no.match.2 &lt;- month.si.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.2[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match.2[, AREA := 0] no.match.2[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] #Merge all together and proportion catch to known months month.si &lt;- month.si [!is.na(SPPLIVMT), ] month.si.2 &lt;- month.si.2[!is.na(SPPLIVMT), ] if(nrow(month.si.2) &gt; 0){ month.si.2[, SIZE := i.SIZE] month.si.2[, i.SIZE := NULL] setcolorder(month.si.2, names(month.si)) month.si &lt;- rbindlist(list(month.si, month.si.2)) } month.si[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.si[, unk := i.SPPLIVMT * prop] month.si[, unk2 := i.SPPVALUE * prop] month.si[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.si, c(&#39;unk&#39;, &#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.si[, AREA := 0] month.si[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] setcolorder(month.si, names(month.solved)) setcolorder(no.match.2, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.si, no.match.2)) rm(list = c(ls(pattern = &#39;month.si&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.6 - GEAR match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;GEAR&#39;) unk.month.g &lt;- unk.month[GEAR != &#39;unknown&#39;] unk.month.g &lt;- unk.month.g[SIZE == &#39;unknown&#39;, ] unk.month.g &lt;- unk.month.g[AREA == 0, ] unk.month.g &lt;- unk.month.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.g, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.g &lt;- k.month[GEAR != &#39;unknown&#39;, ] k.month.g &lt;- k.month.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.g, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.g, match.key) setkeyv(k.month.g, match.key) month.g &lt;- k.month.g[unk.month.g] #No match - need to match with larger aggregation no.match &lt;- month.g[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match, YEAR, NESPP3) setkeyv(k.month.g, key(no.match)) month.g.2 &lt;- k.month.g[no.match] no.match.2 &lt;- month.g.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.2[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match.2[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] no.match.2[, AREA := 0] #Merge all together and proportion catch to known months month.g &lt;- month.g [!is.na(SPPLIVMT), ] month.g.2 &lt;- month.g.2[!is.na(SPPLIVMT), ] if(nrow(month.g.2) &gt; 0){ month.g.2[, GEAR := i.GEAR] month.g.2[, i.GEAR := NULL] setcolorder(month.g.2, names(month.g)) month.g &lt;- rbindlist(list(month.g, month.g.2)) } month.g[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.g[, unk := i.SPPLIVMT * prop] month.g[, unk2 := i.SPPVALUE * prop] month.g[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.g, c(&#39;unk&#39;, &#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.g[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] month.g[, AREA := 0] setcolorder(month.g, names(month.solved)) setcolorder(no.match.2, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.g, no.match.2)) rm(list = c(ls(pattern = &#39;month.g&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.7 - AREA match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;AREA&#39;) unk.month.a &lt;- unk.month[GEAR == &#39;unknown&#39;] unk.month.a &lt;- unk.month.a[SIZE == &#39;unknown&#39;, ] unk.month.a &lt;- unk.month.a[AREA != 0, ] unk.month.a &lt;- unk.month.a[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.a, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.a &lt;- k.month[AREA != 0, ] k.month.a &lt;- k.month.a[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.a, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.a, match.key) setkeyv(k.month.a, match.key) month.a &lt;- k.month.a[unk.month.a] #No match - need to match with larger aggregation no.match &lt;- month.a[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match, YEAR, NESPP3) setkeyv(k.month.a, key(no.match)) month.a.2 &lt;- k.month.a[no.match] no.match.2 &lt;- month.a.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;AREA&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.AREA&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to first QY/HY no.match.2[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match.2[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] no.match.2[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] #Merge all together and proportion catch to known months month.a &lt;- month.a [!is.na(SPPLIVMT), ] month.a.2 &lt;- month.a.2[!is.na(SPPLIVMT), ] if(nrow(month.a.2) &gt; 0){ month.a.2[, AREA := i.AREA] month.a.2[, i.AREA := NULL] setcolorder(month.a.2, names(month.a)) month.a &lt;- rbindlist(list(month.a, month.a.2)) } month.a[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.a[, unk := i.SPPLIVMT * prop] month.a[, unk2 := i.SPPVALUE * prop] month.a[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.a, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.a[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] month.a[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] setcolorder(month.a, names(month.solved)) setcolorder(no.match.2, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.a, no.match.2)) rm(list = c(ls(pattern = &#39;month.a&#39;), ls(pattern = &#39;no.match&#39;))) #3.A.8 - Species only - no other match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;) unk.month.sp &lt;- unk.month[GEAR == &#39;unknown&#39;] unk.month.sp &lt;- unk.month.sp[SIZE == &#39;unknown&#39;, ] unk.month.sp &lt;- unk.month.sp[AREA == 0, ] unk.month.sp &lt;- unk.month.sp[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(unk.month.sp, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) k.month.sp &lt;- k.month[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;)] setnames(k.month.sp, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.month.sp, match.key) setkeyv(k.month.sp, match.key) month.sp &lt;- k.month.sp[unk.month.sp] #No match - assign to first QY/HY no.match &lt;- month.sp[is.na(SPPLIVMT), ] no.match[, c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) no.match[, c(&#39;QY&#39;, &#39;HY&#39;) := 1] no.match[, AREA := 0] no.match[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] no.match[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] #proportion catch to known months month.sp &lt;- month.sp [!is.na(SPPLIVMT), ] month.sp[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] month.sp[, unk := i.SPPLIVMT * prop] month.sp[, unk2 := i.SPPVALUE * prop] month.sp[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.HY&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(month.sp, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) month.sp[, AREA := 0] month.sp[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.month[, GEAR]))] month.sp[, SIZE := factor(&#39;unknown&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] setcolorder(month.sp, names(month.solved)) setcolorder(no.match, names(month.solved)) month.solved &lt;- rbindlist(list(month.solved, month.sp, no.match)) rm(list = c(ls(pattern = &#39;month.sp&#39;), ls(pattern = &#39;no.match&#39;))) #Merge back month.solved setcolorder(month.solved, names(comland.agg)) comland.agg &lt;- rbindlist(list(k.month, month.solved)) setkey(comland.agg, YEAR, QY, HY, SIZE, GEAR, AREA, NESPP3, UTILCD) comland.agg &lt;- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland.agg)] setnames(comland.agg, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #3.B SIZE------------------------------------------------------------------------------ unk.size &lt;- comland.agg[SIZE == &#39;unknown&#39;, ] k.size &lt;- comland.agg[SIZE != &#39;unknown&#39;, ] #3.B.1 - All match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;GEAR&#39;, &#39;AREA&#39;) unk.size.all &lt;- unk.size[GEAR != &#39;unknown&#39;] unk.size.all &lt;- unk.size.all[AREA != 0, ] k.size.all &lt;- k.size[GEAR != &#39;unknown&#39;, ] k.size.all &lt;- k.size.all[AREA != 0, ] setkeyv(unk.size.all, match.key) setkeyv(k.size.all, match.key) size.all &lt;- k.size.all[unk.size.all] #No match - need to match with larger aggregation no.match &lt;- size.all[is.na(SPPLIVMT), ] no.match[, c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY, GEAR, AREA) setkeyv(k.size.all, key(no.match)) size.all.2 &lt;- k.size.all[no.match] no.match.2 &lt;- size.all.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3, GEAR, AREA) setkeyv(k.size.all, key(no.match.2)) size.all.3 &lt;- k.size.all[no.match.2] no.match.3 &lt;- size.all.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match.3, YEAR, NESPP3, AREA) setkeyv(k.size.all, key(no.match.3)) size.all.4 &lt;- k.size.all[no.match.3] no.match.4 &lt;- size.all.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match.4, YEAR, NESPP3) setkeyv(k.size.all, key(no.match.4)) size.all.5 &lt;- k.size.all[no.match.4] no.match.5 &lt;- size.all.5[is.na(SPPLIVMT), ] no.match.5[, c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.5, c(&#39;i.AREA&#39;, &#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to SIZE to small no.match.5[, SIZE := factor(&#39;small&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] #Merge all together and proportion catch to known sizes size.all &lt;- size.all [!is.na(SPPLIVMT), ] size.all.2 &lt;- size.all.2[!is.na(SPPLIVMT), ] size.all.2[, QY := i.QY] size.all.2[, i.QY := NULL] setcolorder(size.all.2, names(size.all)) size.all.3 &lt;- size.all.3[!is.na(SPPLIVMT), ] size.all.3[, QY := i.QY] size.all.3[, HY := i.HY] size.all.3[, i.QY := NULL] size.all.3[, i.HY := NULL] setcolorder(size.all.3, names(size.all)) size.all.4 &lt;- size.all.4[!is.na(SPPLIVMT), ] size.all.4[, QY := i.QY] size.all.4[, HY := i.HY] size.all.4[, GEAR := i.GEAR] size.all.4[, i.QY := NULL] size.all.4[, i.HY := NULL] size.all.4[, i.GEAR := NULL] setcolorder(size.all.4, names(size.all)) size.all.5 &lt;- size.all.5[!is.na(SPPLIVMT), ] size.all.5[, QY := i.QY] size.all.5[, HY := i.HY] size.all.5[, GEAR := i.GEAR] size.all.5[, AREA := i.AREA] size.all.5[, i.QY := NULL] size.all.5[, i.HY := NULL] size.all.5[, i.GEAR := NULL] size.all.5[, i.AREA := NULL] setcolorder(size.all.5, names(size.all)) size.all &lt;- rbindlist(list(size.all, size.all.2, size.all.3, size.all.4, size.all.5)) size.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] size.all[, unk := i.SPPLIVMT * prop] size.all[, unk2 := i.SPPVALUE * prop] size.all[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(size.all, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setcolorder(no.match.5, names(size.all)) size.solved &lt;- rbindlist(list(size.all, no.match.5)) rm(list = c(ls(pattern = &#39;size.all&#39;), ls(pattern = &#39;no.match&#39;))) #3.B.2 - GEAR match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;GEAR&#39;) unk.size.g &lt;- unk.size[GEAR != &#39;unknown&#39;] unk.size.g &lt;- unk.size.g[AREA == 0, ] unk.size.g[, AREA := NULL] k.size.g &lt;- k.size[GEAR != &#39;unknown&#39;, ] k.size.g &lt;- k.size.g[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;SIZE&#39;, &#39;UTILCD&#39;)] setnames(k.size.g, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.size.g, match.key) setkeyv(k.size.g, match.key) size.g &lt;- k.size.g[unk.size.g] #No match - need to match with larger aggregation no.match &lt;- size.g[is.na(SPPLIVMT), ] no.match[, c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY, GEAR) setkeyv(k.size.g, key(no.match)) size.g.2 &lt;- k.size.g[no.match] no.match.2 &lt;- size.g.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3, GEAR) setkeyv(k.size.g, key(no.match.2)) size.g.3 &lt;- k.size.g[no.match.2] no.match.3 &lt;- size.g.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match.3, YEAR, NESPP3) setkeyv(k.size.g, key(no.match.3)) size.g.4 &lt;- k.size.g[no.match.3] no.match.4 &lt;- size.g.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to SIZE to small no.match.4[, SIZE := factor(&#39;small&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] no.match.4[, AREA := 0] #Merge all together and proportion catch to known sizes size.g &lt;- size.g [!is.na(SPPLIVMT), ] size.g.2 &lt;- size.g.2[!is.na(SPPLIVMT), ] size.g.2[, QY := i.QY] size.g.2[, i.QY := NULL] setcolorder(size.g.2, names(size.g)) size.g.3 &lt;- size.g.3[!is.na(SPPLIVMT), ] size.g.3[, QY := i.QY] size.g.3[, HY := i.HY] size.g.3[, i.QY := NULL] size.g.3[, i.HY := NULL] setcolorder(size.g.3, names(size.g)) size.g.4 &lt;- size.g.4[!is.na(SPPLIVMT), ] size.g.4[, QY := i.QY] size.g.4[, HY := i.HY] size.g.4[, GEAR := i.GEAR] size.g.4[, i.QY := NULL] size.g.4[, i.HY := NULL] size.g.4[, i.GEAR := NULL] setcolorder(size.g.4, names(size.g)) size.g &lt;- rbindlist(list(size.g, size.g.2, size.g.3, size.g.4)) size.g[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] size.g[, unk := i.SPPLIVMT * prop] size.g[, unk2 := i.SPPVALUE * prop] size.g[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(size.g, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) size.g[, AREA := 0] setcolorder(size.g, names(size.solved)) setcolorder(no.match.4, names(size.g)) size.solved &lt;- rbindlist(list(size.solved, size.g, no.match.4)) rm(list = c(ls(pattern = &#39;size.g&#39;), ls(pattern = &#39;no.match&#39;))) #3.B.3 - AREA match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;AREA&#39;) unk.size.a &lt;- unk.size[GEAR == &#39;unknown&#39;] unk.size.a &lt;- unk.size.a[AREA != 0, ] unk.size.a[, GEAR := NULL] k.size.a &lt;- k.size[AREA != 0, ] k.size.a &lt;- k.size.a[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;SIZE&#39;, &#39;UTILCD&#39;)] setnames(k.size.a, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.size.a, match.key) setkeyv(k.size.a, match.key) size.a &lt;- k.size.a[unk.size.a] #No match - need to match with larger aggregation no.match &lt;- size.a[is.na(SPPLIVMT), ] no.match[, c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY, AREA) setkeyv(k.size.a, key(no.match)) size.a.2 &lt;- k.size.a[no.match] no.match.2 &lt;- size.a.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3, AREA) setkeyv(k.size.a, key(no.match.2)) size.a.3 &lt;- k.size.a[no.match.2] no.match.3 &lt;- size.a.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match.3, YEAR, NESPP3) setkeyv(k.size.a, key(no.match.3)) size.a.4 &lt;- k.size.a[no.match.3] no.match.4 &lt;- size.a.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;AREA&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.AREA&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to SIZE to small no.match.4[, SIZE := factor(&#39;small&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] no.match.4[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.size[, GEAR]))] #Merge all together and proportion catch to known sizes size.a &lt;- size.a [!is.na(SPPLIVMT), ] size.a.2 &lt;- size.a.2[!is.na(SPPLIVMT), ] size.a.2[, QY := i.QY] size.a.2[, i.QY := NULL] setcolorder(size.a.2, names(size.a)) size.a.3 &lt;- size.a.3[!is.na(SPPLIVMT), ] size.a.3[, QY := i.QY] size.a.3[, HY := i.HY] size.a.3[, i.QY := NULL] size.a.3[, i.HY := NULL] setcolorder(size.a.3, names(size.a)) size.a.4 &lt;- size.a.4[!is.na(SPPLIVMT), ] size.a.4[, QY := i.QY] size.a.4[, HY := i.HY] size.a.4[, AREA := i.AREA] size.a.4[, i.QY := NULL] size.a.4[, i.HY := NULL] size.a.4[, i.AREA := NULL] setcolorder(size.a.4, names(size.a)) size.a &lt;- rbindlist(list(size.a, size.a.2, size.a.3, size.a.4)) size.a[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] size.a[, unk := i.SPPLIVMT * prop] size.a[, unk2 := i.SPPVALUE * prop] size.a[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(size.a, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) size.a[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.size[, GEAR]))] setcolorder(size.a, names(size.solved)) setcolorder(no.match.4, names(size.a)) size.solved &lt;- rbindlist(list(size.solved, size.a, no.match.4)) rm(list = c(ls(pattern = &#39;size.a&#39;), ls(pattern = &#39;no.match&#39;))) #3.B.4 - Species only - no other match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;) unk.size.sp &lt;- unk.size[GEAR == &#39;unknown&#39;] unk.size.sp &lt;- unk.size.sp[SIZE == &#39;unknown&#39;, ] unk.size.sp &lt;- unk.size.sp[AREA == 0, ] unk.size.sp[, c(&#39;GEAR&#39;, &#39;AREA&#39;) := NULL] k.size.sp &lt;- k.size[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;SIZE&#39;, &#39;UTILCD&#39;)] setnames(k.size.sp, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.size.sp, match.key) setkeyv(k.size.sp, match.key) size.sp &lt;- k.size.sp[unk.size.sp] #No match - need to match with larger aggregation no.match &lt;- size.sp[is.na(SPPLIVMT), ] no.match[, c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY) setkeyv(k.size.sp, key(no.match)) size.sp.2 &lt;- k.size.sp[no.match] no.match.2 &lt;- size.sp.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3) setkeyv(k.size.sp, key(no.match.2)) size.sp.3 &lt;- k.size.sp[no.match.2] no.match.3 &lt;- size.sp.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to SIZE to small no.match.3[, SIZE := factor(&#39;small&#39;, levels = c(&#39;large&#39;, &#39;small&#39;, &#39;unknown&#39;))] no.match.3[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.size[, GEAR]))] no.match.3[, AREA := 0] #Merge together and proportion catch to known sizes size.sp &lt;- size.sp [!is.na(SPPLIVMT), ] size.sp.2 &lt;- size.sp.2[!is.na(SPPLIVMT), ] size.sp.2[, QY := i.QY] size.sp.2[, i.QY := NULL] setcolorder(size.sp.2, names(size.sp)) size.sp.3 &lt;- size.sp.3[!is.na(SPPLIVMT), ] size.sp.3[, QY := i.QY] size.sp.3[, HY := i.HY] size.sp.3[, i.QY := NULL] size.sp.3[, i.HY := NULL] setcolorder(size.sp.3, names(size.sp)) size.sp &lt;- rbindlist(list(size.sp, size.sp.2, size.sp.3)) size.sp[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] size.sp[, unk := i.SPPLIVMT * prop] size.sp[, unk2 := i.SPPVALUE * prop] size.sp[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.SIZE&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(size.sp, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) size.sp[, AREA := 0] size.sp[, GEAR := factor(&#39;unknown&#39;, levels = levels(k.size[, GEAR]))] setcolorder(size.sp, names(size.solved)) setcolorder(no.match.3, names(size.solved)) size.solved &lt;- rbindlist(list(size.solved, size.sp, no.match.3)) rm(list = c(ls(pattern = &#39;size.sp&#39;), ls(pattern = &#39;no.match&#39;))) #Merge back size.solved setcolorder(size.solved, names(comland.agg)) comland.agg &lt;- rbindlist(list(k.size, size.solved)) setkey(comland.agg, YEAR, QY, HY, SIZE, GEAR, AREA, NESPP3, UTILCD) comland.agg &lt;- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland.agg)] setnames(comland.agg, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #3.C GEAR------------------------------------------------------------------------------ unk.gear &lt;- comland.agg[GEAR == &#39;unknown&#39;, ] k.gear &lt;- comland.agg[GEAR != &#39;unknown&#39;, ] #3.C.1 - All match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;SIZE&#39;, &#39;AREA&#39;) unk.gear.all &lt;- unk.gear[AREA != 0, ] k.gear.all &lt;- k.gear[AREA != 0, ] setkeyv(unk.gear.all, match.key) setkeyv(k.gear.all, match.key) gear.all &lt;- k.gear.all[unk.gear.all] #No match - need to match with larger aggregation no.match &lt;- gear.all[is.na(SPPLIVMT), ] no.match[, c(&#39;GEAR&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.GEAR&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY, SIZE, AREA) setkeyv(k.gear.all, key(no.match)) gear.all.2 &lt;- k.gear.all[no.match] no.match.2 &lt;- gear.all.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3, SIZE, AREA) setkeyv(k.gear.all, key(no.match.2)) gear.all.3 &lt;- k.gear.all[no.match.2] no.match.3 &lt;- gear.all.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match.3, YEAR, NESPP3, AREA) setkeyv(k.gear.all, key(no.match.3)) gear.all.4 &lt;- k.gear.all[no.match.3] no.match.4 &lt;- gear.all.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop AREA setkey(no.match.4, YEAR, NESPP3) setkeyv(k.gear.all, key(no.match.4)) gear.all.5 &lt;- k.gear.all[no.match.4] no.match.5 &lt;- gear.all.5[is.na(SPPLIVMT), ] no.match.5[, c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.5, c(&#39;i.AREA&#39;, &#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to GEAR to other no.match.5[, GEAR := factor(&#39;other&#39;, levels = levels(k.gear[, GEAR]))] #Merge all together and proportion catch to known gears gear.all &lt;- gear.all [!is.na(SPPLIVMT), ] gear.all.2 &lt;- gear.all.2[!is.na(SPPLIVMT), ] gear.all.2[, QY := i.QY] gear.all.2[, i.QY := NULL] setcolorder(gear.all.2, names(gear.all)) gear.all.3 &lt;- gear.all.3[!is.na(SPPLIVMT), ] gear.all.3[, QY := i.QY] gear.all.3[, HY := i.HY] gear.all.3[, i.QY := NULL] gear.all.3[, i.HY := NULL] setcolorder(gear.all.3, names(gear.all)) gear.all.4 &lt;- gear.all.4[!is.na(SPPLIVMT), ] gear.all.4[, QY := i.QY] gear.all.4[, HY := i.HY] gear.all.4[, SIZE := i.SIZE] gear.all.4[, i.QY := NULL] gear.all.4[, i.HY := NULL] gear.all.4[, i.SIZE := NULL] setcolorder(gear.all.4, names(gear.all)) gear.all.5 &lt;- gear.all.5[!is.na(SPPLIVMT), ] gear.all.5[, QY := i.QY] gear.all.5[, HY := i.HY] gear.all.5[, SIZE := i.SIZE] gear.all.5[, AREA := i.AREA] gear.all.5[, i.QY := NULL] gear.all.5[, i.HY := NULL] gear.all.5[, i.SIZE := NULL] gear.all.5[, i.AREA := NULL] setcolorder(gear.all.5, names(gear.all)) gear.all &lt;- rbindlist(list(gear.all, gear.all.2, gear.all.3, gear.all.4, gear.all.5)) gear.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] gear.all[, unk := i.SPPLIVMT * prop] gear.all[, unk2 := i.SPPVALUE * prop] gear.all[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.GEAR&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(gear.all, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setcolorder(no.match.5, names(gear.all)) gear.solved &lt;- rbindlist(list(gear.all, no.match.5)) rm(list = c(ls(pattern = &#39;gear.all&#39;), ls(pattern = &#39;no.match&#39;))) #3.C.2 - Species only - no other match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;SIZE&#39;) unk.gear.sp &lt;- unk.gear[AREA == 0, ] unk.gear.sp[, &#39;AREA&#39; := NULL] k.gear.sp &lt;- k.gear[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(match.key, &#39;GEAR&#39;, &#39;UTILCD&#39;)] setnames(k.gear.sp, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkeyv(unk.gear.sp, match.key) setkeyv(k.gear.sp, match.key) gear.sp &lt;- k.gear.sp[unk.gear.sp] #No match - need to match with larger aggregation no.match &lt;- gear.sp[is.na(SPPLIVMT), ] no.match[, c(&#39;GEAR&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.GEAR&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY, SIZE) setkeyv(k.gear.sp, key(no.match)) gear.sp.2 &lt;- k.gear.sp[no.match] no.match.2 &lt;- gear.sp.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3, SIZE) setkeyv(k.gear.sp, key(no.match.2)) gear.sp.3 &lt;- k.gear.sp[no.match.2] no.match.3 &lt;- gear.sp.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.GEAR&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match.3, YEAR, NESPP3) setkeyv(k.gear.sp, key(no.match.3)) gear.sp.4 &lt;- k.gear.sp[no.match.3] no.match.4 &lt;- gear.sp.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - assign to GEAR to other no.match.4[, GEAR := factor(&#39;other&#39;, levels = levels(k.gear[, GEAR]))] no.match.4[, AREA := 0] #Merge all together and proportion catch to known gears gear.sp &lt;- gear.sp [!is.na(SPPLIVMT), ] gear.sp.2 &lt;- gear.sp.2[!is.na(SPPLIVMT), ] gear.sp.2[, QY := i.QY] gear.sp.2[, i.QY := NULL] setcolorder(gear.sp.2, names(gear.sp)) gear.sp.3 &lt;- gear.sp.3[!is.na(SPPLIVMT), ] gear.sp.3[, QY := i.QY] gear.sp.3[, HY := i.HY] gear.sp.3[, i.QY := NULL] gear.sp.3[, i.HY := NULL] setcolorder(gear.sp.3, names(gear.sp)) gear.sp.4 &lt;- gear.sp.4[!is.na(SPPLIVMT), ] gear.sp.4[, QY := i.QY] gear.sp.4[, HY := i.HY] gear.sp.4[, SIZE := i.SIZE] gear.sp.4[, i.QY := NULL] gear.sp.4[, i.HY := NULL] gear.sp.4[, i.SIZE := NULL] setcolorder(gear.sp.4, names(gear.sp)) gear.sp &lt;- rbindlist(list(gear.sp, gear.sp.2, gear.sp.3, gear.sp.4)) gear.sp[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] gear.sp[, unk := i.SPPLIVMT * prop] gear.sp[, unk2 := i.SPPVALUE * prop] gear.sp[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.GEAR&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(gear.sp, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) gear.sp[, AREA := 0] setcolorder(gear.sp, names(gear.solved)) setcolorder(no.match.4, names(gear.solved)) gear.solved &lt;- rbindlist(list(gear.solved, gear.sp, no.match.4)) rm(list = c(ls(pattern = &#39;gear.sp&#39;), ls(pattern = &#39;no.match&#39;))) #Merge back gear.solved setcolorder(gear.solved, names(comland.agg)) comland.agg &lt;- rbindlist(list(k.gear, gear.solved)) setkey(comland.agg, YEAR, QY, HY, SIZE, GEAR, AREA, NESPP3, UTILCD) comland.agg &lt;- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland.agg)] setnames(comland.agg, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #3.D AREA------------------------------------------------------------------------------ unk.area &lt;- comland.agg[AREA == 0, ] k.area &lt;- comland.agg[AREA != 0, ] #3.C.1 - All match match.key &lt;- c(&#39;YEAR&#39;, &#39;NESPP3&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;SIZE&#39;, &#39;GEAR&#39;) unk.area.all &lt;- unk.area k.area.all &lt;- k.area setkeyv(unk.area.all, match.key) setkeyv(k.area.all, match.key) area.all &lt;- k.area.all[unk.area.all] #No match - need to match with larger aggregation no.match &lt;- area.all[is.na(SPPLIVMT), ] no.match[, c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match, c(&#39;i.AREA&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop QY setkey(no.match, YEAR, NESPP3, HY, SIZE, GEAR) setkeyv(k.area.all, key(no.match)) area.all.2 &lt;- k.area.all[no.match] no.match.2 &lt;- area.all.2[is.na(SPPLIVMT), ] no.match.2[, c(&#39;AREA&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.2, c(&#39;i.AREA&#39;, &#39;i.QY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;QY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop HY setkey(no.match.2, YEAR, NESPP3, SIZE, GEAR) setkeyv(k.area.all, key(no.match.2)) area.all.3 &lt;- k.area.all[no.match.2] no.match.3 &lt;- area.all.3[is.na(SPPLIVMT), ] no.match.3[, c(&#39;AREA&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3, c(&#39;i.AREA&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop SIZE setkey(no.match.3, YEAR, NESPP3, GEAR) setkeyv(k.area.all, key(no.match.3)) area.all.4 &lt;- k.area.all[no.match.3] no.match.4 &lt;- area.all.4[is.na(SPPLIVMT), ] no.match.4[, c(&#39;AREA&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.4, c(&#39;i.AREA&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Drop GEAR setkey(no.match.4, YEAR, NESPP3) setkeyv(k.area.all, key(no.match.4)) area.all.5 &lt;- k.area.all[no.match.4] no.match.5 &lt;- area.all.5[is.na(SPPLIVMT), ] no.match.5[, c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.5, c(&#39;i.AREA&#39;, &#39;i.GEAR&#39;, &#39;i.SIZE&#39;, &#39;i.QY&#39;, &#39;i.HY&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;GEAR&#39;, &#39;SIZE&#39;, &#39;QY&#39;, &#39;HY&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Still no match - use 3 or 5 year window then drop year years &lt;- unique(no.match.5[, YEAR], by = key(no.match.5)) no.match.6 &lt;- c() area.all.6 &lt;- c() for(i in 1:length(years)){ #3 year window k.area.3y &lt;- comland.agg[AREA != 0 &amp; YEAR %in% (years[i] - 1):(years[i] + 1), ] setkey(k.area.3y, NESPP3, AREA) k.area.3y &lt;- k.area.3y[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(key(k.area.3y), &#39;UTILCD&#39;)] setnames(k.area.3y, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) unk.area.3y &lt;- no.match.5[YEAR == years[i], ] setkey(unk.area.3y, NESPP3) setkey(k.area.3y, NESPP3) area.3y &lt;- k.area.3y[unk.area.3y] no.match.3y &lt;- area.3y[is.na(SPPLIVMT), ] no.match.3y[, c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.3y, c(&#39;i.AREA&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) no.match.6 &lt;- rbindlist(list(no.match.6, no.match.3y)) area.all.6 &lt;- rbindlist(list(area.all.6, area.3y)) } years &lt;- unique(no.match.6[, YEAR], by = key(no.match.6)) no.match.7 &lt;- c() area.all.7 &lt;- c() for(i in 1:length(years)){ #5 year window k.area.5y &lt;- comland.agg[AREA != 0 &amp; YEAR %in% (years[i] - 2):(years[i] + 2), ] setkey(k.area.5y, NESPP3, AREA) k.area.5y &lt;- k.area.5y[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(key(k.area.5y), &#39;UTILCD&#39;)] setnames(k.area.5y, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) unk.area.5y &lt;- no.match.6[YEAR == years[i], ] setkey(unk.area.5y, NESPP3) setkey(k.area.5y, NESPP3) area.5y &lt;- k.area.5y[unk.area.5y] no.match.5y &lt;- area.5y[is.na(SPPLIVMT), ] no.match.5y[, c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.5y, c(&#39;i.AREA&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) no.match.7 &lt;- rbindlist(list(no.match.7, no.match.5y)) area.all.7 &lt;- rbindlist(list(area.all.7, area.5y)) } #Drop year setkey(no.match.7, NESPP3) k.area.all &lt;- k.area.all[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = c(&#39;NESPP3&#39;, &#39;AREA&#39;, &#39;UTILCD&#39;)] setnames(k.area.all, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setkey(k.area.all, NESPP3) area.all.8 &lt;- k.area.all[no.match.7] no.match.8 &lt;- area.all.8[is.na(SPPLIVMT), ] no.match.8[, c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;) := NULL] setnames(no.match.8, c(&#39;i.AREA&#39;, &#39;i.UTILCD&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;), c(&#39;AREA&#39;, &#39;UTILCD&#39;, &#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #If still no match - leave as unknown #Merge all together and proportion catch to known areas area.all &lt;- area.all [!is.na(SPPLIVMT), ] area.all.2 &lt;- area.all.2[!is.na(SPPLIVMT), ] area.all.2[, QY := i.QY] area.all.2[, i.QY := NULL] setcolorder(area.all.2, names(area.all)) area.all.3 &lt;- area.all.3[!is.na(SPPLIVMT), ] area.all.3[, QY := i.QY] area.all.3[, HY := i.HY] area.all.3[, i.QY := NULL] area.all.3[, i.HY := NULL] setcolorder(area.all.3, names(area.all)) area.all.4 &lt;- area.all.4[!is.na(SPPLIVMT), ] area.all.4[, QY := i.QY] area.all.4[, HY := i.HY] area.all.4[, SIZE := i.SIZE] area.all.4[, i.QY := NULL] area.all.4[, i.HY := NULL] area.all.4[, i.SIZE := NULL] setcolorder(area.all.4, names(area.all)) area.all.5 &lt;- area.all.5[!is.na(SPPLIVMT), ] area.all.5[, QY := i.QY] area.all.5[, HY := i.HY] area.all.5[, SIZE := i.SIZE] area.all.5[, GEAR := i.GEAR] area.all.5[, i.QY := NULL] area.all.5[, i.HY := NULL] area.all.5[, i.SIZE := NULL] area.all.5[, i.GEAR := NULL] setcolorder(area.all.5, names(area.all)) area.all.6 &lt;- area.all.6[!is.na(SPPLIVMT), ] setcolorder(area.all.6, names(area.all)) area.all.7 &lt;- area.all.7[!is.na(SPPLIVMT), ] setcolorder(area.all.7, names(area.all)) area.all.8 &lt;- area.all.8[!is.na(SPPLIVMT), ] setcolorder(area.all.8, names(area.all)) area.all &lt;- rbindlist(list(area.all, area.all.2, area.all.3, area.all.4, area.all.5, area.all.6, area.all.7, area.all.8)) area.all[, prop := SPPLIVMT / sum(SPPLIVMT), by = match.key] area.all[, unk := i.SPPLIVMT * prop] area.all[, unk2 := i.SPPVALUE * prop] area.all[, c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;, &#39;i.SPPLIVMT&#39;, &#39;i.SPPVALUE&#39;, &#39;i.AREA&#39;, &#39;i.UTILCD&#39;, &#39;prop&#39;) := NULL] setnames(area.all, c(&#39;unk&#39;,&#39;unk2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) setcolorder(no.match.8, names(area.all)) area.solved &lt;- rbindlist(list(area.all, no.match.8)) rm(list = c(ls(pattern = &#39;area.all&#39;), ls(pattern = &#39;no.match&#39;))) #Merge back area.solved setcolorder(area.solved, names(comland.agg)) comland.agg &lt;- rbindlist(list(k.area, area.solved)) setkey(comland.agg, YEAR, QY, HY, SIZE, GEAR, AREA, NESPP3, UTILCD) comland.agg &lt;- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland.agg)] setnames(comland.agg, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #------------------------------------------------------------------------------- if(sum.by == &#39;EPU&#39;){ #Assign EPU based on statarea gom&lt;-c(500, 510, 512:515) gb&lt;-c(521:526, 551, 552, 561, 562) mab&lt;-c(537, 539, 600, 612:616, 621, 622, 625, 626, 631, 632) ss&lt;-c(463:467, 511) comland.agg[AREA %in% gom, EPU := &#39;GOM&#39;] comland.agg[AREA %in% gb, EPU := &#39;GB&#39;] comland.agg[AREA %in% mab, EPU := &#39;MAB&#39;] comland.agg[AREA %in% ss, EPU := &#39;SS&#39;] comland.agg[is.na(EPU), EPU := &#39;OTHER&#39;] comland.agg[, EPU := factor(EPU, levels = c(&#39;GOM&#39;, &#39;GB&#39;, &#39;MAB&#39;, &#39;SS&#39;, &#39;OTHER&#39;))] setkey(comland.agg, YEAR, NESPP3, QY, GEAR, SIZE, EPU, UTILCD) comland.agg &lt;- comland.agg[, list(sum(SPPLIVMT), sum(SPPVALUE)), by = key(comland.agg)] setnames(comland.agg, c(&#39;V1&#39;, &#39;V2&#39;), c(&#39;SPPLIVMT&#39;, &#39;SPPVALUE&#39;)) #Note - NAFO landings by division only so not available in sum.by = &quot;stat.area&quot; #Add NAFO foreign landings - Data from http://www.nafo.int/data/frames/data.html temp &lt;- tempfile() download.file(&quot;https://www.nafo.int/Portals/0/Stats/nafo-21b-60-69.zip?ver=2016-08-03-063915-850&quot;,temp) nafo.60 &lt;- as.data.table(read.csv(unz(temp, &quot;NAFO21B-60-69.txt&quot;))) unlink(temp) download.file(&quot;https://www.nafo.int/Portals/0/Stats/nafo-21b-70-79.zip?ver=2016-08-03-063915-850&quot;,temp) nafo.70 &lt;- as.data.table(read.csv(unz(temp, &quot;NAFO21B-70-79.txt&quot;))) unlink(temp) download.file(&quot;https://www.nafo.int/Portals/0/Stats/nafo-21b-80-89.zip?ver=2016-08-03-063915-850&quot;,temp) nafo.80 &lt;- as.data.table(read.csv(unz(temp, &quot;NAFO21B-80-89.txt&quot;))) unlink(temp) download.file(&quot;https://www.nafo.int/Portals/0/Stats/nafo-21b-90-99.zip?ver=2016-08-03-063915-850&quot;,temp) nafo.90 &lt;- as.data.table(read.csv(unz(temp, &quot;NAFO21B-90-99.txt&quot;))) unlink(temp) download.file(&quot;https://www.nafo.int/Portals/0/Stats/nafo-21b-2000-09.zip?ver=2016-08-03-063915-850&quot;,temp) nafo.00 &lt;- as.data.table(read.csv(unz(temp, &quot;NAFO21B-2000-09.txt&quot;))) unlink(temp) download.file(&quot;https://www.nafo.int/Portals/0/Stats/nafo-21b-2010-15.zip?ver=2017-06-01-164323-460&quot;,temp) nafo.10 &lt;- as.data.table(read.csv(unz(temp, &quot;nafo-21b-2010-15/NAFO21B-2010-15.csv&quot;))) unlink(temp) #2010 + data have different column headers setnames(nafo.10, c(&#39;Gear&#39;, &#39;AreaCode&#39;, &#39;SpeciesEffort&#39;), c(&#39;GearCode&#39;, &#39;Divcode&#39;, &#39;Code&#39;)) nafo &lt;- rbindlist(list(nafo.60, nafo.70, nafo.80, nafo.90, nafo.00, nafo.10)) #Remove US landings (Country code 22), extra divisions (use only 47, 51:56, 61:63), #and effort codes (1:3) nafo &lt;- nafo[Country != 22 &amp; Divcode %in% c(47, 51:56, 61:63) &amp; Code &gt; 3, ] #Deal with unknown monthly catch????? #Get nafo code in a similar format to comland nafoland &lt;- nafo[, list(Year, GearCode, Tonnage, Divcode, Code, Catches)] nafoland[, MONTH := 0] setnames(nafoland, &#39;Catches&#39;, &#39;SPPLIVMT&#39;) month &lt;- c(&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;) for(i in 1:12){ nafoland.month &lt;- nafo[, list(Year, GearCode, Tonnage, Divcode, Code, get(month[i]))] nafoland.month[, MONTH := i] setnames(nafoland.month, names(nafoland.month)[6], &#39;SPPLIVMT&#39;) nafoland &lt;- rbindlist(list(nafoland, nafoland.month)) } nafoland &lt;- nafoland[SPPLIVMT != 0,] nafoland[, EPU := factor(NA, levels = c(&#39;GOM&#39;, &#39;GB&#39;, &#39;MAB&#39;, &#39;SS&#39;, &#39;OTHER&#39;))] nafoland[Divcode == 47, EPU := &#39;SS&#39;] nafoland[Divcode == 51, EPU := &#39;GOM&#39;] nafoland[Divcode %in% c(52, 54:56), EPU := &#39;GB&#39;] nafoland[Divcode %in% c(53, 61:63), EPU := &#39;MAB&#39;] nafoland[is.na(EPU), EPU := &#39;OTHER&#39;] nafoland[, Divcode := NULL] ##Fix missing Scotian Shelf data from 21B SS.nafo &lt;- as.data.table(read.csv(file.path(data.dir.3, &quot;SS_NAFO_21A.csv&quot;), skip = 8)) #Add NAFOSPP code to SS.nafo nafo.spp &lt;- as.data.table(read.csv(file.path(data.dir.3, &#39;species.txt&#39;))) setnames(nafo.spp, &quot;Abbreviation&quot;, &quot;Species_ASFIS&quot;) nafo.spp &lt;- nafo.spp[, list(Code, Species_ASFIS)] SS.nafo &lt;- merge(SS.nafo, nafo.spp, by = &#39;Species_ASFIS&#39;, all.x = T) #Only grab missing data SS.nafo &lt;- SS.nafo[Year %in% c(2003, 2008, 2009), ] setkey(SS.nafo, Year, Code) SS.land &lt;- SS.nafo[, sum(Catch...000.Kg.), by = key(SS.nafo)] setnames(SS.land, &quot;V1&quot;, &quot;SPPLIVMT&quot;) #Add GearCode, Tonnage, Month, and EPU SS.land[, GearCode := 99] SS.land[, Tonnage := 0] SS.land[, MONTH := 0] SS.land[, EPU := &#39;SS&#39;] setcolorder(SS.land, names(nafoland)) nafoland &lt;- rbindlist(list(nafoland, SS.land)) #Rectify NAFO codes with US codes #Species setnames(nafoland, c(&#39;Year&#39;, &#39;GearCode&#39;, &#39;Tonnage&#39;, &#39;Code&#39;), c(&#39;YEAR&#39;, &#39;NAFOGEAR&#39;, &#39;TONCL1&#39;, &#39;NAFOSPP&#39;)) spp &lt;- as.data.table(sqlQuery(channel, &quot;select NAFOSPP, NESPP3 from CFSPP&quot;)) #Fix missing NAFO codes missing.spp &lt;- data.table(NAFOSPP = c(110, 141, 189, 480, 484, 487, 488, 489), NESPP3 = c(240, 509, 512, 366, 368, 367, 370, 369)) spp &lt;- rbindlist(list(spp, missing.spp)) setkey(spp, NAFOSPP) spp &lt;- unique(spp, by = key(spp)) #Fix many to one relationships spp[NAFOSPP == 199, NESPP3 := 524] spp[NAFOSPP == 299, NESPP3 := 525] spp[NAFOSPP == 469, NESPP3 := 359] spp[NAFOSPP == 499, NESPP3 := 526] spp[NAFOSPP == 529, NESPP3 := 764] spp[NAFOSPP == 699, NESPP3 := 899] nafoland &lt;- merge(nafoland, spp, by = &#39;NAFOSPP&#39;, all.x = T) #fix codes nafoland[NAFOSPP == 309, NESPP3 := 150L] nafoland[NAFOSPP == 462, NESPP3 := 481L] nafoland[NAFOSPP == 464, NESPP3 := 355L] nafoland[NAFOSPP == 468, NESPP3 := 493L] nafoland[NAFOSPP == 704, NESPP3 := 817L] #remove species without a match nafoland &lt;- nafoland[!is.na(NESPP3), ] #Remove herring catch - already included from Maine Data earlier nafoland &lt;- nafoland[NESPP3 != 168, ] #Gearcodes gear &lt;- as.data.table(sqlQuery(channel, &quot;select NEGEAR, NAFOGEAR from Gear&quot;)) gear &lt;- unique(gear, by = &#39;NAFOGEAR&#39;) nafoland &lt;- merge(nafoland, gear, by = &#39;NAFOGEAR&#39;, all.x = T) #fix codes nafoland[NAFOGEAR == 8, NEGEAR := 50L] nafoland[NAFOGEAR == 9, NEGEAR := 370L] nafoland[NAFOGEAR == 19, NEGEAR := 58L] nafoland[NAFOGEAR == 49, NEGEAR := 60L] nafoland[NAFOGEAR == 56, NEGEAR := 21L] #Tonnage nafoland[TONCL1 == 7, TONCL1 := 6L] #Drop NAFO codes nafoland[, c(&#39;NAFOGEAR&#39;, &#39;NAFOSPP&#39;) := NULL] #Fix skates #get little skates and winter skates from skates(ns) - use survey in half years #Generate Half year variable in comland nafoland.skates &lt;- nafoland[NESPP3 == 365, ] nafoland.skates[MONTH %in% 1:6, Half := 1] nafoland.skates[MONTH %in% 7:12, Half := 2] setkey(skate.hake.nafo, YEAR, Half, EPU) nafoland.skates &lt;- merge(nafoland.skates, skate.hake.nafo, by = key(skate.hake.nafo), all.x = T) nafoland.skates[NESPP3 == 365, little := little.per * SPPLIVMT] nafoland.skates[is.na(little), little := 0] nafoland.skates[NESPP3 == 365, winter := winter.per * SPPLIVMT] nafoland.skates[is.na(winter), winter := 0] nafoland.skates[NESPP3 == 365, other.skate := SPPLIVMT - (little + winter)] #Little (366), winter (367), skates(ns) (365) #put skates in nafoland format to merge back little &lt;- nafoland.skates[, list(YEAR, Half, EPU, TONCL1, MONTH, NESPP3, NEGEAR, little)] little[, NESPP3 := 366L] setnames(little, &quot;little&quot;, &quot;SPPLIVMT&quot;) little &lt;- little[SPPLIVMT &gt; 0, ] winter &lt;- nafoland.skates[, list(YEAR, Half, EPU, TONCL1, MONTH, NESPP3, NEGEAR, winter)] winter[, NESPP3 := 367L] setnames(winter, &quot;winter&quot;, &quot;SPPLIVMT&quot;) winter &lt;- winter[SPPLIVMT &gt; 0, ] other &lt;- nafoland.skates[, list(YEAR, Half, EPU, TONCL1, MONTH, NESPP3, NEGEAR, other.skate)] other[, NESPP3 := 365L] setnames(other, &quot;other.skate&quot;, &quot;SPPLIVMT&quot;) other &lt;- other[SPPLIVMT &gt; 0, ] #merge all three and reformat for nafoland skates.add.back &lt;- rbindlist(list(little, winter, other)) skates.add.back[, Half := NULL] setcolorder(skates.add.back, names(nafoland)) nafoland &lt;- rbindlist(list(nafoland[NESPP3 != 365, ], skates.add.back)) #aggregate nafo landings #2 - aggregate by quarter year, half year, major gear, and small/large TC nafoland[MONTH %in% 1:3, QY := 1] nafoland[MONTH %in% 4:6, QY := 2] nafoland[MONTH %in% 7:9, QY := 3] nafoland[MONTH %in% 10:12, QY := 4] nafoland[MONTH == 0, QY := 1] nafoland[NEGEAR %in% otter, GEAR := &#39;otter&#39;] nafoland[NEGEAR %in% dredge.sc, GEAR := &#39;dredge.sc&#39;] nafoland[NEGEAR %in% pot, GEAR := &#39;pot&#39;] nafoland[NEGEAR %in% longline, GEAR := &#39;longline&#39;] nafoland[NEGEAR %in% seine, GEAR := &#39;seine&#39;] nafoland[NEGEAR %in% gillnet, GEAR := &#39;gillnet&#39;] nafoland[NEGEAR %in% midwater, GEAR := &#39;midwater&#39;] nafoland[NEGEAR %in% dredge.o, GEAR := &#39;dredge.o&#39;] nafoland[NEGEAR == 99, GEAR := &#39;unknown&#39;] nafoland[is.na(GEAR), GEAR := &#39;other&#39;] nafoland[, GEAR := as.factor(GEAR)] nafoland[TONCL1 %in% 1:3, SIZE := &#39;small&#39;] nafoland[TONCL1 &gt; 3, SIZE := &#39;large&#39;] nafoland[TONCL1 == 0, SIZE := &#39;unknown&#39;] nafoland[, SIZE := as.factor(SIZE)] setkey(nafoland, YEAR, QY, GEAR, SIZE, EPU, NESPP3) nafoland.agg &lt;- nafoland[, sum(SPPLIVMT), by = key(nafoland)] setnames(nafoland.agg, &quot;V1&quot;, &quot;SPPLIVMT&quot;) #Create dummy variable for value nafoland.agg[, SPPVALUE := 0] nafoland.agg[, UTILCD := 0] #Merge comland and nafoland setcolorder(nafoland.agg, names(comland.agg)) if(foreign == &#39;y&#39;){ comland.agg[, US := T] nafoland.agg[, US := F] } comland.nafo &lt;- rbindlist(list(comland.agg, nafoland.agg)) #Remove Menhaden data #save(comland.nafo, file = paste(out.dir, &quot;comland_Menhaden.RData&quot;, sep = &#39;&#39;)) comland &lt;- comland.nafo[NESPP3 != 221, ] } if(sum.by == &#39;stat.area&#39;) comland &lt;- comland.agg #Output file if(landed == &#39;n&#39;) file.landed &lt;- &#39;&#39; else file.landed &lt;- &#39;_meatwt&#39; if(adjust.ppi == &#39;n&#39;) file.adjust &lt;- &#39;&#39; else file.adjust &lt;- &#39;_deflated&#39; if(sum.by == &#39;EPU&#39;) file.by &lt;- &#39;&#39; else file.by &lt;- &#39;_stat_areas&#39; file.name &lt;- paste0(&#39;comland&#39;, file.landed, file.adjust, file.by, &#39;.RData&#39;) save(comland, file = file.path(out.dir, file.name)) 17.1.2.1 Data Processing The landings data were formatted for inclusion in the ecodata R package with the following R code. # Process commerical data # These data were derived from the Commercial Fisheries Database Biological Sample. More information about # these data are available at https://noaa-edab.github.io/tech-doc/comdat.html library(dplyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_comdat &lt;- function(save_clean = F){ load(file.path(raw.dir,&quot;Commercial_data_pull_19.RData&quot;)) commercial &lt;- commercial %&gt;% dplyr::rename(EPU = Region) %&gt;% dplyr::select(-Source) if (save_clean){ usethis::use_data(comdat, overwrite = T) } else { return(comdat) } } 17.1.3 Data analysis Fisheries dependent data from Comlands is used in several indicators for the State of the Ecosystem report; the more complicated analyses are detailed in their own sections. The most straightforward use of this data are the aggregate landings indicators. These are calculated by first assigning the various species into aggregate groups. Species are also marked by which management body manages them. Landings are then summed by year, EPU, aggregate group, and whether they are managed or not. Both managed and unmanaged totals are added together to get the final amount of total landings for that aggregate group within its respective region. Both the total and those landings managed by the management body receiving the report are reported. Proportions of managed landings to total landings are also reported in tabular form. 17.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) ## Ecosystem-wide and managed species total landings opar &lt;- par(mfrow = c(5, 1), mar = c(0, 0, 0, 0), oma = c(4, 6.5, 2, 6)) soe.plot(SOE.data, &quot;Time&quot;,&quot;Apex Predator Landings GB&quot;, stacked = &quot;A&quot;,status = F, endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986, ymin = FALSE, y.lower = 0,cex.stacked = 1.5 ) soe.plot(SOE.data, &quot;Time&quot;, &quot;Piscivore Landings GB&quot;, stacked = &quot;B&quot;,status = F, endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986, ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = &quot;Time&quot;, y.var2 = &quot;Piscivore NEFMC managed species sea food GB&quot;,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Planktivore Landings GB&quot;, stacked = &quot;C&quot;,status = F, endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986, ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = &quot;Time&quot;, y.var2 = &quot;Planktivore NEFMC managed species sea food GB&quot;,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Benthivore Landings GB&quot;, stacked = &quot;D&quot;,status = F, endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986, ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = &quot;Time&quot;, y.var2 = &quot;Benthivore NEFMC managed species sea food GB&quot;,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Benthos Landings GB&quot;, stacked = &quot;E&quot;,status = F, endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 10^3,end.start = 2007, x.start = 1986, ymin = FALSE, y.lower = 0 , extra = TRUE, x.var2 = &quot;Time&quot;, y.var2 = &quot;Benthos NEFMC managed species sea food GB&quot;,cex.stacked = 1.5) soe.stacked.axis(&quot;Year&quot;, expression(&quot;Landings, 10&quot;^3*&quot;metric tons&quot;), x.line = 2.7) Figure 17.2: NEFMC seafood specific landings (red) and total commericial landings (black) in Georges Bank (A: Apex predators, B: Piscivore, C: Planktivore, D: Benthivore, E: Benthos). "],
["long-term-sea-surface-temperature.html", "18 Long-term Sea Surface Temperature 18.1 Methods", " 18 Long-term Sea Surface Temperature Description: Long-term sea-surface temperatures Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Database pull Contributor(s): Kevin Friedland Data steward: Kevin Friedland, kevin.friedland@noaa.gov Point of contact: Kevin Friedland, kevin.friedland@noaa.gov Public availability statement: Source data are available here. 18.1 Methods Data for long-term sea-surface temperatures were derived from the NOAA extended reconstructed sea surface temperature data set (ERSST V5). The ERSST V5 dataset is parsed into 2° x 2° gridded bins between 1854-present with monthly temporal resolution. Data were interpolated in regions with limited spatial coverage, and heavily damped during the period between 1854-1880 when collection was inconsistent (Huang, Thorne, et al. 2017b; Huang, Thorne, et al. 2017a). For this analysis, 19 bins were selected that encompassed the Northeast US Continental Shelf region (see Friedland and Hare 2007). 18.1.1 Data sources This indicator is derived from the NOAA ERSST V5 dataset (Huang, Thorne, et al. 2017b). 18.1.2 Data extraction Table 18.1: Coordinates used in NOAA ERSST V5 data extraction. Longitude Latitude -74 40 -74 38 -72 40 -70 44 -70 42 -70 40 -68 44 -68 42 R code used in extracting time series of long-term SST data. # Include R code here # all year years=&quot;ersst&quot; # a year #years=&quot;.2016&quot; # which data setwd(&quot;C:/2_ersst/datafiles_v4&quot;) setwd(&quot;C:/2_ersst/datafiles_v5&quot;) # NES standard bounded by 34-46N and 78-62W minlon= -78; maxlon= -62; minlat= 34; maxlat= 46 dataoutfile=&quot;C:/2_ersst/nes_std_area_v5.csv&quot; # DELETE ONLY FILE WILL APPEND AND DOUBLE DATA file.remove(dataoutfile) ################################################## END SET # LABRODOR SEA #minlon= -66; #maxlon= -44; #minlat= 50; #maxlat= 70 #dataoutfile=&quot;C:/2_ersst/lab_sea.csv&quot; # New eel #minlon= -80; maxlon= -40; minlat= 20; maxlat= 40 #dataoutfile=&quot;C:/2_ersst/new eel.csv&quot; # G of Mex #minlon= -98; maxlon= -82; minlat= 18; maxlat= 30 #dataoutfile=&quot;C:/2_ersst/gomex_area.csv&quot; # USSE INCLUDING 28-36N and 80-76W not bounding #minlon= -80; #maxlon= -76; #minlat= 28; #maxlat= 36 #dataoutfile=&quot;C:/2_ersst/usse_area.csv&quot; # GSL standard bounded by 34-46N and 78-62W #minlon= -68; maxlon= -60; minlat= 46; maxlat= 48 #dataoutfile=&quot;C:/2_ersst/gsl.csv&quot; # PACIFIC area bounded by 10-30N and 166-146W #minlon= -166; #maxlon= -146; #minlat= 10; #maxlat= 30 #dataoutfile=&quot;C:/1_analyses/ersst/pac_islands_area.csv&quot; # Baltic area bounded by 52-66N and 14-28E #minlon= 14; #maxlon= 28; #minlat= 52; #maxlat= 66 #dataoutfile=&quot;C:/2_ersst/baltic area.csv&quot; # North Atlantic Area bounded by 30-70N and 80W-20E #minlon= -80; #maxlon= -2; #minlat= 30; #maxlat= 70 #dataoutfile=&quot;C:/2_ersst/na area1.csv&quot; # and ... #minlon= 0; #maxlon= 20; #minlat= 30; #maxlat= 70 #dataoutfile=&quot;C:/1_analyses/ersst/na area2.csv&quot; # NorthEAST Atlantic Area bounded by 55-70N and 10W-20E #minlon= -10; #maxlon= -2; #minlat= 56; #maxlat= 70 #dataoutfile=&quot;C:/1_analyses/ersst/ne atl 1.csv&quot; # and ... #minlon= 0; #maxlon= 10; #minlat= 56; #maxlat= 70 #dataoutfile=&quot;C:/1_analyses/ersst/ne atl 2.csv&quot; # Pacific steelhead #minlon= -160; #maxlon= -122; #minlat= 40; #maxlat= 62 #dataoutfile=&quot;C:/1_analyses/ersst/pac sh.csv&quot; #North Pacific in two parts #1 #minlon= -180; #maxlon= -120; #minlat= 30; #maxlat= 72 #dataoutfile=&quot;C:/1_analyses/ersst/n_pac_1.csv&quot; #2 #minlon= 120; #maxlon= 178; #minlat= 30; #maxlat= 72 #dataoutfile=&quot;C:/1_analyses/ersst/n_pac_2.csv&quot; # North Atlantic Area bounded by 20-70N and 100W-30E #minlon= -100; #maxlon= -2; #minlat= 20; #maxlat= 70 #dataoutfile=&quot;C:/1_analyses/ersst/na area1.csv&quot; # and ... #minlon= 0; #maxlon= 30; #minlat= 20; #maxlat= 70 #dataoutfile=&quot;C:/1_analyses/ersst/na area2.csv&quot; # constants for area R &lt;- 6371 # Earth mean radius [km] dheight = 222 #library(ncdf) library(ncdf4) # ERSST data # lon goes from 0E to 358E with lon at center of box # lat goes from 88S to 88N with lat at center of box # start with lon based on degrees lonew # array 1 2 ... 90 91 92 ... 180 # lon 0 2 ... 178 180 182 ... 358 # lonew 0 2 ... 178 -180 -178 ... -2 # star with lat + deg N, - deg S # array 1 ... 45 ... 89 # lon -88 ... 0 ... 88 # -&gt; -&gt; -&gt; TO KEEP THINGS SIMPLE, RETRIEVALS CAN&#39;T BE CONTINUOUS FROM - LONS TO + LONs # have to extract from -180W to -2W separately from 0E to 180E # -&gt; -&gt; -&gt; OUTPUT APPENDS SO NEED TO DELETE FILE IF ALREADY EXISTS # -&gt; -&gt; -&gt; USE APPROPRIATE lon lat and outfile block: # set lon limits in array units if ( minlon &lt; 0){ arrayminlon=(minlon+360)/2+1 } else { arrayminlon=minlon/2+1 } if ( maxlon &lt; 0){ arraymaxlon=(maxlon+360)/2+1 } else { arraymaxlon=maxlon/2+1 } # set lat limits in array units arrayminlat=minlat/2+45 arraymaxlat=maxlat/2+45 filelist=list.files(pattern=years) numfiles=length(filelist) for (filenum in 1:numfiles){ # ersst = open.ncdf(filelist[filenum]) ersst = nc_open(filelist[filenum]) print(filelist[filenum]) # sst = get.var.ncdf( ersst, &quot;sst&quot;) sst &lt;- ncvar_get(ersst,&quot;sst&quot; ) year=as.numeric(substr(filelist[filenum],10,13)) month=as.numeric(substr(filelist[filenum],14,15)) for (arrlons in arrayminlon:arraymaxlon){ for (arrlats in arrayminlat:arraymaxlat){ if ( arrlons &lt; 91){ regenlon=(arrlons-1)*2 } else { regenlon=(arrlons-1)*2-360 } regenlat=(arrlats-45)*2 long1=regenlon-1 *pi/180 lat1=regenlat-1 *pi/180 long2=regenlon+1 *pi/180 lat2=regenlat-1 *pi/180 dwidth1 &lt;- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R long1=regenlon-1 *pi/180 lat1=regenlat+1 *pi/180 long2=regenlon+1 *pi/180 lat2=regenlat+1 *pi/180 dwidth2 &lt;- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R area=((dwidth1 + dwidth1)/2) * dheight dataline &lt;- matrix(c(year, month, regenlon, regenlat, round(sst[arrlons,arrlats],digits=2),area),1,6) if(is.finite(sst[arrlons,arrlats])) { write.table(dataline,file=dataoutfile,sep=&quot;,&quot;,row.name=F,col.names=F,append=TRUE) } } } # close.ncdf(ersst) nc_close(ersst) } 18.1.3 Data Processing Data were formatted for inclusion in the ecodata R package with the following R code. #process ERSST long-term SST data library(dplyr) library(tidyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_long_term_sst &lt;- function(save_clean = F){ long_term_sst &lt;- read.csv(file.path(raw.dir,&quot;ersst annual mean.csv&quot;)) %&gt;% dplyr::rename(Time = Year, Value = Mean) %&gt;% mutate(Var = &quot;long-term sst&quot;, EPU = &quot;All&quot;, Units = &quot;degreesC&quot;) if (save_clean) { usethis::use_data(long_term_sst, overwrite = T) } else { return(long_term_sst) } } 18.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mar = c(4, 6, 2, 6)) soe.plot(SOE.data, &quot;Time&quot;, &quot;long term sst&quot;, end.start = 2007, line.forward = TRUE, point.cex = 0.8, rel.y.text = 1.1, x.line = 2, y.line = 2.3, x.label = &#39;Year&#39;, rel.y.num = 1.1, y.label = expression(paste(&quot;Mean SST (&quot;,degree,&quot;C)&quot;))) Figure 18.1: Long-term sea surface temperatures on the Northeast Continental Shelf. References "],
["mid-atlantic-harmful-algal-bloom-indicator.html", "19 Mid-Atlantic Harmful Algal Bloom Indicator 19.1 Methods", " 19 Mid-Atlantic Harmful Algal Bloom Indicator Description: An aggregation of reported algal bloom data in Chesapeake Bay between 2007-2017. Found in: State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Database pull Contributor(s): Sean Hardison, Virginia Department of Health Data steward: Sean Hardison, sean.hardison@noaa.gov Point of contact: Sean Hardison, sean.hardison@noaa.gov Public availability statement: Source data for this indicator are available here. Processed time series can be found here. 19.1 Methods We presented two indicator time series for reports of algal blooms in the southern portion of Chesapeake Bay between 2007-2017. The first indicator was observations of algal blooms above 5000 cell ml-1. This threshold was developed by the Virginia Department of Health (VDH) for Microcystis spp. algal blooms based on World Health Organization guidelines (Organization 2003; Health 2011). VDH also uses this same threshold for other algal species blooms in Virginia waters. When cell concentrations are above 5000 cell ml-1, VDH recommends initiation of biweekly water sampling and that relevant local agencies be notified of the elevated cell concentrations. The second indicator we reported, blooms of Cochlodinium polykrikoides at cell concentrations &gt;300 cell ml-1, was chosen due to reports of high ichthyotoxicity seen at these levels. Tang and Gobler (2009) showed that fish exposed to cultured C. polykrikoides at densities as low 330 cells ml-1 saw 100% mortality within 1 hour, which if often far less than C. polykrikoides cell concentrations seen in the field. Algal bloom data were not available for 2015 nor 2010. The algal bloom information presented here are a synthesis of reported events, and has been updated to include data not presented in the 2018 State of the Ecosystem Report. 19.1.1 Data sources Source data were obtained from VDH. Sampling, identification, and bloom characterization was completed by the VDH, Phytoplankton Analysis Laboratory at Old Dominion University, Reece Lab at the Virginia Institute of Marine Science, and Virginia Department of Environmental Quality. Problem algal species were targeted for identification via light microscopy followed by standard or quantitative PCR assays and/or enzyme-linked immunosorbent assay (ELISA). Reports specifying full methodologies from ODU, VIMS, and VDH source data are available upon request. 19.1.2 Data extraction Data were extracted from a series of spreadsheets provided by the VDH. We quantified the number of algal blooms in each year reaching target cell density thresholds in the southern Chesapeake Bay. #SOE R packages library(readxl) library(dplyr) library(tidyr) library(stringr) data.dir &lt;- &quot;data/CB_HAB&quot; #Function to process data - cpm specifies cells per ml filter fixer &lt;- function(cpm){ hab_2007_2012 &lt;- read_excel(file.path(data.dir,&quot;Query_2007-2012.xlsx&quot;)) hab_2013_odu &lt;- read_excel(file.path(data.dir,&quot;2013 ODU Data.xlsx&quot;),skip = 4) hab_2013_vims &lt;- read_excel(file.path(data.dir,&quot;vims_2013.xlsx&quot;),skip = 6) hab_2014_odu &lt;- read_excel(file.path(data.dir,&quot;2014 ODU data.xlsx&quot;)) hab_2014_vims &lt;- read_excel(file.path(data.dir,&quot;FINALforVDH_22Dec14final.xlsx&quot;)) hab_2016 &lt;- read_excel(file.path(data.dir,&quot;HAB_MAP_Data_2016.xlsx&quot;)) hab_2017 &lt;- read_excel(file.path(data.dir,&quot;HAB_MAP_Data_2017.xlsx&quot;),sheet=2) #2012--------------------------------------------------------- HAB_2007_2012 &lt;- hab_2007_2012 %&gt;% filter(!is.na(cells_per_ml)) %&gt;% filter(!is.na(date)) %&gt;% mutate(year = format(as.POSIXct(date), &quot;%Y&quot;)) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% group_by(year, species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() #2013--------------------------------------------------------- #ODU odu_2013 &lt;- gather(hab_2013_odu, species, cells_per_ml, `Pfiesteria like dinoflagellate`:`A. monilatum`) %&gt;% filter(!is.na(cells_per_ml)) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% mutate(year = 2013) %&gt;% group_by(year, species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() #VIMS vims_2013 &lt;- hab_2013_vims %&gt;% filter(!is.na(cells_per_ml)) %&gt;% mutate(year = &quot;2013&quot;) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% group_by(year, species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() HAB_2013 &lt;- rbind(vims_2013, odu_2013) #2014-------------------------------------------------------- #ODU long &lt;- gather(hab_2014_odu, species, cells_per_ml, `Karlodinium veneficum`:`Cyanobacteria bloom`, factor_key = TRUE) hab_2014_odu &lt;- long %&gt;% filter(cells_per_ml != 0) hab_2014_odu$species &lt;- sub(&quot;[.]&quot;,&quot; &quot;, hab_2014_odu$species) hab_2014_odu$cells_per_ml &lt;- gsub(&quot;[A-Za-z+//]&quot;,&#39;&#39;,hab_2014_odu$cells_per_ml) hab_2014_odu$cells_per_ml &lt;- as.numeric(hab_2014_odu$cells_per_ml) hab_2014_odu &lt;- hab_2014_odu %&gt;% mutate(year = &quot;2014&quot;) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% group_by(year,species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() #VIMS long &lt;- gather(hab_2014_vims, species, cells_per_ml, `A. monilatum`:`C. subsalsa`) hab_2014_vims &lt;- long %&gt;% mutate(year = &quot;2014&quot;) %&gt;% filter(!is.na(cells_per_ml)) %&gt;% mutate(cells_per_ml = as.numeric(cells_per_ml)) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% group_by(year,species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() HAB_2014 &lt;- rbind(hab_2014_odu, hab_2014_vims) #2015---------------------------------------------------------- #No data #2016--------------------------------------------------------- HAB_2016 &lt;- hab_2016 %&gt;% mutate(species= plyr::mapvalues(species, from = c(&quot;Eugelna sanguinea&quot;, &quot;Microcystin aeruginosa&quot;, &quot;Microcystis aeruginosa&quot;, &quot;Alexandrium monilatum-likely&quot;, &quot;Alexandrium monilatum&quot;), to = c(&quot;Eugelena spp.&quot;, &quot;Microcystis spp.&quot;, &quot;Microcystis spp.&quot;, &quot;Alexandrium spp.&quot;, &quot;Alexandrium spp.&quot;))) HAB_2016$cells_per_ml &lt;- gsub(&#39;[a-zA-Z+&lt;&gt;]&#39;,&#39;&#39;,HAB_2016$cells_per_ml) HAB_2016 &lt;- HAB_2016 %&gt;% filter(!is.na(cells_per_ml)) %&gt;% mutate(year = 2016, cells_per_ml = as.numeric(cells_per_ml)) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% group_by(year, species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() #2017------------------------------------------------------------ hab_2017$species = str_trim(hab_2017$species) HAB_2017 &lt;- hab_2017 %&gt;% mutate(species = plyr::mapvalues(species, c(&quot;A. monilatum&quot;,&quot;Anabaena sp&quot;, &quot;Anabaena sp.&quot;,&quot;Anabaena spp&quot;, &quot;none&quot;,&quot;NO HABs&quot;,&quot;C. polykrikoides&quot;, &quot;Microcystis aeurignosa&quot;,&quot;Cylindrospermopsis sp&quot;), c(&quot;Alexandrium monilatum&quot;, &quot;Anabaena spp.&quot;, &quot;Anabaena spp.&quot;,&quot;Anabaena spp.&quot;, &quot;NA&quot;,&quot;NA&quot;,&quot;Cochlodinium polykrikoides&quot;, &quot;Microcystis aeruginosa&quot;,&quot;Cylindrospermopsis sp.&quot;))) HAB_2017$cells_per_ml &lt;- gsub(&quot;[a-zA-Z+/]&quot;,&#39;&#39;,HAB_2017$cells_per_ml) HAB_2017$cells_per_ml &lt;- str_trim(HAB_2017$cells_per_ml) HAB_2017$cells_per_ml &lt;- as.numeric(HAB_2017$cells_per_ml) HAB_2017 &lt;- HAB_2017 %&gt;% filter(!is.na(cells_per_ml)) %&gt;% mutate(year = &quot;2017&quot;) %&gt;% filter(cells_per_ml &gt;= cpm) %&gt;% group_by(year, species) %&gt;% dplyr::summarise(Events = n()) %&gt;% as.data.frame() #Aggregate-------------------------------------------------------- ts &lt;- rbind(HAB_2007_2012, HAB_2013, HAB_2014, HAB_2016, HAB_2017) return(ts) } #All blooms &gt; 5000 cells ml^-1 full &lt;- fixer(cpm = 5000) full &lt;- full %&gt;% group_by(year) %&gt;% dplyr::summarise(total = sum(Events)) plot(full$year, full$total, type = &quot;o&quot;, ylim = c(0,90), pch = 20, ylab = &quot;Bloom Events&quot;, las = 1, xlab = &quot;Time&quot;, lwd = 2) #cochlodinum &gt; 300 cells ml^-1 cochlo &lt;- fixer(cpm = 300) cochlo[cochlo$species == &quot;C. polykrikoides&quot; | cochlo$species == &quot;C.polykrikoides&quot; ,]$species &lt;- &quot;Cochlodinium polykrikoides&quot; cochlo &lt;- cochlo[cochlo$species == &quot;Cochlodinium polykrikoides&quot;,] cochlo &lt;- cochlo %&gt;% group_by(year) %&gt;% dplyr::summarise(total = sum(Events)) points(cochlo$year, cochlo$total, type = &quot;o&quot;, pch = 20, col = &quot;indianred&quot;, lwd = 2) legend(x = 2007, y = 80, legend = c(expression(paste(&quot;All reports &gt;5000 cells ml&quot;^&quot;-1&quot;)), expression(paste(italic(&quot;C. polykrikoides &quot;),&quot;reports &gt;300 cells ml&quot;^&quot;-1&quot;))), col = c(&quot;black&quot;,&quot;indianred&quot;), lwd = 2, bty = &quot;n&quot;) Figure 19.1: All reported algal blooms &gt;5000 cells ml -1 (black), and reports of C. polykrikoides blooms &gt;300 cells ml -1 (red) between 2007-2017. 19.1.3 Data analysis No data analysis steps took place for this indicator. References "],
["new-england-harmful-algal-bloom-indicator.html", "20 New England Harmful Algal Bloom Indicator 20.1 Methods", " 20 New England Harmful Algal Bloom Indicator Description: Regional incidence of shellfish bed closures due to presence of toxins associated with harmful algae. Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018) Indicator Category: Synthesis of published information Contributor(s): Dave Kulis, Donald M Anderson, Sean Hardison Data steward: Sean Hardison, sean.hardison@noaa.gov Point of contact: Sean Hardison, sean.hardison@noaa.gov Public availability statement: Data are publicly available (see Data Sources below). 20.1 Methods The New England Harmful Algal Bloom (HAB) indicator is a synthesis of shellfish bed closures related to the presence of HAB-associated toxins above threshold levels from 2007-2016 (Figure ??). Standard detection methods were used to identify the presence of toxins associated with Amnesic Shellfish Poisoning (ASP), Paralytic Shellfish Poisoning (PSP), and Diarrhetic Shellfish Poisoning (DSP) by state and federal laboratories. 20.1.0.1 Paralytic Shellfish Poisoning The most common cause of shellfish bed closures in New England is the presence of paralytic shellfish toxins (PSTs) produced by the dinoflagellate Alexandrium catenella. All New England states except Maine relied on the AOAC-approved mouse bioassay method to detect PSTs in shellfish during the 2007-2016 period reported here (International 2005). In Maine, PST detection methods were updated in May 2014 when the state adopted the hydrophilic interaction liquid chromatography (HILIC) UPLC-MS/MS protocol (Boundy et al. 2015) in concordance with National Shellfish Sanitation Program (NSSP) requirements. Prior to this, the primary method used to detect PST in Maine was with the mouse bioassay. 20.1.0.2 Amnesic Shellfish Poisoning Amnesic shellfish poisoning (ASP) is caused by the toxin domoic acid (DA), which is produced by several phytoplankton species belonging to the genus Pseudo-nitzchia. In New England, a UV-HPLC method (Quilliam, Xie, and Hardstaff 1995), which specifies a HPLC-UV protocol, is used for ASP detection. 20.1.0.3 Diarrhetic Shellfish Poisoning Diarrhetic Shellfish Poisoning (DSP) is rare in New England waters, but the presence of the DSP-associated okadaic acid (OA) in mussels was confirmed in Massachusetts in 2015 (J. Deeds, personal communication, July 7, 2018). Preliminary testing for OA in Massachusetts utilized the commercially available Protein Phosphatase Inhibition Assay (PPIA) and these results are confirmed through LC-MS/MS when necessary (Smienk et al. 2012; Stutts and Deeds 2017). 20.1.1 Data sources Data used in this indicator were drawn from the 2017 Report on the ICES-IOC Working Group on Harmful Algal Bloom Dynamics (WGHABD). The report and data are available here. Closure information was collated from information provided by the following organizations: Table 20.1: Shellfish closure information providers. State Source Organization Maine Maine Department of Marine Resources New Hampshire New Hampshire Department of Environmental Services Massachusetts Massachusetts Division of Marine Fisheries Rhode Island Rhode Island Department of Environmental Management Connecticut Connecticut Department of Agriculture 20.1.2 Data extraction Data were extracted from the original report visually and accuracy confirmed with report authors. 20.1.3 Data analysis No data analysis steps took place for this indicator. The script used to develop the figure in the SOE report is below. Figure 20.1: Regional HAB related shellfish bed closures in New England between 2007 and 2016. References "],
["verified-records-of-southern-kingfish.html", "21 Verified Records of Southern Kingfish 21.1 Methods", " 21 Verified Records of Southern Kingfish Description: Fisheries Observer Data – Verified Records of Southern Kingfish Found in: State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Database pull Contributor(s): Debra Duarte, Loren Kellogg Data steward: Gina Shield, gina.shield@noaa.gov Point of contact: Gina Shield, gina.shield@noaa.gov Public availability statement: Due to PII concerns data for this indicator are not publicly available. 21.1 Methods 21.1.1 Data sources The Fisheries Sampling Branch deploys observers on commercial fisheries trips from Maine to North Carolina. On observed tows, observers must fully document all kept and discarded species encountered. Observers must comply with a Species Verification Program (SVP), which requires photo or sample submissions of high priority species at least once per quarter. Photos and samples submitted for verification are identified independently by at least two reviewers. The derived data presented in the Mid-Atlantic State of the Ecosystem report for southern kingfish include records verified by the SVP program only. The occurrence of southern kingfish in SVP records were chosen for inclusion in the report due to the recent increases of the species in SVP observer records since 2010. These data are not a complete list from the New England Fisheries Observer Program (NEFOP). Southern Kingfish are less common than Northern Kingfish in observer data and are possibly misidentified so we have initially included records here only when a specimen record was submitted to and verified through the SVP (see Data extraction). 21.1.2 Data extraction SQL query: /* training trips, 900 trips, incidental takes, most duplicate records (unless different idmethod or idcomment) are not included */ /* this is a general script to pull the data on any species from the SVP*/ /* specimens submitted as photos and samples included separately */ /* nefop_ims.species_verification query that approximates is: select * from nefop_ims.species_verification where SUBJECT_CODE &lt;&gt; &#39;01&#39; and YEAR &gt; &#39;2009&#39; and species = &#39;6617&#39; order by species, year, tripid, haulnum, idmethod but this could includes training trips, duplicates, 900 trips*/ SELECT ss.year YEAR, MONTH, ss.obid as OBSID, ss.tripid TRIPID, NEGEAR, DATELAND, SS.HAUL HAUL, CODE, SPECIES, correct, INCCODE, INCORRECTSPP, idmethod, GIS_LATSBEG, GIS_LONSBEG, GIS_LATSEND, GIS_LONSEND, GIS_LATHBEG, GIS_LONHBEG, GIS_LATHEND, GIS_LONHEND, rr.QTR QTR, rr.link1 link1, rr.link3 LINK3, idcomments from (SELECT substr(c.tripid, 1,3) obid, c.tripid, c.year, HAUL, TO_CHAR(t.dateland, &#39;DD-MON-YY&#39;) as DATELAND, CODE, SPECIES, correct, INCORRECTSPP as INCCODE, comname as INCORRECTSPP, idmethod, idcomments FROM (select id_num, a.tripid, a.year, lpad(CAST(haulnum as varchar2(4 byte)),4,&#39;0&#39;) haul, inc, dateverify, species as CODE, comname as SPECIES, correct, incorrectspp, idmethod, idcomments, obs_contacted, filelocation from nefop_ims.species_verification a, obdbs.obspec b where a.species=b.nespp4) c left join obdbs.obspec d on c.incorrectspp=d.nespp4 join OBSCON.OBSCON_TRIPS_FSB t on c.year||c.tripid=t.year||substr(t.LINK1,10,6) where c.YEAR like &#39;%%&#39; and c.TRIPID like &#39;%%&#39; and c.inc is null and DATELAND between to_date(&#39;01-JAN-10&#39;, &#39;DD-MON-RR&#39;) and to_date(&#39;31-DEC-20&#39;,&#39;DD-MON-RR&#39;) UNION SELECT substr(g.tripid,1,3) obid, g.tripid, g.year, HAUL, TO_CHAR(s.dateland, &#39;DD-MON-YY&#39;) as DATELAND, CODE, SPECIES, correct, INCORRECTSPP as INCCODE, comname as INCORRECTSPP, idmethod, idcomments FROM ((select id_num, e.tripid, e.year, lpad(CAST(haulnum as varchar2(4 byte)),4,&#39;0&#39;) haul, inc, dateverify, species as CODE, comname as SPECIES, correct, incorrectspp, idmethod, idcomments, obs_contacted, filelocation from nefop_ims.species_verification e, obdbs.obspec f where e.species=f.nespp4) g left join obdbs.obspec h on g.incorrectspp=h.nespp4 join obdbs.asmtrp_entry s on g.year||g.tripid=s.yearland||s.tripid) where g.YEAR like &#39;%%&#39; and g.TRIPID like &#39;%%&#39; and g.inc is null and DATELAND between to_date(&#39;01-JAN-10&#39;, &#39;DD-MON-RR&#39;) and to_date(&#39;31-DEC-20&#39;,&#39;DD-MON-RR&#39;) UNION SELECT substr(r.tripid, 1,3) obid, r.tripid, r.year, HAUL, TO_CHAR(s.dateland, &#39;DD-MON-YY&#39;) as DATELAND, CODE, SPECIES, correct, INCORRECTSPP as INCCODE, comname as INCORRECTSPP, idmethod, idcomments FROM ((select id_num, a.tripid, a.year, lpad(CAST(haulnum as varchar2(4 byte)),4,&#39;0&#39;) haul, inc, dateverify, species as CODE, comname as SPECIES, correct, incorrectspp, idmethod, idcomments, obs_contacted, filelocation from nefop_ims.species_verification a, obdbs.obspec b where a.species=b.nespp4) r left join obdbs.obspec d on r.incorrectspp=d.nespp4 join OBPRELIM.TRP_BASE s on r.year||r.tripid=s.yearland||S.TRIPID) where r.YEAR like &#39;%%&#39; and r.TRIPID like &#39;%%&#39; and r.inc is null and DATELAND between to_date(&#39;01-JAN-10&#39;, &#39;DD-MON-RR&#39;) and to_date(&#39;31-DEC-20&#39;,&#39;DD-MON-RR&#39;)) ss left join (/* OBDBS OBHAU */ select substr(link3,1,15) LINK1 , LINK3, substr(link3,1,3) PROGRAM , substr(link3,4,4) YEAR , substr(link3,8,2) MONTH , substr(link3,10,6) TRIPID , substr(link3,16,4) HAULNUM , decode(substr(link3,8,2),&#39;01&#39;,1,&#39;02&#39;,1,&#39;03&#39;,1,&#39;04&#39;,2, &#39;05&#39;,2,&#39;06&#39;,2,&#39;07&#39;,3,&#39;08&#39;,3,&#39;09&#39;,3,&#39;10&#39;,4,&#39;11&#39;,4,&#39;12&#39;,4,null) QTR, NEGEAR, OBSRFLAG, CATEXIST, case when yearhbeg is not null and monthhbeg is not null and dayhbeg is not null then to_date(ltrim(rtrim(YEARHBEG))||ltrim(rtrim(MONTHHBEG))||ltrim(rtrim(DAYHBEG))||ltrim(rtrim(TIMEHBEG)),&#39;YYYYMMDDHH24MI&#39;) else null end datehbeg, case when yearhend is not null and monthend is not null and dayhend is not null then to_date(ltrim(rtrim(YEARHEND))||ltrim(rtrim(MONTHEND))||ltrim(rtrim(DAYHEND))||ltrim(rtrim(TIMEHEND)),&#39;YYYYMMDDHH24MI&#39;) else null end datehend, LATHBEG, LONHBEG, LATHEND, LONHEND, cast( substr(latsbeg,1,2) + substr(latsbeg,3,2) / 60 + substr(latsbeg,5,2) / 3600 as number(8,6)) GIS_LATSBEG , -1 * cast(substr(lonsbeg,1,2) + substr(lonsbeg,3,2) / 60 + substr(lonsbeg,5,2) / 3600 as number(8,6)) GIS_LONSBEG , cast(substr(latsend,1,2) + substr(latsend,3,2) / 60 + substr(latsend,5,2) / 3600 as number(8,6)) GIS_LATSEND, -1 * cast(substr(lonsend,1,2) + substr(lonsend,3,2) / 60 + substr(lonsend,5,2) / 3600 as number(8,6)) GIS_LONSEND, cast(substr(lathbeg,1,2) + substr(lathbeg,3,2) / 60 + substr(lathbeg,5,2) / 3600 as number(8,6)) GIS_LATHBEG, -1 * cast(substr(lonhbeg,1,2) + substr(lonhbeg,3,2) / 60 + substr(lonhbeg,5,2) / 3600 as number(8,6)) GIS_LONHBEG, cast(substr(lathend,1,2) + substr(lathend,3,2) / 60 + substr(lathend,5,2) / 3600 as number(8,6)) GIS_LATHEND, -1 * cast(substr(lonhend,1,2) + substr(lonhend,3,2) / 60 + substr(lonhend,5,2) / 3600 as number(8,6)) GIS_LONHEND, AREA, HAUCOMMENTS from obdbs.obhau_base UNION select substr(link3,1,15) LINK1 , LINK3, substr(link3,1,3) PROGRAM , substr(link3,4,4) YEAR , substr(link3,8,2) MONTH , substr(link3,10,6) TRIPID , substr(link3,16,4) HAULNUM , decode(substr(link3,8,2),&#39;01&#39;,1,&#39;02&#39;,1,&#39;03&#39;,1,&#39;04&#39;,2, &#39;05&#39;,2,&#39;06&#39;,2,&#39;07&#39;,3,&#39;08&#39;,3,&#39;09&#39;,3,&#39;10&#39;,4,&#39;11&#39;,4,&#39;12&#39;,4,null) QTR, NEGEAR, OBSRFLAG, CATEXIST, case when yearhbeg is not null and monthhbeg is not null and dayhbeg is not null then to_date(ltrim(rtrim(YEARHBEG))||ltrim(rtrim(MONTHHBEG))||ltrim(rtrim(DAYHBEG))||ltrim(rtrim(TIMEHBEG)),&#39;YYYYMMDDHH24MI&#39;) else null end datehbeg, case when yearhend is not null and monthend is not null and dayhend is not null then to_date(ltrim(rtrim(YEARHEND))||ltrim(rtrim(MONTHEND))||ltrim(rtrim(DAYHEND))||ltrim(rtrim(TIMEHEND)),&#39;YYYYMMDDHH24MI&#39;) else null end datehend, LATHBEG, LONHBEG, LATHEND, LONHEND, cast( substr(latsbeg,1,2) + substr(latsbeg,3,2) / 60 + substr(latsbeg,5,2) / 3600 as number(8,6)) GIS_LATSBEG , -1 * cast(substr(lonsbeg,1,2) + substr(lonsbeg,3,2) / 60 + substr(lonsbeg,5,2) / 3600 as number(8,6)) GIS_LONSBEG , cast(substr(latsend,1,2) + substr(latsend,3,2) / 60 + substr(latsend,5,2) / 3600 as number(8,6)) GIS_LATSEND, -1 * cast(substr(lonsend,1,2) + substr(lonsend,3,2) / 60 + substr(lonsend,5,2) / 3600 as number(8,6)) GIS_LONSEND, cast(substr(lathbeg,1,2) + substr(lathbeg,3,2) / 60 + substr(lathbeg,5,2) / 3600 as number(8,6)) GIS_LATHBEG, -1 * cast(substr(lonhbeg,1,2) + substr(lonhbeg,3,2) / 60 + substr(lonhbeg,5,2) / 3600 as number(8,6)) GIS_LONHBEG, cast(substr(lathend,1,2) + substr(lathend,3,2) / 60 + substr(lathend,5,2) / 3600 as number(8,6)) GIS_LATHEND, -1 * cast(substr(lonhend,1,2) + substr(lonhend,3,2) / 60 + substr(lonhend,5,2) / 3600 as number(8,6)) GIS_LONHEND, AREA, HAUCOMMENTS from obv10.obhau_base UNION /* ASM HAU */ select substr(link3,1,15) LINK1 , LINK3, substr(link3,1,3) PROGRAM , substr(link3,4,4) YEAR , substr(link3,8,2) MONTH , substr(link3,10,6) TRIPID , substr(link3,16,4) HAULNUM , decode(substr(link3,8,2),&#39;01&#39;,1,&#39;02&#39;,1,&#39;03&#39;,1,&#39;04&#39;,2, &#39;05&#39;,2,&#39;06&#39;,2,&#39;07&#39;,3,&#39;08&#39;,3,&#39;09&#39;,3,&#39;10&#39;,4,&#39;11&#39;,4,&#39;12&#39;,4,null) QTR, NEGEAR, OBSRFLAG, cast(null as varchar2(1)) CATEXIST, to_date(ltrim(rtrim(YEARHBEG))||ltrim(rtrim(MONTHHBEG))||ltrim(rtrim(DAYHBEG))||ltrim(rtrim(TIMEHBEG)),&#39;YYYYMMDDHH24MI&#39;)datehbeg, to_date(ltrim(rtrim(YEARHEND))||ltrim(rtrim(MONTHEND))||ltrim(rtrim(DAYHEND))||ltrim(rtrim(TIMEHEND)),&#39;YYYYMMDDHH24MI&#39;)datehend, LATHBEG, LONHBEG, LATHEND, LONHEND, cast(null as number(8,6)) GIS_LATSBEG , cast(null as number) GIS_LONSBEG , cast(null as number(8,6)) GIS_LATSEND, cast(null as number) GIS_LONSEND, cast(substr(lathbeg,1,2) + substr(lathbeg,3,2) / 60 + substr(lathbeg,5,2) / 3600 as number(8,6)) GIS_LATHBEG, -1 * cast(substr(lonhbeg,1,2) + substr(lonhbeg,3,2) / 60 + substr(lonhbeg,5,2) / 3600 as number(8,6)) GIS_LONHBEG, cast(substr(lathend,1,2) + substr(lathend,3,2) / 60 + substr(lathend,5,2) / 3600 as number(8,6)) GIS_LATHEND, -1 * cast(substr(lonhend,1,2) + substr(lonhend,3,2) / 60 + substr(lonhend,5,2) / 3600 as number(8,6)) GIS_LONHEND, AREA, HAUCOMMENTS from asmhau_prelim p left outer join obtriptrack t on substr(p.link3,1,15) = t.link1 where (substr(link3,4,6) between &#39;201005&#39; and &#39;201012&#39; and substr(link3,1,3) between &#39;230&#39; and &#39;234&#39;) or (substr(link3,4,6) &gt; &#39;201012&#39; and substr(link3,1,3) &lt;&gt; &#39;900&#39;) and substr(link3,1,15) not in (select link1 from obv10.obtrp_base) ) rr on ss.YEAR||ss.TRIPID||ss.HAUL = rr.YEAR||rr.TRIPID||ltrim(rr.HAULNUM) where ss.OBID like &#39;%%&#39; and ss.TRIPID like &#39;%%&#39; and (CODE like &#39;%6617%&#39;) and HAUL like &#39;%%&#39; and SPECIES like &#39;%%&#39; and IDCOMMENTS like &#39;%%&#39; ORDER BY SPECIES, YEAR, TRIPID, HAUL ; 21.1.3 Data analysis Time series were summed by year and plotted, and mapped data for individual records were plotted according to the location where gear was hauled. As coordinate data were not always available for each record, the map does not include all occurrences of southern kingfish, but was included for spatial context. R code use to aggregate data: library(dplyr) total_by_year&lt;- source %&gt;% filter(SPECIES == &quot;KINGFISH, SOUTHERN&quot;) %&gt;% #Filter species group_by(YEAR) %&gt;% dplyr::summarise(n = n()) #Sum observations per year 21.1.4 Plotting #For map and plot. Latitude and longitude data for this figure are not publicly available. sk.dat &lt;- SOE.data[grepl(&quot;Lat&quot;,SOE.data$Units) &amp; grepl(&quot;Southern Kingfish&quot;,SOE.data$Var),] lon &lt;- sk.dat[as.numeric(sk.dat$Value) &lt; 0,]$Value lat &lt;- sk.dat[as.numeric(sk.dat$Value) &gt; 0,]$Value #create data.frame df &lt;- data.frame(year = sk.dat$Time, lon = lon, lat = lat) #set color palette colors1 &lt;- adjustcolor(matlab.like2(8),.5) colors2 &lt;- adjustcolor(matlab.like2(8),.5) colors3 &lt;- adjustcolor(matlab.like2(8),.5) colors4 &lt;- adjustcolor(matlab.like2(8),1) colors &lt;- c(colors1[1:2], colors2[3:4], colors3[5:6],colors4[7:8]) #map values to colors df &lt;- df %&gt;% arrange(year) %&gt;% mutate(colors = plyr::mapvalues(year, from = c(&quot;2010&quot;,&quot;2011&quot;,&quot;2012&quot;,&quot;2013&quot;, &quot;2014&quot;,&quot;2015&quot;,&quot;2016&quot;,&quot;2017&quot;), to = c(colors))) colors &lt;- df$colors #projection map.crs &lt;- CRS(&quot;+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0 +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot;) #data.frame to sp object coordinates(df) &lt;- ~lon+lat df@proj4string &lt;- map.crs df &lt;- spTransform(df, map.crs) #plot par(fig = c(0,1,0,1)) plot(coast, xlim = c(-76,-73), ylim = c(35,40.5),col = &quot;grey&quot;) plot(df, pch = 16, col = colors, add = T, cex = 2.5) occur &lt;- SOE.data[SOE.data$Var == &quot;Southern Kingfish observer sightings&quot;,]$Value time &lt;- SOE.data[SOE.data$Var == &quot;Southern Kingfish observer sightings&quot;,]$Time ts &lt;- zoo(occur,time) par(fig = c(0.5,1, 0.1, .5), new = T, bty = &quot;l&quot;,mar = c(5,6,3,1)) barplot(occur,time, col = matlab.like2(8), xlab = c(&quot;Time&quot;),ylab = &quot;S. Kingfish Occurrence, n&quot;, cex.lab = 1, las = 1, cex.axis = 1) axis(1,at = seq(1250,18500,length.out = 8),labels = c(&quot;2010&quot;,&quot;2011&quot;,&quot;2012&quot;,&quot;2013&quot;, &quot;2014&quot;,&quot;2015&quot;,&quot;2016&quot;,&quot;2017&quot;), cex.axis=1) Figure 21.1: Verified records of Southern Kingfish occurrence in the Mid-Atlantic. "],
["habitat-occupancy-models.html", "22 Habitat Occupancy Models 22.1 Methods", " 22 Habitat Occupancy Models Description: Habitat Occupancy Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Database pull with analysis; Extensive analysis; not yet published; Published methods Contributor(s): Kevin Friedland Data steward: Kevin Friedland, kevin.friedland@noaa.gov Point of contact: Kevin Friedland, kevin.friedland@noaa.gov Public availability statement: Source data are available upon request (see Survdat, CHL/PP, and Data Sources below for more information). Model-derived time series are available here. 22.1 Methods Habitat area with a probability of occupancy greater than 0.5 was modeled for many species throughout the NE-LME using random forest decision tree models. 22.1.1 Data sources Models were parameterized using a suite of static and dynamic predictor variables, with occurrence and catch per unit effort (CPUE) of species from spring and fall Northeast Fisheries Science Center (NEFSC) bottom trawl surveys (BTS) serving as response variables. Sources of variables used in the analyses are described below. 22.1.1.1 Station depth The NEFSC BTS data included depth observations made concurrently with trawls at each station. Station depth was a static variable for these analyses. 22.1.1.2 Ocean temperature and salinity Sea surface and bottom water temperature and salinity measurements were included as dynamic predictor variables in the model, and were collected using Conductivity/Temperature/Depth (CTD) instruments. Ocean temperature and salinity measurements had the highest temporal coverage during the spring (February-April) and fall (September-November) months. Station salinity data were available between 1992-2016. 22.1.1.3 Habitat descriptors A variety of benthic habitat descriptors were incorporated as predictor variables in occupancy models (Table 22.1). The majority of these parameters are based on depth (e.g. BPI, VRM, Prcury, rugosity, seabedforms, slp, and slpslp). The vorticity variable is based on current estimates, and the variable soft_sed based on sediment grain size. Table 22.1: Habitat descriptors used in model parameterization. Variables Notes References Namera_vrm Vector Ruggedness Measure (VRM) measures terrain ruggedness as the variation in three-dimensional orientation of grid cells within a neighborhood based the TNC Northwest Atlantic Marine Ecoregional Assessment (“NAMERA”) data. Hobson (1972); Sappington, Longshore, and Thompson (2007) Prcurv (2 km, 10 km, and 20 km) Benthic profile curvature at 2km, 10km and 20 km spatial scales was derived from depth data. Winship et al. (2018) Rugosity A measure of small-scale variations of amplitude in the height of a surface, the ratio of the real to the geometric surface area. Friedman et al. (2012) seabedforms Seabed topography as measured by a combination of seabed position and slope. http://www.northeastoceandata.org/ Slp (2 km, 10 km, and 20 km) Benthic slope at 2km, 10km and 20km spatial scales. Winship et al. (2018) Slpslp (2 km, 10 km, and 20 km) Benthic slope of slope at 2km, 10km and 20km spatial scales Winship et al. (2018) soft_sed Soft-sediments is based on grain size distribution from the USGS usSeabed: Atlantic coast offshore surficial sediment data. http://www.northeastoceandata.org/ Vort (fall - fa; spring - sp; summer - su; winter - wi) Benthic current vorticity at a 1/6 degree (approx. 19 km) spatial scale. Kinlan et al. (2016) 22.1.1.4 Zooplankton Zooplankton data are acquired through the NEFSC Ecosystem Monitoring Program (“EcoMon”). For more information regarding the collection process for these data, see Kane (2007), Kane (2011), and Morse et al. (2017). The bio-volume of the 18 most abundant zooplankton taxa were considered as potential predictor variables. 22.1.1.5 Remote sensing data Both chlorophyll concentration and SST from remote sensing sources were incorporated as static predictor variables in the model. During the period of 1997-2016, chlorophyll concentrations were derived from observations made by the Sea-viewing Wide Field of View Sensor (SeaWIFS), Moderate Resolution Imaging Spectroradiometer (MODIS-Aqua), Medium Resolution Imaging Spectrometer (MERIS), and Visible and Infrared Imaging/Radiometer Suite (VIIRS). 22.1.2 Data processing 22.1.2.1 Zooplankton Missing values in the EcoMon time series were addressed by summing data over five-year time steps for each seasonal time frame and interpolating a complete field using ordinary kriging. Missing values necessitated interpolation for spring data in 1989, 1990, 1991, and 1994. The same was true of the fall data for 1989, 1990, and 1992. 22.1.2.2 Remote sensing data An overlapping time series of observations from the four sensors listed above was created using a bio-optical model inversion algorithm (Maritorena et al. 2010). Monthly SST data were derived from MODIS-Terra sensor data (available here). 22.1.2.3 Ocean temperature and salinity Date of collection corrections for ocean temperature data were developed using linear regressions for the spring and fall time frames; standardizing to collection dates of April 3 and October 11 for spring and fall. No correction was performed for salinity data. Annual data for ocean temperature and salinity were combined with climatology by season through an optimal interpolation approach. Specifically, mean bottom temperature or salinity was calculated by year and season on a 0.5° grid across the ecosystem, and data from grid cells with &gt;80% temporal coverage were used to calculate a final seasonal mean. Annual seasonal means were then used to calculate combined anomalies for seasonal surface and bottom climatologies. An annual field was then estimated using raw data observations for a year, season, and depth using universal kriging (Hiemstra et al. 2008), with depth included as a covariate (on a standard 0.1° grid). This field was then combined with the climatology anomaly field and adjusted by the annual mean using the variance field from kriging as the basis for a weighted mean between the two. The variance field was divided into quartiles with the lowest quartile assigned a weighting of 4:1 between the annual and climatology values. The optimally interpolated field at these locations was therefore skewed towards the annual data, reflecting their proximity to actual data locations and associated low kriging variance. The highest kriging variance quartile (1:1) reflected less information from the annual field and more from the climatology. 22.1.3 Data analysis 22.1.3.1 Occupancy models Prior to fitting the occupancy models, predictor variables were tested for multi-collinearity and removed if found to be correlated. The final model variables were then chosen utilizing a model selection process as shown by Murphy, Evans, and Storfer (2010) and implemented with the R package rfUtilities (Evans and Murphy 2018). Occupancy models were then fit as two-factor classification models (absence as 0 and presence as 1) using the randomForest R package (Liaw and Wiener 2002). 22.1.3.2 Selection criteria and variable importance The irr R package (Gamer, Lemon, and Singh 2012) was used to calculate Area Under the ROC Curve (AUC) and Cohen’s Kappa for assessing accuracy of occupancy habitat models. Variable importance was assessed by plotting the occurrence of a variable as a root variable versus the mean minimum node depth for the variable (Paluszynska and Biecek 2017), as well as by plotting the Gini index decrease versus accuracy decrease. 22.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4)) soe.plot(SOE.data, &quot;Time&quot;, &quot;sumflo spring habitat occupancy&quot;, stacked = &quot;A&quot;, rel.y.num = 1.1, scale.axis = 10^3, end.start = 2007, full.trend = F, cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;sumflo fall habitat occupancy&quot;, stacked = &quot;B&quot;, rel.y.num = 1.1, scale.axis = 10^3, end.start = 2007, full.trend = F, cex.stacked = 1.5) soe.stacked.axis(&quot;Year&quot;, expression(&quot;Habitat Area, 10&quot;^3*&quot; km&quot;^2), y.line = 2.5) Figure 22.1: Summer flounder spring (A) and fall (B) occupancy habitat area in the Northeast Large Marine Ecosystem. References "],
["fish-productivity-indicator.html", "23 Fish Productivity Indicator 23.1 Methods", " 23 Fish Productivity Indicator Description: Groundfish productivity estimated as the ratio of small fish to large fish Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Database pull with analysis; Published methods Contributor(s): Charles Perretti Data steward: Charles Perretti, charles.perretti@noaa.gov Point of contact: Charles Perretti, charles.perretti@noaa.gov Public availability statement: Source data are available upon request. 23.1 Methods 23.1.1 Data sources Survey data from NEFSC trawl database. These data in their derived form are available through Survdat. 23.1.2 Data extraction Data were extracted from Survdat. 23.1.3 Data analysis We defined size thresholds separating small and large fish for each species based on the 20th percentile of the length distribution across all years. This threshold was then used to calculate a small and large fish index (numbers below and above the threshold, respectively) each year. Although the length percentile corresponding to age-1 fish will vary with species, we use the 20th percentile as an approximation. Biomass was calculated using length–weight relationships directly from the survey data. Following Wigley, McBride, and McHugh (2003), the length-weight relationship was modeled as \\[\\ln W = \\ln a + b \\ln L\\] where \\(W\\) is weight (kg), \\(L\\) is length (cm), and \\(a\\) and \\(b\\) are parameters fit via linear regression. The ratio of small fish numbers of the following year to larger fish biomass in the current year was used as the index of recruitment success. The fall and spring recruitment success anomalies were averaged to provide an annual index of recruitment success. Further details of methods described in Perretti et al. (2017a). 23.1.4 Data processing Productivity data were formatted for inclusion in the ecodata R package using the following R code. #processing small fish per large fish biomass indicator library(dplyr) library(tidyr) library(ggplot2) raw.dir &lt;- here::here(&quot;data-raw&quot;) load(file.path(raw.dir,&quot;dat_spec_rec_forSOE.Rdata&quot;)) load(file.path(raw.dir,&quot;dat_spec_rec_epu_forSOE.Rdata&quot;)) #Select and rename epu_rec_anom &lt;- dat_spec_rec_epu_forSOE %&gt;% dplyr::select(Time, EPU = Region, Value, Units, -Source,Var) #Select, rename, and bind productivity_anomaly &lt;- dat_spec_rec_forSOE %&gt;% dplyr::select(-Source) %&gt;% mutate(EPU = &quot;All&quot;, Var = paste(&quot;NE LME&quot;,Var)) %&gt;% rbind(.,epu_rec_anom) %&gt;% as.data.frame() usethis::use_data(productivity_anomaly, overwrite = TRUE) 23.1.5 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) #### Functions for plotting #### library(ggplot2) library(rpart) library(dplyr) library(stringr) all_epu &lt;- SOE.data %&gt;% filter(str_detect(Var, &quot;All EPU productivity&quot;)) all_epu$Var &lt;- str_trim(str_replace(all_epu$Var, &quot;All EPU productivity&quot;,&quot;&quot;)) # Adjust plot properties adjustAxes &lt;- ggplot2::theme(axis.title = element_text(size = 18), axis.text = element_text(size = 15), plot.title = element_text(size = 20)) ggplot &lt;- function(...) { ggplot2::ggplot(...) + ggplot2::theme_bw() + adjustAxes} # Plot stacked bar with cpts for single var plot_stackbarcpts_single &lt;- function(YEAR, var2bar, x, xlab, ylab, titl, file_suffix, leg_font_size = 10, remove_leg = FALSE, leg_ncol = 1, wcpts = TRUE, wdashed = TRUE, height = 5.5, width = 8) { dat2bar &lt;- data.frame(YEAR, var2bar, x) dat2plot &lt;- dat2bar %&gt;% tidyr::gather(variable, value, -YEAR, -var2bar) %&gt;% dplyr::mutate(var2bar = gsub(pattern = &quot;_&quot;, replacement = &quot; &quot;, x = var2bar), var2bar = gsub(pattern = &quot;Atl.&quot;, replacement = &quot;&quot;, x = var2bar), var2bar = gsub(pattern = &quot;Atl&quot;, replacement = &quot;&quot;, x = var2bar), var2bar = gsub(pattern = &quot;NS and combined&quot;, replacement = &quot;&quot;, x = var2bar), var2bar = gsub(pattern = &quot;YT&quot;, replacement = &quot;Yellowtail&quot;, x = var2bar), var2bar = gsub(pattern = &quot; GoM&quot;, replacement = &quot; GOM&quot;, x = var2bar)) p &lt;- ggplot(dat2plot, aes(x = YEAR)) + geom_bar(data = dat2plot %&gt;% filter(value &gt; 0), aes(y = value, fill = var2bar), stat = &quot;identity&quot;) + geom_bar(data = dat2plot %&gt;% filter(value &lt; 0), aes(y = value, fill = var2bar), stat = &quot;identity&quot;) + geom_hline(size = 0.3, aes(yintercept = 0)) + xlab(xlab) + ylab(ylab) + ggtitle(titl) + guides(fill = guide_legend(ncol = leg_ncol)) + theme(axis.title = element_text(size = 16), axis.text = element_text(size = 15), plot.title = element_text(size = 20), legend.text = element_text(size = leg_font_size), legend.title = element_blank()) if(remove_leg) p &lt;- p + theme(legend.position = &quot;none&quot;) print(p) # ggsave(plot = p, # filename = &quot;./productivity_all.eps&quot;, # width = width, # height = height) } # Plot stacked bars plot_stackbarcpts &lt;- function(YEAR, var2bar, top, mid, bot, top_lab, mid_lab, bot_lab, xlab = &quot;&quot;, ylab = &quot;&quot;, titl = &quot;&quot;) { dat2bar &lt;- data.frame(YEAR, var2bar, top, mid, bot) dat2plot &lt;- dat2bar %&gt;% tidyr::gather(variable, value, -YEAR, -var2bar) %&gt;% dplyr::mutate(variable = ifelse(variable == &quot;top&quot;, top_lab, ifelse(variable == &quot;mid&quot;, mid_lab, bot_lab))) dat2plot$variable &lt;- factor(dat2plot$variable, levels = c(top_lab, mid_lab, bot_lab)) p &lt;- ggplot(dat2plot, aes(x = YEAR)) + geom_bar(data = dat2plot %&gt;% filter(value &gt; 0), aes(y = value, fill = var2bar), stat = &quot;identity&quot;) + geom_bar(data = dat2plot %&gt;% filter(value &lt; 0), aes(y = value, fill = var2bar), stat = &quot;identity&quot;) + facet_wrap(~ variable, ncol = 1) + geom_hline(size = 0.3, aes(yintercept = 0)) + xlab(xlab) + ylab(ylab) + ggtitle(titl) + guides(fill = guide_legend(ncol = 1)) + theme(axis.title = element_text(size = 16), axis.text = element_text(size = 15), plot.title = element_text(size = 20), strip.text = element_text(size = 15), legend.title = element_blank()) print(p) } # Recruit per spawner (all stocks in one figure) plot_stackbarcpts_single(YEAR = all_epu$Time, var2bar = all_epu$Var, x = all_epu$Value, titl = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;Small fish per large fish biomass (anomaly)&quot;, height = 7, width = 9) Figure 23.1: Groundfish productivity across all stocks in the Northeast Large Marine Ecosystem. Figure 23.2: Groundfish productivity visualized by EPU. References "],
["recreational-fishing-indicators.html", "24 Recreational Fishing Indicators 24.1 Methods", " 24 Recreational Fishing Indicators Description: A variety of indicators derived from MRIP Recreational Fisheries Statistics, including total recreational catch, total angler trips by region, annual diversity of recreational fleet effort, and annual diversity of managed species. Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Database pull with analysis Contributor(s): Geret DePiper, Scott Steinbeck Data steward: Geret DePiper, geret.depiper@noaa.gov Point of contact: Geret DePiper, geret.depiper@noaa.gov Public availability statement: Data sets are publicly available (see Data Sources below). 24.1 Methods We used total recreational harvest as an indicator of seafood production and total recreational trips and total recreational anglers as proxies for recreational value generated from the Mid-Atlantic and New England regions respectively. We estimated both recreational catch diversity in managed species (MAFMC, NEFMC, and ASMFC) and fleet effort diversity using the effective Shannon index. 24.1.1 Data sources All recreational fishing indicator data, including number of recreationally harvested fish, number of angler trips, and number of anglers, were downloaded from the MRIP Recreational Fisheries Statistics Queries portal. Relevant metadata including information regarding data methodology updates are available at the query site. Note that 2017 data were considered preliminary at the time of the data pull. Data sets were queried by region on the MRIP site, and for the purposes of the SOE, the “NORTH ATLANTIC” and “MID-ATLANTIC” regions were mapped to the New England and Mid-Atlantic report versions respectively. All query pages are accessible through the MRIP Recreational Fisheries Statistics site. The number of recreationally harvested fish was found by selecting “TOTAL HARVEST (A + B1)” on the Catch Time Series Query page. Catch diversity estimates were also derived from the total catch time series (see below). Species included in the diversity of catch analysis included American eel, Atlantic cod, Atlantic mackerel, Atlantic sturgeon, black drum, black sea bass, bluefish, cobia, haddock, pollock, red drum, scup, Spanish mackerel, spiny dogfish, spot, spotted seatrout, striped bass, summer flounder, tautog, tilefish, weakfish, winter flounder, and “All Other Species”. Angler trips (listed as “TOTAL” trips) were pulled from the MRIP Effort Time Series Query page, and included data from 1981 - 2019. Time series of recreational fleet effort diversity were calculated from this data set (see below). The number of anglers was total number of anglers from the MRFSS Participation Time Series Query, and includes data from 1981 - 2016. 24.1.2 Data analysis Recreational fleet effort diversity REC_DATA &lt;- read.csv(&#39;X:/gdepiper/ESR2018/SOE/Data/Rec_Days_Fished_2018.csv&#39;, as.is=TRUE) REC_DATA$P_Shore &lt;- -(as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$Shore)) / as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$All_Modes))) * log(as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$Shore)) / as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$All_Modes))) REC_DATA$P_Private &lt;- -(as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$Private_Rental)) / as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$All_Modes))) * log(as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$Private_Rental)) / as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$All_Modes))) REC_DATA$P_Party &lt;- -(as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$Party_Charter)) / as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$All_Modes))) * log(as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$Party_Charter)) / as.numeric(gsub(&quot;&quot;,&quot;&quot;,&#39;&quot;&quot;&#39;,REC_DATA$All_Modes))) REC_DATA$Value &lt;- exp(REC_DATA$P_Shore+REC_DATA$P_Private+REC_DATA$P_Party) REC_DATA$Region[REC_DATA$Region==&#39;&quot;MID-ATLANTIC&quot;&#39;] &lt;- &#39;MA&#39; REC_DATA$Region[REC_DATA$Region==&#39;&quot;NORTH ATLANTIC&quot;&#39;] &lt;- &#39;NE&#39; E_SHANNON &lt;- subset(REC_DATA, select=c(&#39;Time&#39;,&#39;Region&#39;,&#39;Value&#39;)) E_SHANNON$Units &lt;- &#39;Effective Shannon&#39; E_SHANNON$Var &lt;- &#39;Recreational fleet effort diversity across modes&#39; E_SHANNON$Source &lt;- &#39;MRIP effort time series, processed to generate diversity measure.&#39; Recreational catch diversity REC_CATCH &lt;- read.csv(&#39;X:/gdepiper/ESR2018/SOE/Data/Rec_Species_Quantity_2018.csv&#39;, as.is=TRUE) REC_CATCH$Value &lt;- as.numeric(gsub(&quot;,&quot;,&quot;&quot;,REC_CATCH$Value)) TOT_REC_CATCH &lt;- aggregate(Value~Time+Region, data=REC_CATCH, FUN=sum) names(TOT_REC_CATCH) &lt;- c(&#39;Time&#39;,&#39;Region&#39;,&#39;Tot_Catch&#39;) REC_CATCH &lt;- merge(REC_CATCH,TOT_REC_CATCH, by=c(&#39;Time&#39;,&#39;Region&#39;)) REC_CATCH$P_Catch &lt;- -(REC_CATCH$Value/REC_CATCH$Tot_Catch* log(REC_CATCH$Value/REC_CATCH$Tot_Catch)) REC_CATCH &lt;- aggregate(P_Catch~Time+Region, data=REC_CATCH, FUN=sum) REC_CATCH$Value &lt;- exp(REC_CATCH$P_Catch) REC_CATCH &lt;- subset(REC_CATCH, select=c(&#39;Time&#39;,&#39;Region&#39;,&#39;Value&#39;)) REC_CATCH$Region[REC_CATCH$Region==&quot;MID-ATLANTIC&quot;] &lt;- &#39;MA&#39; REC_CATCH$Region[REC_CATCH$Region==&quot;NORTH ATLANTIC&quot;] &lt;- &#39;NE&#39; REC_CATCH$Units &lt;- &#39;Effective Shannon&#39; REC_CATCH$Var &lt;- &#39;Recreational Diversity of Catch&#39; ##Species include: American Eel, Atlantic Cod, Atlantic Mackerel, Atlantic Sturgeon, Black Drum, Black Sea Bass, Bluefish, ##Cobia, Haddock, Pollock, Red Drum, Scup, Spanish Mackerel, Spiny Dogfish, Spot, Spotted Seatrout, Striped Bass, Summer Flounder, Tautog, Tilefish, Weakfish, Winter Flounder, #and All Other Species. REC_CATCH$Source &lt;- &#39;MRIP catch time series.&#39; 24.1.3 Data processing Recreational fishing indicators were formatted for inclusion in the ecodata R package using the following code. # Processing for recreational fishing indicators library(dplyr) library(tidyr) library(stringr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_rec &lt;- function(save_clean = F){ files = list.files(raw.dir, pattern = &quot;REC_HARVEST|Rec_participants|Rec_angler|Rec_Species&quot;) for (i in 1:length(files)) assign(files[i], read.csv(file.path(raw.dir,files[i]))) recdat &lt;- NULL for (i in ls()){ if (stringr::str_detect(i, &quot;REC_|Rec_&quot;)){ d &lt;- get(i) %&gt;% dplyr::select(Time, EPU = Region, Value, Units, Var) assign(&#39;recdat&#39;,rbind(recdat, d)) } } recdat &lt;- recdat %&gt;% filter(!is.na(EPU)) if (save_clean){ usethis::use_data(recdat, overwrite = T) } else { return(recdat) } } 24.1.4 Plotting Figure 24.1: Recreational effort diversity and diversity of recreational catch in New England. Figure 24.2: Total recreational seafood harvest in New England. Figure 24.3: Recreational effort and number of recreational anglers in New England. "],
["right-whale-abundance.html", "25 Right Whale Abundance 25.1 Methods", " 25 Right Whale Abundance Description: Right Whale Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Synthesis of published information; Published methods Contributor(s): Christopher D. Orphanides Data steward: Chris Orphanides, chris.orphanides@noaa.gov Point of contact: Richard Pace, richard.pace@noaa.gov Public availability statement: Source data are available from the New England Aquarium upon request. Derived data are available here 25.1 Methods 25.1.1 Data sources The North Atlantic right whale abundance estimates were taken from a published document (see Pace, Corkeron, and Kraus 2017), except for the most recent 2016 and 2017 estimates. Abundance estimates from 2016 and 2017 were taken from the 2016 NOAA marine mammal stock assessment (Hayes et al. 2017) and an unpublished 2017 stock assessment. 25.1.2 Data extraction Data were collected from existing reports and validated by report authors. 25.1.3 Data analysis Analysis for right whale abundance estimates is provided by Pace, Corkeron, and Kraus (2017), and code can be found in the supplemental materials. 25.1.4 Data processing Time series of right whale abundance estimates were formatted for inclusion in the ecodata R package using the following R code. #Processing for North Atlantic Right Whale data #See full documentation for these data at https://noaa-edab.github.io/tech-doc/right-whale-abundance.html library(dplyr) library(tidyr) library(stringr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_narw &lt;- function(save_clean = F){ narw &lt;- read.csv(file.path(raw.dir, &quot;narw_numbers.csv&quot;)) %&gt;% gather(.,Var,Value,-YEAR) %&gt;% mutate(Var = tolower(paste(&quot;right whale abundance&quot;,Var)), Units = &quot;n&quot;, EPU = &quot;All&quot;) %&gt;% mutate(Var = ifelse(str_detect(Var, &quot;median&quot;), &quot;right whale abundance median&quot;, ifelse(str_detect(Var, &quot;lcl&quot;), &quot;right whale abundance lcl&quot;, &quot;right whale abundance ucl&quot;))) %&gt;% dplyr::rename(Time = YEAR) if (save_clean){ usethis::use_data(narw, overwrite = T) } else { return(narw) } } 25.1.5 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) par(mar = c(5,5,4,1)) soe.plot(SOE.data, &#39;Time&#39;, &quot;right_whale_median&quot;, rel.y.num = 1.2, full.trend = F, lwd = 1.5, point.cex = 1, end.start = 2007, x.label = &#39;Year&#39;, y.label = &#39;Abundance, n&#39;, rel.y.text = 1) lw_CI &lt;- SOE.data[Var == &#39;right_whale_lower_95&#39;, list(Time, Value)] up_CI &lt;- SOE.data[Var == &#39;right_whale_upper_95&#39;, list(Time, Value)] points(lw_CI, type = &quot;l&quot;, lty = 2, col = adjustcolor(&quot;darkorange&quot;, .6), lwd = 2.5) points(up_CI, type = &quot;l&quot;, lty = 2, col = adjustcolor(&quot;darkorange&quot;, .6), lwd = 2.5) Figure 25.1: North Atlantic right whale population estimates shown with 95% credible intervals. References "],
["seabird-diet-and-productivity.html", "26 Seabird diet and productivity 26.1 Methods", " 26 Seabird diet and productivity Description: Common tern annual diet and productivity at seven Gulf of Maine colonies managed by the National Audubon Society’s Seabird Restoration Program Indicator category: Published method Found in: State of the Ecosystem - New England (2019) Contributor(s): Don Lyons, Steve Kress, Paula Shannon, Sue Schubel Data steward: Don Lyons, dlyons@audubon.org Point of contact: Don Lyons, dlyons@audubon.org Public availability statement: Please email dlyons@audubon.org for further information and queries on this indicator source data. 26.1 Methods Chick diet Common tern chick diet was quantified at each of the seven nesting sites (Fig. 26.2a) by observing chick provisioning from portable observation blinds. The locations of observation blinds within each site were chosen to maximize the number of visible nests, and provisioning observations took place between mid-June and early August annually. Observations of chick diet were made during one or two three to four hour periods throughout the day, but typically proceed according to nest activity levels (moreso in the morning hours). Observations began with chicks as soon as they hatched, and continue until the chicks fledged or died. Most common tern prey species were identifiable to the species level due to distinct size, color and shape. However, when identification was not possible or was unclear, prey species were listed as “unknown” or “unknown fish”. More detailed methods can be found in Hall, Kress, and Griffin (2000). Nest productivity Common tern nest productivity, in terms of the number of fledged chicks per nest, was collected annually from fenced enclosures at island nesting sites (known as “productivity plots”). Newly hatched chicks within these enclosures were weighed, marked or banded, and observed until fledging, death, or until a 15 day period had passed when chicks were assumed to have fledged. Productivity was also quantified from observer blinds for nests outside of the productivity plots where chicks were marked for identification. More detailed methods for quantifying nest productivity can be found in Hall and Kress (2004) 26.1.1 Data sources Common tern diet and nest productivity data were provided by the National Audubon Society’s Seabird Restoration Program. 26.1.2 Data processing Diet and productivity data were formatted for inclusion in the ecodata R package using the following R code. # Process raw common tern productivity and diet data # If save == TRUE, processed data are saved as a .Rds file to the data directory. # Function returns a processed data frame containing time series of productivity and diet # composition for common tern at different islands in southern Gulf of Maine. The first three letters # in the variable &quot;Var&quot; correspond to the island at which sampling occurred, with COTE referring to # Common Tern. If &quot;Var&quot; includes productivity, then &quot;Value&quot; is the average number of fledged chicks. # Otherwise, if &quot;Var&quot; specifies diet, then &quot;Value&quot; is equal to the count of observed prey type. # For example, &quot;EER COTE Diet Amphipod&quot; refers to the number of preyed upon amphipods observed at Eastern # Egg Rock in a given year. library(dplyr) library(tidyr) #Get raw data raw.dir &lt;- here::here(&quot;data-raw&quot;) get_commontern &lt;- function(save_clean = F){ d &lt;- read.csv(file.path(raw.dir,&quot;Audubon SRP Common Tern Data.csv&quot;)) #Process common_tern &lt;- d %&gt;% filter(Island != &quot;&quot;) %&gt;% tidyr::gather(., Var, Value, -Year, -Species, -Island) %&gt;% mutate(Species = ifelse(Var != &quot;Productivity&quot;, paste(Species, &quot;Diet&quot;), &quot;COTE&quot;)) %&gt;% unite(., &quot;Var&quot;, c(&quot;Island&quot;, &quot;Species&quot;, &quot;Var&quot;), sep = &quot; &quot;) %&gt;% dplyr::rename(Time = Year) %&gt;% mutate(EPU = &quot;GOM&quot;, Units = ifelse(stringr::str_detect(Var, &quot;Productivity&quot;), &quot;fledged chicks per nest&quot;,&quot;N&quot;)) if (save_clean){ usethis::use_data(common_tern, overwrite = T) } else { return(common_tern) } } 26.1.3 Data analysis Raw diet data were used to create time series of mean shannon diversity through time and across study sites using the vegan R package (Oksanen et al. 2019). Diet diversity is presented along with nest productivity (+/- 1 SE) below. #Calculating time series of diversity indices using the vegan package. diet_div &lt;- ecodata::common_tern %&gt;% filter(str_detect(Var, &quot;Diet&quot;), !str_detect(Var, &quot;Sum&quot;)) %&gt;% mutate(Island = word(Var, 1), Var = word(Var, 4)) %&gt;% group_by(Island, Time) %&gt;% dplyr::summarise(evenness = diversity(Value)/log(specnumber(Value)), shannon = diversity(Value), simpson = diversity(Value, index = &quot;simpson&quot;)) %&gt;% gather(.,Var,Value,-Island, -Time) %&gt;% group_by(Var, Time) %&gt;% dplyr::summarize(Value = mean(Value, na.rm = T), sd = sd(Value, na.rm = T), n = n()) %&gt;% group_by(Var) %&gt;% mutate(hline = mean(Value, na.rm = T)) aggregate_prod &lt;- ecodata::common_tern %&gt;% filter(!str_detect(Var, &quot;Diet|Sum&quot;)) %&gt;% mutate(Island = word(Var, 1), Var = word(Var, 3), Island = plyr::mapvalues(Island, from = c(&quot;EER&quot;,&quot;JI&quot;,&quot;MR&quot;,&quot;OGI&quot;,&quot;PINWR&quot;,&quot;SINWR&quot;,&quot;STI&quot;), to = c(&quot;Eastern Egg Rock&quot;, &quot;Jenny Island&quot;, &quot;Matinicus Rock&quot;, &quot;Outer Green Island&quot;, &quot;Pond Island&quot;, &quot;Seal Island&quot;,&quot;Stratton Island&quot;))) %&gt;% group_by(Time) %&gt;% dplyr::summarise(Mean = mean(Value, na.rm = T), SE = sd(Value, na.rm = T)/sqrt(n()), SD = sd(Value, na.rm = T), n = n()) %&gt;% mutate(Mean = ifelse(is.na(SE),NA,Mean), se.low = Mean - SE, se.high = Mean + SE, hline = mean(Mean, na.rm = T)) prodplot &lt;- aggregate_prod %&gt;% ggplot() + #Highlight last ten years annotate(&quot;rect&quot;, fill = shade.fill, alpha = shade.alpha, xmin = x.shade.min , xmax = x.shade.max, ymin = -Inf, ymax = Inf) + geom_line(aes(x = Time, y = Mean), size = lwd-0.75) + geom_point(aes(x = Time, y = Mean), size = pcex-0.75) + geom_gls(aes(x = Time, y = Mean)) + geom_errorbar(aes(x = Time, ymin = se.low, ymax = se.high), width = 0.25) + scale_x_continuous(expand = c(0.01, 0.01),limits = c(1991,2018)) + guides(color = FALSE) + ggtitle(&quot;Common tern productivity&quot;) + ylab(expression(&quot;Fledged chicks per nest&quot;)) + xlab(&quot;Time&quot;)+ geom_hline(aes(yintercept = hline), color = &quot;black&quot;, size = hline.size, alpha = hline.alpha, linetype = hline.lty) + labs(tag = &quot;a&quot;) + theme_ts() + theme(title = element_text(size = 7)) shannon &lt;- diet_div %&gt;% filter(Var == &quot;shannon&quot;) %&gt;% ggplot(aes(x = Time, y = Value)) + annotate(&quot;rect&quot;, fill = shade.fill, alpha = shade.alpha, xmin = x.shade.min , xmax = x.shade.max, ymin = -Inf, ymax = Inf) + geom_line() + geom_point() + #geom_gls() + scale_x_continuous(expand = c(0.01, 0.01),limits = c(1992,2018)) + ggtitle(&quot;Common tern diet diversity&quot;)+ ylab(expression(&quot;Shannon Diversity&quot;)) + xlab(&quot;&quot;)+ geom_hline(aes(yintercept = hline), size = hline.size, alpha = hline.alpha, linetype = hline.lty) + labs(tag = &quot;b&quot;) + theme_ts() + theme(title = element_text(size = 7)) prodplot + shannon + plot_layout(ncol = 2, widths=c(4,4)) Figure 26.1: a. Mean common tern productivity at nesting sites in Gulf of Maine. Error bars show +/- 1 SE of the mean. b. Shannon diversity of common tern diets observed at nesting sites in Gulf of Maine. Diversity of common tern diets has been predominantly above the long-term mean since 2006. Along with nest productivity and diet diversity indices, we presented maps of sampling sites in Gulf of Maine and mean prey frequencies across sites. Prey occurring in less than 5\\% of common tern diets was excluded for visual clarity. Figure 26.2: Common terns: a. Locations of the seven sampled common tern nesting sites in Gulf of Maine (EER = Eastern Egg Rock, JI = Jenny Island, MR = Matinicus Rock, OGI = Outer Green Island, PINWR = Pond Island National Wildlife Refuge, SINWR = Seal Island National Wildlife Refuge, STI = Stratton Island), and b. Prey frequencies in the diets of common tern observed across the seven colonies in Gulf of Maine. Prey occurring in &lt;5% of common tern diets were excluded for clarity. References "],
["seasonal-sst-anomalies.html", "27 Seasonal SST Anomalies 27.1 Methods", " 27 Seasonal SST Anomalies Description: Seasonal SST Anomalies Indicator category: Database pull with analysis Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Contributor(s): Sean Hardison, Vincent Saba Data steward: Sean Hardison, sean.hardison@noaa.gov Point of contact: Sean Hardison, sean.hardison@noaa.gov Public availability statement: Source data are available here. 27.1 Methods 27.1.1 Data sources Data for seasonal sea surface tempature anomalies (Fig. ??) were derived from the NOAA optimum interpolation sea surface temperature high resolution data set (NOAA OISST V2) provided by NOAA/OAR/ESRL PSD, Boulder, CO. The data extend from 1981 to present, and provide a 0.25° x 0.25° global grid of SST measurements (Reynolds et al. 2007). 27.1.2 Data extraction Individual files containing daily mean SST data for each year during the period of 1981-present were downloaded from the OI SST V5 site. Yearly data provided as layered rasters were masked according to the extent of Northeast US Continental Shelf. Data were split into three month seasons for (Winter = Jan, Feb, Mar; Spring = Apr, May, Jun; Summer = July, August, September; Fall = Oct, Nov, Dec). 27.1.3 Data analysis We calculated the long-term mean (LTM) for each season-specific stack of rasters over the period of 1982-2010, and then subtracted the (LTM) from daily mean SST values to find the SST anomaly for a given year. The use of climatological reference periods is a standard procedure for the calculation of meteorological anomalies (WMO 2017). Prior to 2019 State of the Ecosystem reports, SST anomaly information made use of a 1982-2012 reference period. A 1982-2010 reference period was adopted to facilitate calculating anomalies from a standard NOAA ESRL data set. R code used in extraction and processing: #Processing for spatial SST anomaly library(dplyr) library(raster) library(sf) library(ggplot2) library(ncdf4) library(reshape2) rast_prep &lt;- function(r){ r &lt;- rotate(r) #Rotate r &lt;- crop(r, extent(-77,-60,35,46)) #Crop return(r) } raw.dir &lt;- &quot;~/git/ecodata/inst/extdata/gridded&quot; crs &lt;- &quot;+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0 +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; #These data are large files that are not included among ecodata source files. They are accessible #here: https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html sst.2018 &lt;- rast_prep(stack(file.path(raw.dir, &quot;sst.day.mean.2018.nc&quot;))) ltm &lt;- rast_prep(stack(file.path(raw.dir, &quot;sst.day.mean.ltm.1982-2010.nc&quot;))) # save(sst.2018, file = &quot;~/git/ecodata/inst/extdata/gridded/SST.2018.rdata&quot;) # save(sst.2018, file = &quot;~/git/ecodata/inst/extdata/gridded/SST.LTM.rdata&quot;) winter.ltm &lt;- ltm[[1:90]] spring.ltm &lt;- ltm[[91:181]] summer.ltm &lt;- ltm[[182:273]] fall.ltm &lt;- ltm[[274:365]] winter.anom &lt;- sst.2018[[1:90]] - winter.ltm spring.anom &lt;- sst.2018[[91:181]] - spring.ltm summer.anom &lt;- sst.2018[[182:273]] - summer.ltm fall.anom &lt;- sst.2018[[274:365]] - fall.ltm rast_process &lt;- function(r, season){ r &lt;- stackApply(r, indices = rep(1,nlayers(r)),mean) #Find mean anomaly crs(r) &lt;- crs #Add SOE CRS r &lt;- disaggregate(r, 5) #interpolate step 1 - create higher res grid r &lt;- focal(r, w=matrix(1,nrow=5,ncol=5), fun=mean, na.rm=TRUE, pad=TRUE) #interpolate step 2 - moving window r &lt;- as(r, &quot;SpatialPointsDataFrame&quot;) #Convert to ggplot-able object r &lt;- as.data.frame(r) r &lt;- r %&gt;% reshape2::melt(id = c(&quot;y&quot;,&quot;x&quot;)) %&gt;% dplyr::rename(Latitude = y, Longitude = x) %&gt;% dplyr::select(-variable) %&gt;% mutate(Season = season) %&gt;% dplyr::rename(Value = value) return(r) } #Converts raster to data.frame for easy use with ggplot2 seasonal_sst_anomaly_gridded &lt;- rbind(rast_process(winter.anom,season = &quot;Winter&quot;), rast_process(spring.anom,season = &quot;Spring&quot;), rast_process(summer.anom, season = &quot;Summer&quot;), rast_process(fall.anom, season = &quot;Fall&quot;)) 27.1.4 Plotting Figure 27.1: Seasonal sea-surface temperature anomalies in the Mid-Atlantic Bight. References "],
["stockstatus.html", "28 Single Species Status Indicator 28.1 Methods", " 28 Single Species Status Indicator Description: Summary of the most recent stock assessment results for each assessed species. Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Synthesis of published information Contributor(s): Sarah Gaichas, based on code and spreadsheets originally provided by Chris Legault Data steward: Sarah Gaichas sarah.gaichas@noaa.gov Point of contact: Sarah Gaichas sarah.gaichas@noaa.gov Public availability statement: All stock assessment results are publicly available (see Data Sources). Summarized data are available here. 28.1 Methods 28.1.1 Data sources “Data” used for this indicator are the outputs of stock assessment models and review processes, including reference points (proxies for fishing mortality limits and stock biomass targets and limits), and the current fishing mortality rate and biomass of each stock. The spreadsheet summarizes the most recent stock assessment updates for each species, which are available on the Northeast Fisheries Science Center (NEFSC) website at: https://www.nefsc.noaa.gov/saw/reports.html https://www.nefsc.noaa.gov/publications/crd/crd1717/ Additional assessments are reported directly to the New England Fishery Management Council (NEFMC): http://s3.amazonaws.com/nefmc.org/Document-2-SAFE-Report-for-Fishing-Year-2016.pdf http://s3.amazonaws.com/nefmc.org/4_NEFSC_SkateMemo_July_2017.pdf 28.1.2 Data extraction Each assessment document was searched to find the following information (often but not always summarized under a term of reference to determine stock status in the executive summary): Bcur: current year biomass, (most often spawning stock biomass (SSB) or whatever units the reference points are in) Fcur: current year fishing mortality, F Bref: biomass reference point, a proxy of Bmsy (the target) Fref: fishing mortality reference point, a proxy of Fmsy 28.1.3 Data processing The following R code was used to process the stock status data set for inclusion in the ecodata R package. # Processing single-species stock mortality and biomass data # Read more about this data at https://noaa-edab.github.io/tech-doc/stockstatus.html library(dplyr) library(tidyr) library(ggplot2) data.dir &lt;- here::here(&#39;inst&#39;,&#39;extdata&#39;) get_stocks &lt;- function(save_clean = F){ assess &lt;- read.csv(file.path(data.dir, &quot;2018assess.csv&quot;)) decode &lt;- read.csv(file.path(data.dir, &quot;2018decoder.csv&quot;)) stock_status &lt;- assess %&gt;% group_by(Entity.Name) %&gt;% filter(Assessment.Year == max(Assessment.Year)) %&gt;% #Find last year assessment occurred for each stock ungroup() %&gt;% left_join(.,decode, by = &quot;Entity.Name&quot;) %&gt;% #Join in list of managed species dplyr::select(Entity.Name, Assessment.Year, F.Fmsy, B.Bmsy, Council, Code) %&gt;% #select column variables to keep mutate(id = 1:length(Entity.Name)) %&gt;% gather(.,Var, Value,-id,-Entity.Name,-Assessment.Year,-Council,-Code) %&gt;% #wide to long dplyr::select(-id) %&gt;% dplyr::rename(`Last assessment` = Assessment.Year, Stock = Entity.Name) %&gt;% #rename variables for clarity mutate(Units = &quot;unitless&quot;) if (save_clean){ usethis::use_data(stock_status, overwrite = T) } else { return(stock_status) } } 28.1.4 Data analysis For each assessed species, Bcur is divided by Bref and Fcur is divided by Fref. They are then plotted for each species on an x-y plot, with Bcur/Bref on the x axis, and Fcur/Fref on the y axis. 28.1.5 Plotting The script used to develop the figure in the SOE is below, with an example figure. Different lines are commented out of the script to produce the Mid- Atlantic or New England figures. The positioning table used to make the plot (the data frame named “decoder” below) is available here. Figure 28.1: Summary of single species status for NEFMC and jointly managed stocks. "],
["slopewater-proportions.html", "29 Slopewater proportions 29.1 Methods", " 29 Slopewater proportions Description: Percent total of water type observed in the deep Northeast Channel (150-200 m water depth). Indicator category: Published methods Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2019) Contributors: Paula Fratantoni, paula.fratantoni@noaa.gov; David Mountain, NOAA Fisheries, retired. Data steward: Kimberly Bastille, kimberly.bastille@noaa.gov Point of contact: Paula Fratantoni, paula.fratantoni@noaa.gov Public availability statement: Source data are publicly available at ftp://ftp.nefsc.noaa.gov/pub/hydro/matlab_files/yearly and in the World Ocean Database housed at http://www.nodc.noaa.gov/OC5/SELECT/dbsearch/dbsearch.html under institute code 258 29.1 Methods 29.1.1 Data sources The slope water composition index incorporates temperature and salinity measurements collected on Northeast Fisheries Science Center surveys between 1977-present within the geographic confines of the Northeast Channel in the Gulf of Maine. Early measurements were made using water samples collected primarily with Niskin bottles at discreet depths, mechanical bathythermographs and expendable bathythermograph probes, but by 1991 the CTD – an acronym for conductivity temperature and depth – became standard equipment on all NEFSC surveys. 29.1.2 Data extraction While all processed hydrographic data are archived in an Oracle database (OCDBS), we work from Matlab-formatted files stored locally. 29.1.3 Data analysis Temperature and salinity measurements are examined to assess the composition of the waters entering the Gulf of Maine through the Northeast Channel. The analysis closely follows the methodology described by D. G. Mountain (2012). This method assumes that the waters flowing into the Northeast Channel between 150 and 200 meters depth are composed of slope waters, originating offshore of the continental shelf, and shelf waters, originating on the continental shelf south of Nova Scotia. For each survey in the hydrographic archive, ocean temperature and salinity observations sampled in the area just inside the Northeast Channel (bounded by 42.2-42.6° latitude north and 66-66.8° longitude west) and between 150 - 200 meters depth are extracted and a volume-weighted average temperature and salinity is calculated. The volume weighting is accomplished by apportioning the area within the Northeast Channel polygon among the stations occupying the region, based on inverse distance squared weighting. The result of this calculation is a timeseries of volume-average temperature and salinity having a temporal resolution that matches the survey frequency in the database. The average temperature and salinity observed at depth in the Northeast Channel is assumed to be the product of mixing between three distinct sources having the following temperature and salinity characteristics: (1) Warm Slope Water (T=10 °C, S=35), (2) Labrador Slope Water (T=6 °C, S=34.7) and (3) Scotian Shelf Water (T=2 °C, S=32). As described by D. G. Mountain (2012), the relative proportion of each source is determined via a rudimentary 3-point mixing algorithm. On a temperature-salinity diagram, lines connecting the T-S coordinates for these three sources form a triangle, the sides of which represent mixing lines between the sources. A water sample that is a mixture of two sources will have a temperature and salinity that falls somewhere along the line connecting the two sources on the temperature-salinity diagram. Observations of temperature and salinity collected within the Northeast Channel would be expected to fall within the triangle if the water sampled is a mixture of the three sources. Simple geometry allows us to calculate the relative proportion of each source in a given measurement. As an example, a line drawn from the T-S point representing shelf water through an observed T-S in the center of the triangle will intersect the opposite side of the triangle (the mixing line connecting the coordinates of the two slope water sources). This intersecting T-S value may then be used to calculate the relative proportions (percentage) of the two slope water sources. Using this method, the percentage of Labrador slope water and Warm slope water are determined for the timeseries of volume-average temperature and salinity. It should be noted that our method assumes that the temperature and salinity properties associated with the source watermasses are constant. In reality, these may vary from year to year, modified by atmospheric forcing, mixing and/or advective processes. Likewise, other sources are periodically introduced into the Northeast Channel, including intrusions of Gulf Stream water flowing into the Gulf of Maine and modified shelf water flowing out of the Gulf of Maine along the flank of Georges Bank. These sources are not explicitely considered in the 3-point mixing algorithm and may introduce errors in the proportional estimates. 29.1.4 Data processing Source data were formatted for inclusion in the ecodata R package using the following R code. # Process slopewater proportion time series # # Slopewater proportions give the percent total of water type observed in # the deep Northeast Channel (150-200 m depth). # # Raw data fields correspond to year, water mass flavor (WSW = Warm Slope Water, LSLW = Labrador Slope Water), # and proportion of total expressed as a percentage. library(dplyr) library(tidyr) #Get raw raw.dir &lt;- here::here(&quot;data-raw&quot;) #input raw get_slopewater &lt;- function(save_clean = F){ d &lt;- read.csv(file.path(raw.dir,&quot;slopewater_proportions.csv&quot;)) slopewater &lt;- d %&gt;% dplyr::rename(Time = year, Var = water.mass.flavor, Value = prop) %&gt;% mutate(EPU = &quot;GOM&quot;, Units = &quot;unitless&quot;, Var2 = &quot;proportion ne channel&quot;) %&gt;% unite(.,Var,c(Var,Var2), sep = &quot; &quot;) %&gt;% as.data.frame() if (save_clean){ usethis::use_data(slopewater, overwrite = T) } else { return(slopewater) } } 29.1.5 Plotting sw.df &lt;- slopewater %&gt;% mutate(Var, Var = plyr::mapvalues(Var, from = c(&quot;WSW proportion ne channel&quot;, &quot;LSLW proportion ne channel&quot;), to = c(&quot;WSW&quot;,&quot;LSLW&quot;))) %&gt;% dplyr::rename(Origin = Var) %&gt;% group_by(Origin) %&gt;% mutate(hline = mean(Value)) sw.df$Origin &lt;- factor(sw.df$Origin, levels = c(&quot;WSW&quot;,&quot;LSLW&quot;)) ggplot(data = sw.df) + geom_line(aes(x = Time, y = Value, color = Origin))+ geom_point(aes(x = Time, y = Value, color = Origin)) + ylab(&quot;Percent of Total Slopewater&quot;) + ggtitle(&quot;Slopewater Proportions in NE Channel&quot;)+ scale_x_continuous(expand = c(0.01, 0.01))+ geom_hline(aes(yintercept = hline, color = Origin), size = hline.size, alpha = hline.alpha, linetype = hline.lty)+ theme_ts() + theme(strip.text=element_text(hjust=0, face = &quot;italic&quot;)) Figure 29.1: Proportion of warm slope water (WSW) and Labrador slope water (LSLW) entering the GOM through the Northeast Channel. References "],
["species-density-estimates.html", "30 Species Density Estimates 30.1 Methods", " 30 Species Density Estimates Description: Current and Historical Species Distributions Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018), State of the Ecosystem - Mid-Atlantic (2017, 2018) Indicator category: Database pull; Database pull with analysis Contributor: Kevin Friedland Data steward: Kevin Friedland Point of contact: Kevin Friedland, kevin.friedland@noaa.gov Public availability statement: Source data are publicly available. 30.1 Methods We used kernel density plots to depict shifts in species’ distributions over time. These figures characterize the probability of a species occurring in a given area based on NEFSC Bottom Trawl Survey data. Kernel density estimates (KDEs) of distributions are shown for the period of 1970-1979 (shaded blue) and most recent three years of survey data (shaded red) (e.g. Figure 30.1). Results are typically visualized for spring and fall bottom trawl surveys seperately. Three probability levels (25%, 50%, 75%) are shown for each time period, where the 25% region depicts the core area of the distribution and the 75% region shows the area occupied more broadly by the species. A wide array of KDEs for many ecologically and economically important species on the Northeast US Continental Shelf are available here. 30.1.1 Data sources Current and historical species distributions are based on the NEFSC Bottom Trawl Survey data (aka “Survdat”) and depth strata. Strata are available as shapefiles that can be downloaded here (listed as “strata.shp”). 30.1.2 Data analysis library(ecodata) library(maps) library(mapdata) library(ks) library(marmap) library(raster) library(geosphere) library(dplyr) # plot ks plots #----STUFF TO SET----------------- data.dir &lt;- here::here(&quot;data&quot;) gis.dir &lt;- here::here(&quot;gis&quot;) # Gives most recent period. 2015-2019 input is 2016-2018 data rminyr &lt;- 2015 rmaxyr &lt;- 2019 # tlevel is density of color for KD contours areas tlevel=75 # move later color_b=&quot;blue&quot; color_r=&quot;orange3&quot; color_r=&quot;tomato3&quot; # Code to read in strata and compute areas. Or read from cache. # readin in strata.shp and compute areas of strata # TrawlStrata &lt;- raster::shapefile(file.path(gis.dir,&quot;BTS_Strata.shp&quot;)) # AREA &lt;- geosphere::areaPolygon(TrawlStrata, r=6371000)/10^6 # for array of strata and area and make into dataframe # stratareas &lt;- cbind(TrawlStrata@data$STRATA, AREA) # colnames(stratareas) &lt;- c(&quot;STRATA&quot;,&quot;AREA&quot;) # stratareas &lt;- data.frame(stratareas) load(file.path(data.dir, &quot;StratAreas.Rdata&quot;)) # Query bathymetry or load from cache # getNOAA.bathy(lon1 = -77, lon2 = -65, lat1 = 35, lat2 = 45, # resolution = 10) -&gt; nesbath load(file.path(data.dir, &quot;nesbath.Rdata&quot;)) #Load raw survey data load(file.path(data.dir, &quot;Survdat.RData&quot;)) # MUST run addTrans function # color transparency addTrans &lt;- function(color,trans){ # This function adds transparancy to a color. # Define transparancy with an integer between 0 and 255 # 0 being fully transparant and 255 being fully visable # Works with either color and trans a vector of equal length, # or one of the two of length 1. if (length(color)!=length(trans)&amp;!any(c(length(color),length(trans))==1)) stop(&quot;Vector lengths not correct&quot;) if (length(color)==1 &amp; length(trans)&gt;1) color &lt;- rep(color,length(trans)) if (length(trans)==1 &amp; length(color)&gt;1) trans &lt;- rep(trans,length(color)) num2hex &lt;- function(x) { hex &lt;- unlist(strsplit(&quot;0123456789ABCDEF&quot;,split=&quot;&quot;)) return(paste(hex[(x-x%%16)/16+1],hex[x%%16+1],sep=&quot;&quot;)) } rgb &lt;- rbind(col2rgb(color),trans) res &lt;- paste(&quot;#&quot;,apply(apply(rgb,2,num2hex),2,paste,collapse=&quot;&quot;),sep=&quot;&quot;) return(res) } plot_kd &lt;- function(species, season, exclude_years){ # stata to use # offshore strata to use CoreOffshoreStrata &lt;- c(seq(1010,1300,10),1340, seq(1360,1400,10),seq(1610,1760,10)) # inshore strata to use, still sampled by Bigelow CoreInshore73to12 &lt;- c(3020, 3050, 3080 ,3110 ,3140 ,3170, 3200, 3230, 3260, 3290, 3320, 3350 ,3380, 3410 ,3440) # combine strata_used &lt;- c(CoreOffshoreStrata,CoreInshore73to12) survdat &lt;- survdat %&gt;% dplyr::select(c(CRUISE6,STATION,STRATUM,SVSPP,YEAR, SEASON,LAT,LON,ABUNDANCE,BIOMASS)) %&gt;% filter(SEASON == season, STRATUM %in% strata_used) %&gt;% # delete record form non-core strata and get unique records, # should be one per species distinct() %&gt;% # add field with rounded BIOMASS scaler used to adjust distributions mutate(LOGBIO = round(log10(BIOMASS * 10+10))) # trim the data....to prepare to find stations only survdat_stations &lt;- survdat %&gt;% dplyr::select(CRUISE6, STATION, STRATUM, YEAR) %&gt;% distinct() # make table of strata by year numtowsstratyr &lt;- table(survdat_stations$STRATUM,survdat_stations$YEAR) # find records to keep based on core strata rectokeep &lt;- stratareas$STRATA %in% strata_used # add rec to keep to survdat stratareas &lt;- cbind(stratareas,rectokeep) # delete record form non-core strata stratareas_usedonly &lt;- stratareas[!stratareas$rectokeep==&quot;FALSE&quot;,] areapertow=numtowsstratyr #compute area covered per tow per strata per year for(i in 1:50){ areapertow[,i]=stratareas_usedonly$AREA/numtowsstratyr[,i] } # change inf to NA and round and out in DF areapertow[][is.infinite(areapertow[])]=NA areapertow=round(areapertow) areapertow=data.frame(areapertow) colnames(areapertow) &lt;- c(&quot;STRATUM&quot;,&quot;YEAR&quot;,&quot;AREAWT&quot;) areapertow$STRATUM &lt;- as.numeric(as.character(areapertow$STRATUM)) areapertow$YEAR &lt;- as.numeric(as.character(areapertow$YEAR)) survdat &lt;- survdat %&gt;% inner_join(.,areapertow, by= c(&quot;STRATUM&quot;,&quot;YEAR&quot;)) %&gt;% dplyr::rename(AREAPERTOW = AREAWT) # add col to survdat for PLOTWT survdat$PLOTWT &lt;- NA survdat$PLOTWT &lt;- ceiling(survdat$AREAPERTOW/1000*survdat$LOGBIO/9) if (!is.null(exclude_years)){ sdat &lt;- survdat %&gt;% filter(!YEAR %in% exclude_years) } else { sdat &lt;- survdat } # read species list sps &lt;- ecodata::species_groupings %&gt;% filter(!is.na(SVSPP)) %&gt;% dplyr::select(COMNAME, SVSPP) sps &lt;- sps[!duplicated(sps),] numsps &lt;- nrow(sps) # graph par par(mar = c(0,0,0,0)) par(oma = c(0,0,0,0)) # index 1:numsps, or by species record number for one species, i.e.25:25 tspe &lt;- sps %&gt;% filter(COMNAME == species) # start map map(&quot;worldHires&quot;, xlim=c(-77,-65),ylim=c(35,45), fill=T,border=0,col=&quot;gray&quot;) map.axes() plot(nesbath,deep=-200, shallow=-200, step=1,add=T,lwd=1,col=&quot;gray50&quot;,lty=2) # for base period, 1970 to 1979, find call lons for species and by biomass weighting minyr=1969;maxyr=1980 clons1 = sdat$LON[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==1)] clons2 = sdat$LON[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==2)] clons3 = sdat$LON[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==3)] clons4 = sdat$LON[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==4)] clons5 = sdat$LON[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==4)] # get rid of missings, KS does not like clons1 &lt;- na.omit(clons1) clons2 &lt;- na.omit(clons2) clons3 &lt;- na.omit(clons3) clons4 &lt;- na.omit(clons4) clons5 &lt;- na.omit(clons5) # accumulate all lons, repeating for weighting clons=c(clons1,clons2,clons2,clons3,clons3,clons3,clons4,clons4,clons4,clons4, clons5,clons5,clons5,clons5,clons5) # same for lats clats1 = sdat$LAT[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==1)] clats2 = sdat$LAT[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==2)] clats3 = sdat$LAT[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==3)] clats4 = sdat$LAT[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==4)] clats5 = sdat$LAT[(sdat$YEAR&gt;minyr &amp; sdat$YEAR&lt;maxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==5)] clats1 &lt;- na.omit(clats1) clats2 &lt;- na.omit(clats2) clats3 &lt;- na.omit(clats3) clats4 &lt;- na.omit(clats4) clats5 &lt;- na.omit(clats5) clats=c(clats1,clats2,clats2,clats3,clats3,clats3,clats4,clats4,clats4,clats4, clats5,clats5,clats5,clats5,clats5) # combine lons and lats x=cbind(clons,clats) # compute KD using KS routine Hscv1 &lt;- Hscv.diag(x=x) #fhat.pi1 &lt;- kde(x=x, H=Hscv1) fhat.pi1 &lt;- kde(x, compute.cont=T, binned=F, xmin=c(-77, 35), xmax=c(-65, 45)) # specify grid to match raster stack of OISST... etc. # add to plot each probability separately contour.25 &lt;- with(fhat.pi1, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont[&quot;25%&quot;])) contour.50 &lt;- with(fhat.pi1, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont[&quot;50%&quot;])) contour.75 &lt;- with(fhat.pi1, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont[&quot;75%&quot;])) for (j in 1:length(contour.75)){ polygon(unlist(contour.75[[j]][2]), unlist(contour.75[[j]][3]),col=addTrans(color_b,tlevel), border=F) } for (j in 1:length(contour.50)){ polygon(unlist(contour.50[[j]][2]), unlist(contour.50[[j]][3]),col=addTrans(color_b,tlevel), border=F) } for (j in 1:length(contour.25)){ polygon(unlist(contour.25[[j]][2]), unlist(contour.25[[j]][3]),col=addTrans(color_b,tlevel), border=F) } clons1 = sdat$LON[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==1)] clons2 = sdat$LON[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==2)] clons3 = sdat$LON[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==3)] clons4 = sdat$LON[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==4)] clons5 = sdat$LON[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==4)] # get rid of missings, KS does not like clons1 &lt;- na.omit(clons1) clons2 &lt;- na.omit(clons2) clons3 &lt;- na.omit(clons3) clons4 &lt;- na.omit(clons4) clons5 &lt;- na.omit(clons5) # accumulate all lons, repeating for weighting clons=c(clons1,clons2,clons2,clons3,clons3,clons3,clons4,clons4,clons4,clons4, clons5,clons5,clons5,clons5,clons5) # same for lats clats1 = sdat$LAT[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==1)] clats2 = sdat$LAT[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==2)] clats3 = sdat$LAT[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==3)] clats4 = sdat$LAT[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==4)] clats5 = sdat$LAT[(sdat$YEAR&gt;rminyr &amp; sdat$YEAR&lt;rmaxyr &amp; sdat$SVSPP==tspe$SVSPP &amp; sdat$PLOTWT ==5)] clats1 &lt;- na.omit(clats1) clats2 &lt;- na.omit(clats2) clats3 &lt;- na.omit(clats3) clats4 &lt;- na.omit(clats4) clats5 &lt;- na.omit(clats5) clats=c(clats1,clats2,clats2,clats3,clats3,clats3,clats4,clats4,clats4,clats4, clats5,clats5,clats5,clats5,clats5) x=cbind(clons,clats) Hscv2 &lt;- Hscv.diag(x=x) #fhat.pi2 &lt;- kde(x=x, H=Hscv2) fhat.pi2 &lt;- kde(x, compute.cont=T, binned=F, xmin=c(-77, 35), xmax=c(-65, 45)) # specify grid to match raster stack of OISST... etc. # add to plot each probability separately contour.25 &lt;- with(fhat.pi2, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont[&quot;25%&quot;])) contour.50 &lt;- with(fhat.pi2,contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont[&quot;50%&quot;])) contour.75 &lt;- with(fhat.pi2, contourLines(x=eval.points[[1]],y=eval.points[[2]], z=estimate,levels=cont[&quot;75%&quot;])) for (j in 1:length(contour.75)){ polygon(unlist(contour.75[[j]][2]), unlist(contour.75[[j]][3]),col=addTrans(color_r,tlevel), border=F) } for (j in 1:length(contour.50)){ polygon(unlist(contour.50[[j]][2]), unlist(contour.50[[j]][3]),col=addTrans(color_r,tlevel), border=F) } for (j in 1:length(contour.25)){ polygon(unlist(contour.25[[j]][2]), unlist(contour.25[[j]][3]),col=addTrans(color_r,tlevel), border=F) } text(-70,37.5, pos=4,labels = species) segments(-69.5, 37,-68.5, 37,lwd=1,col=&quot;gray50&quot;,lty=2) text(-68.5,37, pos=4,labels = &quot;200m&quot;) stline=36.5 text(-70.4,stline, pos=4,labels = &quot;25% 50% 75%&quot;) incline=-.3 segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_b,tlevel)) segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_b,tlevel)) segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_b,tlevel)) text(-66.8,stline+incline, pos=4,labels = &quot;Base&quot;) incline=-.7 segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_r,tlevel)) segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_r,tlevel)) segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_r,tlevel)) text(-66.8,stline+incline, pos=4,labels = &quot;Recent&quot;) incline=-1.1 segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_b,tlevel)) segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_b,tlevel)) segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_b,tlevel)) segments(-70, stline+incline,-69,stline+incline ,lwd=20,col=addTrans(color_r,tlevel)) segments(-70, stline+incline,-68,stline+incline ,lwd=20,col=addTrans(color_r,tlevel)) segments(-70, stline+incline,-67,stline+incline ,lwd=20,col=addTrans(color_r,tlevel)) text(-66.8,stline+incline, pos=4,labels = &quot;Overlap&quot;) } 30.1.3 Plotting plot_kd(species = &quot;SEA SCALLOP&quot;, season = &quot;SPRING&quot;, exclude_years = NULL) Figure 30.1: Current and historical sea scallop kernel density estimates derived from spring survey data. Current estimates derived from 2016-2018 data. "],
["species-distribution-indicators.html", "31 Species Distribution Indicators 31.1 Methods", " 31 Species Distribution Indicators Description: Species mean depth, along-shelf distance, and distance to coastline Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Extensive analysis; not yet published Contributor(s): Kevin Friedland Data steward: Kevin Friedland, kevin.friedland@noaa.gov Point of contact: Kevin Friedland, kevin.friedland@noaa.gov Public availability statement: Source data are available upon request (read more here). Derived data may be downloaded here. 31.1 Methods Three metrics quantifying spatial-temporal distribution shifts within fish populations were developed by Friedland et al. (2018), including mean depth, along-shelf distance, and distance to coastline. Along-shelf distance is a metric for quantifying the distribution of a species through time along the axis of the US Northeast Continental Shelf, which extends northeastward from the Outer Banks of North Carolina. Values in the derived time series correspond to mean distance in km from the southwest origin of the along-shelf axis at 0 km. The along-shelf axis begins at 76.53°W 34.60°N and terminates at 65.71°W 43.49°N. Once mean distance is found, depth of occurrence and distance to coastline can be calculated for each species’ positional center. Analyses present in the State of the Ecosystem (SOE) reports include mean depth and along-shelf distance for Atlantic cod, sea scallop, summer flounder, and black sea bass. 31.1.1 Data sources Data for these indicators were derived from fishery-independent bottom trawl survey data collected by the Northeast Fisheries Science Center (NEFSC). 31.1.2 Data analysis Species distribution indicators were derived using the following R code. library(raster) library(ncdf4) library(stats) library(geosphere) library(plyr) # set wd C:\\1_analyses_ne_shelf\\along shelf pos #setwd(choose.dir(default=getwd())) setwd(&quot;C:/1_analyses_ne_shelf/along shelf pos&quot;) wd=getwd() # name of survey data file select season &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; surfilename=&quot;Survdat_8_2017.Rdata&quot; # select season SPRING &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; selseaon=&quot;SPRING&quot; outfile=&quot;dhdc_8_2017_fallASS_sprDATA.csv&quot; outfile=&quot;dhdc_8_2017_fallASS_sprDATA jc.csv&quot; outfile=&quot;dhdc_8_2017_fallASS_sprDATA joe.csv&quot; outfile=&quot;dhdc_8_2017_fallASS_sprDATA all.csv&quot; # select season FALL &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; selseaon=&quot;FALL&quot; #outfile=&quot;dhdc_1_2017_sprASS_fallDATA.csv&quot; outfile=&quot;dhdc_1_2017_sprASS_fallDATA jc.csv&quot; outfile=&quot;dhdc_1_2017_sprASS_fallDATA joe.csv&quot; outfile=&quot;dhdc_1_2017_sprASS_fallDATA all.csv&quot; # read species list sps.csv #sps=read.csv(file.choose(), header = TRUE) sps=read.csv(file=&quot;sps.csv&quot;, header = TRUE) sps=read.csv(file=&quot;sps 312.csv&quot;, header = TRUE) sps=read.csv(file=&quot;sps_joe.csv&quot;, header = TRUE) sps=read.csv(file=&quot;sps_spring.csv&quot;, header = TRUE) sps=read.csv(file=&quot;sps_fall.csv&quot;, header = TRUE) numsps=nrow(sps) # select species to analyze &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; spptokeep=sps$SVSPP #spptokeep=c(73,74,75,76,77) #spptokeep=c(312) #Jonah Crab 312 # read in depth grid gdepth=raster(&quot;nes_bath_data.nc&quot;, band=1) # read in coordinates for along shelf diagnal diag.csv #diag=read.csv(file.choose(), header = TRUE) diag=read.csv(file=&quot;diag.csv&quot;, header = TRUE) # read in coordinate for coast nes_coastline.csv #nescoast=read.csv(file.choose(), header = TRUE) nescoast=read.csv(file=&quot;nes_coastline.csv&quot;, header = TRUE) # read in coordinate for coast nes_coast_2.csv #nescoast2=read.csv(file.choose(), header = TRUE) nescoast2=read.csv(file=&quot;nes_coast_2.csv&quot;, header = TRUE) # read in coordinate for coast V2 hersey_high_2.csv #nesc2=read.csv(file.choose(), header = TRUE) nesc2=read.csv(file=&quot;hersey_high_2.csv&quot;, header = TRUE) # constants radt=pi/180 R &lt;- 6371 # Earth mean radius [km] # CODE TO READ IN STRATA COMPUTE AREAS, NOW JUST READ IN STRATAREAS dataframe # readin in strata.shp and compute areas of strata #TrawlStrata&lt;-shapefile(file.choose()) #plot(TrawlStrata) #AREA&lt;-areaPolygon(TrawlStrata, r=6371000)/10^6 # for array of strata and area and make into dataframe #stratareas=cbind(TrawlStrata@data$STRATA, AREA) #colnames(stratareas) &lt;- c(&quot;STRATA&quot;,&quot;AREA&quot;) #stratareas=data.frame(stratareas) #save(stratareas, file=&quot;stratareas.rdata&quot;) # load stratareas load(&quot;stratareas.rdata&quot;) # get durvey datafile Survdat.RData #load(file.choose()) #load(file=&quot;Survdat.RData&quot;) load(file=surfilename) # trin the data.... need to choose season retvars &lt;- c(&quot;CRUISE6&quot;,&quot;STATION&quot;,&quot;STRATUM&quot;,&quot;SVSPP&quot;,&quot;YEAR&quot;,&quot;SEASON&quot;,&quot;LAT&quot;,&quot;LON&quot;,&quot;DEPTH&quot;,&quot;ABUNDANCE&quot;,&quot;BIOMASS&quot;) survdat &lt;- survdat[retvars] survdat &lt;- survdat[(survdat$SEASON==selseaon),] # stata to use # offshore strata to use CoreOffshoreStrata&lt;-c(seq(1010,1300,10),1340, seq(1360,1400,10),seq(1610,1760,10)) # inshore strata to use, still sampled by Bigelow CoreInshore73to12=c(3020, 3050, 3080 ,3110 ,3140 ,3170, 3200, 3230, 3260, 3290, 3320, 3350 ,3380, 3410 ,3440) # combine strata_used=c(CoreOffshoreStrata,CoreInshore73to12) # find records to keep based on core strata rectokeep=survdat$STRATUM %in% strata_used #table(rectokeep) # add rec to keep to survdat survdat=cbind(survdat,rectokeep) # delete record form non-core strata survdat=survdat[!survdat$rectokeep==&quot;FALSE&quot;,] # get rid of species survdat$rectokeep=survdat$SVSPP %in% spptokeep survdat=survdat[!survdat$rectokeep==&quot;FALSE&quot;,] # find unique tow records only, since length and weight removed # unique deletes to one record per species survdat &lt;- unique(survdat) # add field with rounded BIOMASS scaler used to adjust distributions survdat$LOGBIO &lt;- round(log10(survdat$BIOMASS * 10+10)) # take a look go from 1 to 5? table(survdat$LOGBIO) # trim the data.... to prepare to find stations only retvars &lt;- c(&quot;CRUISE6&quot;,&quot;STATION&quot;,&quot;STRATUM&quot;,&quot;YEAR&quot;) survdat_stations &lt;- survdat[retvars] # unique reduces to a record per tow survdat_stations &lt;- unique(survdat_stations) # make table of strata by year numtowsstratyr=table(survdat_stations$STRATUM,survdat_stations$YEAR) # find records to keep based on core strata rectokeep=stratareas$STRATA %in% strata_used # add rec to keep to survdat stratareas=cbind(stratareas,rectokeep) # delete record form non-core strata stratareas_usedonly=stratareas[!stratareas$rectokeep==&quot;FALSE&quot;,] # creat areapertow areapertow=numtowsstratyr #compute area covered per tow per strata per year for(i in 1:47){ areapertow[,i]=stratareas_usedonly$AREA/numtowsstratyr[,i] } # change inf to NA and round and out in DF areapertow[][is.infinite(areapertow[])]=NA areapertow=round(areapertow) areapertow=data.frame(areapertow) colnames(areapertow) &lt;- c(&quot;STRATA&quot;,&quot;YEAR&quot;,&quot;AREAWT&quot;) # add col to survdat for strata weight survdat$AREAPERTOW=NA #fill AREAPERTOW dimsurvdat=dim(survdat) for (i in 1:dimsurvdat[1]){ survdat$AREAPERTOW[i]=areapertow$AREAWT[which(survdat$STRATUM[i]==areapertow$STRATA &amp; survdat$YEAR[i]==areapertow$YEAR)] } table(ceiling(survdat$AREAPERTOW/1000)) table(survdat$LOGBIO) table(ceiling(survdat$AREAPERTOW/1000*survdat$LOGBIO/9)) # add col to survdat for PLOTWT survdat$PLOTWT=NA survdat$PLOTWT= ceiling(survdat$AREAPERTOW/1000*survdat$LOGBIO/9) table(survdat$PLOTWT) # Plot stations plot(survdat$LON[survdat$YEAR==1974],survdat$LAT[survdat$YEAR==1974]) # put in shorter name sdat=survdat # clear some space remove(survdat) # number of records to evaluate numrecs=nrow(sdat) #====================================================================================== # TAKEN OUT SINCE THE SAME AS GASDIST #blank array for ASDIST #d = array(data = NA, dim = nrow(diag)) # block to calculate diag distance ASDIST #for (j in 1:numrecs) { # print(numrecs-j) # # lat1=sdat$LAT[j]* radt # long1=sdat$LON[j]* radt # for (i in 1:150){ # lat2=diag$LAT[i]* radt # long2=diag$LON[i]* radt # d[i] &lt;- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R # } # dindex=which(d==min(d)) ## # lat1=34.60* radt # long1=-76.53* radt # # lat2=diag$LAT[dindex]* radt # long2=diag$LON[dindex]* radt # sdat$ASDIST[j] = acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R #} #====================================================================================== # TAKEN OUT SINCE THE SAME AS GDTOC #blank array for DTOC #d = array(data = NA, dim = nrow(nescoast)) # block to calculate diag distance DTOC #for (j in 1:numrecs) { ## print(numrecs-j) # lat1=sdat$LAT[j]* radt # long1=sdat$LON[j]* radt # for (i in 1:nrow(nescoast)){ # lat2=nescoast$LAT[i]* radt # long2=nescoast$LON[i]* radt # d[i] &lt;- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R # } # sdat$DTOC[j] = d[which(d==min(d))] #} #====================================================================================== print(&quot;distance to coast using geosphere&quot;) #### Geosphere package to calc distance to coastline from pts (lon,lat), returns meters dd = array(data = NA, dim = nrow(sdat)) pts = data.frame(sdat$LON, sdat$LAT) #line = t(rbind(nescoast$Longitude, nescoast$Latitude)) #line = t(rbind(nesc2$LON, nesc2$LAT)) line_nescoast2 = t(rbind(nescoast2$LON, nescoast2$LAT)) #dd=dist2Line(pts[,], line) dd=dist2Line(pts[,], line_nescoast2) sdat$GDTOC=dd/1000 # convert meters to KM # TESTING (look at sdat to compare dtc (geosphere) to DTOC (from loop)) # plot(line) # lines(diag) # lines(line) # points(nescoast) #ddtest=data.frame(dd) #ddtest$distance=NULL #plot(nescoast2) #lines(nescoast2) #ptt=105 #points(sdat$LON[ptt], sdat$LAT[ptt], col=&quot;red&quot;); sdat$DTOC[ptt]; sdat$dtc[ptt]; points(ddtest[ptt,], col=&quot;green&quot;) #====================================================================================== print(&quot;diag distance using geosphere&quot;) # Find distance to diagonal line (diag), use coordinates of nearest point to find distance to NC outerbanks (min(diag)) dd2 = array(data = NA, dim = nrow(sdat)) dd2 = dist2Line(pts[,], diag, distfun=distHaversine) #Distance of closest point to data along diag line to NC coast p1 = diag[1,] #start of line p3 = diag[150,] #end of line p2 = data.frame(dd2[,2], dd2[,3]) distNC = distCosine(p1, p2, r=6378137) /1000 # convert to KM (Great circle distance) sdat$GASDIST = distNC #====================================================================================== # create column for missing depth data intially with depth data sdat$MISDEPTH=sdat$DEPTH # find cases with missing depth data missingdepth=which(is.na(sdat$DEPTH)) # fill only those records in misdepth with depth from grid for(k in missingdepth){ sdat$MISDEPTH[k] = extract(gdepth,cbind(sdat$LON[k],sdat$LAT[k])) * -1 } #========================================================================================= #outline=paste(&quot;YR&quot;,&quot;,&quot;,&quot;SVSPP&quot;,&quot;,&quot;,&quot;mASDIST&quot;,&quot;,&quot;,&quot;mDTOC&quot;,&quot;,&quot;,&quot;mMISDEPTH&quot;,&quot;,&quot;,&quot;mLAT&quot;,&quot;,&quot;,&quot;mLON&quot;,&quot;,&quot;,&quot;mGASDIST&quot;,&quot;,&quot;,&quot;mGDTOC&quot;) #write.table(outline,file=outfile,row.name=F,col.names=F,append=TRUE) out_data=array(NA,c((max(sdat$YEAR)-min(sdat$YEAR)+1)*numsps,7)) row_c=0 for (i in 1:numsps){ print (i) for(j in min(sdat$YEAR):max(sdat$YEAR)){ row_c=row_c+1 #sumdist=sum(sdat$ASDIST[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) #lendist=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) #mASDIST =sumdist / lendist #sumdist=sum(sdat$DTOC[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) #lendist=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) #mDTOC =sumdist / lendist sumdepth=sum(sdat$MISDEPTH[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) lendepth=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) mMISDEPTH =sumdepth / lendepth sumdepth=sum(sdat$LAT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) lendepth=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) mLAT =sumdepth / lendepth sumdepth=sum(sdat$LON[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) lendepth=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) mLON =sumdepth / lendepth sumdepth=sum(sdat$GASDIST[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) lendepth=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) mGASDIST =sumdepth / lendepth sumdepth=sum(sdat$GDTOC[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]] *sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) lendepth=sum(sdat$PLOTWT[sdat$YEAR==j &amp; sdat$SVSPP==sps$SVSPP[i]]) mGDTOC =sumdepth / lendepth out_data[row_c,1]=j out_data[row_c,2]=sps$SVSPP[i] out_data[row_c,3]=mMISDEPTH out_data[row_c,4]=mLAT out_data[row_c,5]=mLON out_data[row_c,6]=mGASDIST out_data[row_c,7]=mGDTOC } } #outline=paste(j,&quot;,&quot;,sps$SVSPP[i],&quot;,&quot;,mMISDEPTH,&quot;,&quot;,mLAT,&quot;,&quot;,mLON,&quot;,&quot;,mGASDIST,&quot;,&quot;,mGDTOC) #write.table(outline,file=outfile,row.name=F,col.names=F,append=TRUE) out_data=data.frame(out_data) names(out_data)[names(out_data)==&quot;X1&quot;] &lt;- &quot;YR&quot; names(out_data)[names(out_data)==&quot;X2&quot;] &lt;- &quot;SP&quot; names(out_data)[names(out_data)==&quot;X3&quot;] &lt;- &quot;DEPTH&quot; names(out_data)[names(out_data)==&quot;X4&quot;] &lt;- &quot;LAT&quot; names(out_data)[names(out_data)==&quot;X5&quot;] &lt;- &quot;LON&quot; names(out_data)[names(out_data)==&quot;X6&quot;] &lt;- &quot;ASDIST&quot; names(out_data)[names(out_data)==&quot;X7&quot;] &lt;- &quot;DTEOC&quot; write.csv(out_data,file=outfile ) 31.1.3 Data processing Distribution indicators were further formatted for inclusion in the ecodata R package using the following R code. #Aggregate species distribution metrics for the Northeast Shelf library(dplyr) library(tidyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_species_dist &lt;- function(save_clean = F){ species_dist &lt;- read.csv(file.path(raw.dir, &quot;sp dist.csv&quot;)) %&gt;% dplyr::rename(depth = DEPTH, Latitude = LAT, Longitude = LON, `along-shelf distance` = ASD, `distance to coast` = DTC, Time = Year) %&gt;% gather(.,Var,Value,-Time) %&gt;% mutate(EPU = &quot;All&quot;, Units = ifelse(str_detect(Var,&quot;distance&quot;),&quot;km&quot;, ifelse(str_detect(Var,&quot;Latitude&quot;), &quot;degreesN&quot;,ifelse(str_detect(Var,&quot;Longitude&quot;), &quot;degreesW&quot;,ifelse(str_detect(Var, &quot;depth&quot;), &quot;m&quot;,NA))))) if (save_clean){ usethis::use_data(species_dist, overwrite = T) } else { return(species_dist) } } 31.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4)) soe.plot(SOE.data, &quot;Time&quot;, &quot;black sea bass spring mean along-shelf dist&quot;, stacked = &quot;A&quot;, rel.y.num = 1.1, suppressAxis = T, line.forward = T, tol = 0.15, x.start = 1963, end.start = 2007, cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;black sea bass fall mean along-shelf dist&quot;, stacked = &quot;B&quot;, rel.y.num = 1.1, tol = 0.15, end.start = 2007, cex.stacked = 1.5) soe.stacked.axis(&quot;Year&quot;, &quot;Mean along-shelf distance, km&quot;, y.line = 3.0) Figure 31.1: Black sea bass along-shelf distance trends in spring (A) and fall (B). opar &lt;- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(3.5, 5, 2, 4)) soe.plot(SOE.data, &quot;Time&quot;, &quot;black sea bass spring mean depth&quot;, stacked = &quot;A&quot;, rel.y.num = 1.1, suppressAxis = T, tol = 0.15, x.start = 1963, end.start = 2007, cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;black sea bass fall mean depth&quot;, stacked = &quot;B&quot;, rel.y.num = 1.1, tol = 0.15, end.start = 2007,line.forward = T, cex.stacked = 1.5) soe.stacked.axis(&quot;Year&quot;, &quot;Mean depth, m&quot;, y.line = 3.0) Figure 31.2: Mean depth of black sea bass in spring (A) and fall (B) on the Northeast Continental Shelf. References "],
["survdat.html", "32 Survey Data 32.1 Methods", " 32 Survey Data Description: Survdat (Survey database) Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Database pull Contributor(s): Sean Lucey Data steward: Sean Lucey sean.lucey@noaa.gov Point of contact: Sean Lucey sean.lucey@noaa.gov Public availability statement: Source data are available to qualified researchers upon request (see “Access Information” here). Derived data used in SOE reports are available here. 32.1 Methods The Northeast Fisheries Science Center (NEFSC) has been conducting standardized bottom trawl surveys in the fall since 1963 and spring since 1968. The surveys follow a stratified random design. Fish species and several invertebrate species are enumerated on a tow by tow basis (Azarovitz 1981). The data are housed in the NEFSC’s survey database (SVDBS) maintained by the Ecosystem Survey Branch. Direct pulls from the database are not advisable as there have been several gear modifications and vessel changes over the course of the time series (Miller et al. 2010). Survdat was developed as a database query that applies the appropriate calibration factors for a seamless time series since the 1960s. As such, it is the base for many of the other analyses conducted for the State of the Ecosystem report that involve fisheries independent data. The Survdat script can be broken down into two sections. The first pulls the raw data from SVDBS. While the script is able to pull data from more than just the spring and fall bottom trawl surveys, for the purposes of the State of the Ecosystem reports only the spring and fall data are used. Survdat identifies those research cruises associated with the seasonal bottom trawl surveys and pulls the station and biological data. Station data includes tow identification (cruise, station, and stratum), tow location and date, as well as several environmental variables (depth, surface/bottom salinity, and surface/bottom temperature). Stations are filtered for representativness using a station, haul, gear (SHG) code for tows prior to 2009 and a tow, operations, gear, and aquisition (TOGA) code from 2009 onward. The codes that correspond to a representative tow (SHG &lt;= 136 or TOGA &lt;= 1324) are the same used by assessment biologists at the NEFSC. Biological data includes the total biomass and abundance by species, as well as lengths and number at length. The second section of the Survdat script applies the calibration factors. There are four calibrartion factors applied (Table 17.1). Calibration factors are pulled directly from SVDBS. Vessel conversions were made from either the NOAA Ship Delaware II or NOAA Ship Henry Bigelow to the NOAA Ship Albatross IV which was the primary vessel for most of the time series. The Albatross was decommisioned in 2009 and the Bigelow is now the primary vessel for the bottom trawl survey. Table 32.1: Calibration factors for NEFSC trawl survey data Name Code Applied Door Conversion DCF &lt;1985 Net Conversion GCF 1973 - 1981 (Spring) Vessel Conversion I VCF Delaware II records Vessel Conversion II BCF Henry Bigelow records The output from Survdat is an RData file that contains all the station and biological data, corrected as noted above, from the NEFSC Spring Bottom Trawl Survey and NEFSC Fall Bottom Trawl Survey. The RData file is a data.table, a powerful wrapper for the base data.frame (https://cran.r-project.org/web/packages/data.table/data.table.pdf). There are also a series of tools that have been developed in order to utilize the Survdat data set (https://github.com/slucey/RSurvey). 32.1.1 Data sources Survdat is a database query of the NEFSC survey database (SVDBS).These data are available to qualified researchers upon request. More information on the data request process is available under the “Access Information” field here. 32.1.2 Data extraction Extraction methods are described above. The following is the R code used in the data extraction process. #Survdat.r #This script will generate data from the NEFSC bottom trawl surveys #SML #------------------------------------------------------------------------------- #User parameters if(Sys.info()[&#39;sysname&#39;]==&quot;Windows&quot;){ data.dir &lt;- &quot;L:\\\\Rworkspace\\\\RSurvey&quot; out.dir &lt;- &quot;L:\\\\EcoAP\\\\Data\\\\survey&quot; memory.limit(4000) } if(Sys.info()[&#39;sysname&#39;]==&quot;Linux&quot;){ data.dir &lt;- &quot;/home/slucey/slucey/Rworkspace/RSurvey&quot; out.dir &lt;- &quot;/home/slucey/slucey/EcoAP/Data/survey&quot; uid &lt;- &#39;slucey&#39; cat(&quot;Oracle Password: &quot;) pwd &lt;- scan(stdin(), character(), n = 1) } shg.check &lt;- &#39;y&#39; # y = use only SHG &lt;=136 or TOGA &lt;= 1324 (&gt;2008) raw.check &lt;- &#39;n&#39; # y = save data without conversions (survdat.raw), will still # save data with conversions (survdat) all.season &lt;- &#39;n&#39; # y = save data with purpose code 10 not just spring/fall # (survdat.allseason), will not save survdat regular use.SAD &lt;- &#39;y&#39; # y = grab data from Survey Analysis Database (SAD) for # assessed species #------------------------------------------------------------------------------- #Required packages library(RODBC); library(data.table) #------------------------------------------------------------------------------- #Created functions #Convert output to text for RODBC query sqltext &lt;- function(x){ out &lt;- x[1] if(length(x) &gt; 1){ for(i in 2:length(x)){ out &lt;- paste(out, x[i], sep = &quot;&#39;,&#39;&quot;) } } out &lt;- paste(&quot;&#39;&quot;, out, &quot;&#39;&quot;, sep = &#39;&#39;) return(out) } #------------------------------------------------------------------------------- #Begin script if(Sys.info()[&#39;sysname&#39;]==&quot;Windows&quot;){ channel &lt;- odbcDriverConnect() } else { channel &lt;- odbcConnect(&#39;sole&#39;, uid, pwd) } #Generate cruise list if(all.season == &#39;n&#39;){ cruise.qry &lt;- &quot;select unique year, cruise6, svvessel, season from mstr_cruise where purpose_code = 10 and year &gt;= 1963 and (season = &#39;FALL&#39; or season = &#39;SPRING&#39;) order by year, cruise6&quot; } if(all.season == &#39;y&#39;){ cruise.qry &lt;- &quot;select unique year, cruise6, svvessel, season from mstr_cruise where purpose_code = 10 and year &gt;= 1963 order by year, cruise6&quot; } cruise &lt;- as.data.table(sqlQuery(channel, cruise.qry)) cruise &lt;- na.omit(cruise) setkey(cruise, CRUISE6, SVVESSEL) #Use cruise codes to select other data cruise6 &lt;- sqltext(cruise$CRUISE6) #Station data if(shg.check == &#39;y&#39;){ preHB.station.qry &lt;- paste(&quot;select unique cruise6, svvessel, station, stratum, tow, decdeg_beglat as lat, decdeg_beglon as lon, begin_est_towdate as est_towdate, avgdepth as depth, surftemp, surfsalin, bottemp, botsalin from Union_fscs_svsta where cruise6 in (&quot;, cruise6, &quot;) and SHG &lt;= 136 and cruise6 &lt;= 200900 order by cruise6, station&quot;, sep=&#39;&#39;) HB.station.qry &lt;- paste(&quot;select unique cruise6, svvessel, station, stratum, tow, decdeg_beglat as lat, decdeg_beglon as lon, begin_est_towdate as est_towdate, avgdepth as depth, surftemp, surfsalin, bottemp, botsalin from Union_fscs_svsta where cruise6 in (&quot;, cruise6, &quot;) and TOGA &lt;= 1324 and cruise6 &gt; 200900 order by cruise6, station&quot;, sep=&#39;&#39;) preHB.sta &lt;- as.data.table(sqlQuery(channel, preHB.station.qry)) HB.sta &lt;- as.data.table(sqlQuery(channel, HB.station.qry)) station &lt;- rbindlist(list(preHB.sta, HB.sta)) } if(shg.check == &#39;n&#39;){ station.qry &lt;- paste(&quot;select unique cruise6, svvessel, station, stratum, tow, decdeg_beglat as lat, decdeg_beglon as lon, begin_est_towdate as est_towdate, avgdepth as depth, surftemp, surfsalin, bottemp, botsalin from UNION_FSCS_SVSTA where cruise6 in (&quot;, cruise6, &quot;) order by cruise6, station&quot;, sep=&#39;&#39;) station &lt;- as.data.table(sqlQuery(channel, station.qry)) } setkey(station, CRUISE6, SVVESSEL) #merge cruise and station survdat &lt;- merge(cruise, station) #Catch data catch.qry &lt;- paste(&quot;select cruise6, station, stratum, tow, svspp, catchsex, expcatchnum as abundance, expcatchwt as biomass from UNION_FSCS_SVCAT where cruise6 in (&quot;, cruise6, &quot;) and stratum not like &#39;YT%&#39; order by cruise6, station, svspp&quot;, sep=&#39;&#39;) catch &lt;- as.data.table(sqlQuery(channel, catch.qry)) setkey(catch, CRUISE6, STATION, STRATUM, TOW) #merge with survdat setkey(survdat, CRUISE6, STATION, STRATUM, TOW) survdat &lt;- merge(survdat, catch, by = key(survdat)) #Length data length.qry &lt;- paste(&quot;select cruise6, station, stratum, tow, svspp, catchsex, length, expnumlen as numlen from UNION_FSCS_SVLEN where cruise6 in (&quot;, cruise6, &quot;) and stratum not like &#39;YT%&#39; order by cruise6, station, svspp, length&quot;, sep=&#39;&#39;) len &lt;- as.data.table(sqlQuery(channel, length.qry)) setkey(len, CRUISE6, STATION, STRATUM, TOW, SVSPP, CATCHSEX) #merge with survdat setkey(survdat, CRUISE6, STATION, STRATUM, TOW, SVSPP, CATCHSEX) survdat &lt;- merge(survdat, len, all.x = T) if(raw.check == &#39;y&#39;){ survdat.raw &lt;- survdat save(survdat.raw, file = paste(out.dir, &quot;Survdat_raw.RData&quot;, sep =&#39;&#39;)) } #Conversion Factors #need to make abundance column a double instead of an integer survdat[, ABUNDANCE := as.double(ABUNDANCE)] #Grab all conversion factors off the network convert.qry &lt;- &quot;select * from survan_conversion_factors&quot; convert &lt;- as.data.table(sqlQuery(channel,convert.qry)) #DCF &lt; 1985 Door Conversion dcf.spp &lt;- convert[DCF_WT &gt; 0, SVSPP] for(i in 1:length(dcf.spp)){ survdat[YEAR &lt; 1985 &amp; SVSPP == dcf.spp[i], BIOMASS := BIOMASS * convert[SVSPP == dcf.spp[i], DCF_WT]] } dcf.spp &lt;- convert[DCF_NUM &gt; 0, SVSPP] for(i in 1:length(dcf.spp)){ survdat[YEAR &lt; 1985 &amp; SVSPP == dcf.spp[i], ABUNDANCE := round(ABUNDANCE * convert[SVSPP == dcf.spp[i], DCF_NUM])] } #GCF Spring 1973-1981 Net Conversion gcf.spp &lt;- convert[GCF_WT &gt; 0, SVSPP] for(i in 1:length(gcf.spp)){ survdat[SEASON == &#39;SPRING&#39; &amp; YEAR &gt; 1972 &amp; YEAR &lt; 1982 &amp; SVSPP == gcf.spp[i], BIOMASS := BIOMASS / convert[SVSPP == gcf.spp[i], GCF_WT]] } gcf.spp &lt;- convert[GCF_NUM &gt; 0, SVSPP] for(i in 1:length(gcf.spp)){ survdat[SEASON == &#39;SPRING&#39; &amp; YEAR &gt; 1972 &amp; YEAR &lt; 1982 &amp; SVSPP == gcf.spp[i], ABUNDANCE := round(ABUNDANCE / convert[SVSPP == gcf.spp[i], GCF_NUM])] } #VCF SVVESSEL = DE Vessel Conversion vcf.spp &lt;- convert[VCF_WT &gt; 0, SVSPP] for(i in 1:length(vcf.spp)){ survdat[SVVESSEL == &#39;DE&#39; &amp; SVSPP == vcf.spp[i], BIOMASS := BIOMASS * convert[SVSPP == vcf.spp[i], VCF_WT]] } vcf.spp &lt;- convert[VCF_NUM &gt; 0, SVSPP] for(i in 1:length(vcf.spp)){ survdat[SVVESSEL == &#39;DE&#39; &amp; SVSPP == vcf.spp[i], ABUNDANCE := round(ABUNDANCE * convert[SVSPP == vcf.spp[i], VCF_NUM])] } #Bigelow &gt;2008 Vessel Conversion - need flat files (not on network) #Use Bigelow conversions for Pisces as well (PC) big.fall &lt;- as.data.table(read.csv(file.path(data.dir, &#39;bigelow_fall_calibration.csv&#39;))) big.spring &lt;- as.data.table(read.csv(file.path(data.dir, &#39;bigelow_spring_calibration.csv&#39;))) bf.spp &lt;- big.fall[pW != 1, svspp] for(i in 1:length(bf.spp)){ survdat[SVVESSEL %in% c(&#39;HB&#39;, &#39;PC&#39;) &amp; SEASON == &#39;FALL&#39; &amp; SVSPP == bf.spp[i], BIOMASS := BIOMASS / big.fall[svspp == bf.spp[i], pW]] } bf.spp &lt;- big.fall[pw != 1, svspp] for(i in 1:length(bf.spp)){ survdat[SVVESSEL %in% c(&#39;HB&#39;, &#39;PC&#39;) &amp; SEASON == &#39;FALL&#39; &amp; SVSPP == bf.spp[i], ABUNDANCE := round(ABUNDANCE / big.fall[svspp == bf.spp[i], pw])] } bs.spp &lt;- big.spring[pW != 1, svspp] for(i in 1:length(bs.spp)){ survdat[SVVESSEL %in% c(&#39;HB&#39;, &#39;PC&#39;) &amp; SEASON == &#39;SPRING&#39; &amp; SVSPP == bs.spp[i], BIOMASS := BIOMASS / big.spring[svspp == bs.spp[i], pW]] } bs.spp &lt;- big.spring[pw != 1, svspp] for(i in 1:length(bs.spp)){ survdat[SVVESSEL %in% c(&#39;HB&#39;, &#39;PC&#39;) &amp; SEASON == &#39;SPRING&#39; &amp; SVSPP == bs.spp[i], ABUNDANCE := round(ABUNDANCE / big.spring[svspp == bs.spp[i], pw])] } if(use.SAD == &#39;y&#39;){ sad.qry &lt;- &quot;select svspp, cruise6, stratum, tow, station, sex as catchsex, catch_wt_B_cal, catch_no_B_cal, length, length_no_B_cal from STOCKEFF.I_SV_MERGED_CATCH_CALIB_O&quot; sad &lt;- as.data.table(sqlQuery(channel, sad.qry)) setkey(sad, CRUISE6, STRATUM, TOW, STATION, SVSPP, CATCHSEX, LENGTH) sad &lt;- unique(sad) survdat &lt;- merge(survdat, sad, by = key(sad), all.x = T) #Carry over SAD values to survdat columns and delete SAD columns survdat[!is.na(CATCH_WT_B_CAL), BIOMASS := CATCH_WT_B_CAL] survdat[!is.na(CATCH_NO_B_CAL), ABUNDANCE := CATCH_NO_B_CAL] survdat[, NUMLEN := as.double(NUMLEN)] survdat[!is.na(LENGTH_NO_B_CAL), NUMLEN := LENGTH_NO_B_CAL] survdat[, c(&#39;CATCH_WT_B_CAL&#39;, &#39;CATCH_NO_B_CAL&#39;, &#39;LENGTH_NO_B_CAL&#39;) := NULL] } odbcClose(channel) if(all.season == &#39;n&#39;) save(survdat, file = file.path(out.dir, &quot;Survdat.RData&quot;)) if(all.season == &#39;y&#39;) save(survdat, file = file.path(out.dir, &quot;Survdat_allseason.RData&quot;)) 32.1.3 Data analysis The fisheries independent data contained within the Survdat is used in a variety of products; the more complicated analyses are detailed in their own sections. The most straightforward use of this data is for the resource species aggregate biomass indicators. For the purposes of the aggregate biomass indicators, fall and spring survey data are treated separately. Additionally, all length data is dropped and species seperated by sex at the catch level are merged back together. For the aggregate biomass indicators, Survdat is first post stratified into Ecological Production Units. Stations are labeled by the EPU they fall within using the over function from the rdgal R package (Bivand, Keitt, and Rowlingson 2018). Next, the total number of stations within each EPU per year is counted using unique station records. Biomass is summed by species per year per EPU. Those sums are divided by the appropriate station count to get the EPU mean. Finally, the mean biomasses are summed by aggregate groups. These steps are encompassed in the processing code shown below, which also includes steps taken to format the data set for inclusion in the ecodata R package. #Set up for finding survey proportions #This script was adapted from code written by Sean Lucey at the NEFSC. Note that you #will need the latest survdat file to get this to run successfully. data.dir &lt;- here::here(&quot;data-raw&quot;) #------------------------------------------------------------------------- #Required packages #devtools::install_github(&quot;slucey/RSurvey/Survdat&quot;) library(data.table); library(rgdal); library(Survdat) library(dplyr);library(sf);library(tidyr) #------------------------------------------------------------------------------- #Load raw data and get &quot;strata&quot; aka EPUs here load(file.path(data.dir, &#39;Survdat.RData&#39;)) strata &lt;- ecodata::epu_sf %&gt;% as(&quot;Spatial&quot;) #Generate area table strat.area &lt;- getarea(strata, &#39;EPU&#39;) #Post stratify data survdat.EPU &lt;- poststrat(survdat, strata, &#39;EPU&#39;) setnames(survdat.EPU, &#39;newstrata&#39;, &#39;EPU&#39;) #Subset by season/ strata set fall &lt;- survdat.EPU[SEASON == &#39;FALL&#39;, ] spring &lt;- survdat.EPU[SEASON == &#39;SPRING&#39;, ] #Run stratification prep fall.prep &lt;- stratprep(fall, strat.area, strat.col = &#39;EPU&#39;, area.col = &#39;Area&#39;) spring.prep &lt;- stratprep(spring, strat.area, strat.col = &#39;EPU&#39;, area.col = &#39;Area&#39;) #Calculate mean weight/tow by aggregate groups #n tows n.tows.fall &lt;- unique(fall.prep[, list(YEAR, EPU, ntows)]) n.tows.spring &lt;- unique(spring.prep[, list(YEAR, EPU, ntows)]) #drop length data setkey(fall.prep, YEAR, EPU, STATION, STRATUM, SVSPP, CATCHSEX) fall.prep &lt;- unique(fall.prep, by = key(fall.prep)) fall.prep[, c(&#39;LENGTH&#39;, &#39;NUMLEN&#39;) := NULL] setkey(spring.prep, YEAR, EPU, STATION, STRATUM, SVSPP, CATCHSEX) spring.prep &lt;- unique(spring.prep, by = key(spring.prep)) spring.prep[, c(&#39;LENGTH&#39;, &#39;NUMLEN&#39;) := NULL] #Merge Sexed species setkey(fall.prep, YEAR, EPU, STATION, STRATUM, SVSPP) fall.prep &lt;- fall.prep[, sum(BIOMASS, na.rm = T), by = key(fall.prep)] setkey(spring.prep, YEAR, EPU, STATION, STRATUM, SVSPP) spring.prep &lt;- spring.prep[, sum(BIOMASS, na.rm = T), by = key(spring.prep)] #Sum biomass within an EPU fall.sum &lt;- fall.prep[, sum(V1), by = c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;SVSPP&#39;)] spring.sum &lt;- spring.prep[, sum(V1), by = c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;SVSPP&#39;)] #Merge sum with station count fall.sum &lt;- merge(fall.sum, n.tows.fall, by = c(&#39;YEAR&#39;, &#39;EPU&#39;)) spring.sum &lt;- merge(spring.sum, n.tows.spring, by = c(&#39;YEAR&#39;, &#39;EPU&#39;)) #Calculate mean weight per tow fall.sum[, kg.per.tow := V1 / ntows] fall.mean &lt;- fall.sum[, list(YEAR, EPU, SVSPP, kg.per.tow)] spring.sum[, kg.per.tow := V1 / ntows] spring.mean &lt;- spring.sum[, list(YEAR, EPU, SVSPP, kg.per.tow)] #get species groupings groups &lt;- ecodata::species_groupings %&gt;% dplyr::select(group = SOE_18, SVSPP, comname = COMNAME) %&gt;% filter(!is.na(SVSPP)) %&gt;% as.data.table() #Aggregate by conceptual model groupings fall &lt;- merge(fall.mean, unique(groups[, list(SVSPP, group, comname)]), by = &#39;SVSPP&#39;, all.x = T) spring &lt;- merge(spring.mean, unique(groups[, list(SVSPP, group, comname)]), by = &#39;SVSPP&#39;, all.x = T) #Fix NA group to other fall[ is.na(group), group := &#39;Other&#39;] spring[is.na(group), group := &#39;Other&#39;] #Before aggregating, save data with species ids spring$season &lt;- &quot;spring&quot; fall$season &lt;- &quot;fall&quot; survey_biomass &lt;- rbind(spring, fall) %&gt;% filter(!is.na(SVSPP)) managed &lt;- ecodata::species_groupings %&gt;% dplyr::select(comname = COMNAME, Fed_Managed) nefsc_survey_disaggregated &lt;- survey_biomass %&gt;% group_by(EPU, group, season, YEAR) %&gt;% mutate(Total = sum(kg.per.tow)) %&gt;% mutate(Prop = kg.per.tow/Total) %&gt;% filter(!group %in% c(&quot;Apex Predator&quot;,&quot;Other&quot;), EPU != &quot;SS&quot;) %&gt;% dplyr::rename(Time = YEAR) %&gt;% as.data.frame() %&gt;% left_join(.,managed,by = c(&quot;comname&quot;) ) %&gt;% distinct() %&gt;% complete(Time = full_seq(min(.$Time):max(.$Time),1), nesting(EPU, Fed_Managed,season, group, comname, SVSPP)) %&gt;% dplyr::rename(Management = Fed_Managed, `Feeding guild` = group, Season = season, Proportion = Prop) usethis::use_data(nefsc_survey_disaggregated, overwrite = TRUE) ## GET AGGREGATED SURVEY DATA-------------------------------------------------------- fall.agg &lt;- fall[, sum(kg.per.tow), by = c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;group&#39;)] spring.agg &lt;- spring[, sum(kg.per.tow), by = c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;group&#39;)] # #Total fall.agg[, Total := sum(V1), by = c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;group&#39;)] spring.agg[, Total := sum(V1), by = c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;group&#39;)] #Get in correct long format for SOE #By feeding guild fall.tot &lt;- copy(fall.agg) fall.tot[, Var := paste(group, &#39;Fall Biomass Index&#39;)] setnames(fall.tot, c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;V1&#39;), c(&#39;Time&#39;, &#39;Region&#39;, &#39;Value&#39;)) fall.tot[, Units := &#39;kg tow^-1&#39;] fall.tot &lt;- unique(fall.tot) fall.tot &lt;- mutate(fall.tot, Season = &quot;Fall&quot;) spring.tot &lt;- copy(spring.agg) spring.tot[, Var := paste(group, &#39;Spring Biomass Index&#39;)] setnames(spring.tot, c(&#39;YEAR&#39;, &#39;EPU&#39;, &#39;V1&#39;), c(&#39;Time&#39;, &#39;Region&#39;, &#39;Value&#39;)) spring.tot[, Units := &#39;kg tow^-1&#39;] spring.tot &lt;- unique(spring.tot) spring.tot &lt;- mutate(spring.tot, Season = &quot;Spring&quot;) agg_survey &lt;- rbind(spring.tot, fall.tot) #Merge into one data set nefsc_survey &lt;- agg_survey %&gt;% dplyr::select(Time, EPU = Region, Var, Units, Value) %&gt;% distinct() %&gt;% complete(Time = full_seq(min(.$Time):max(.$Time),1), nesting(EPU,Var)) usethis::use_data(nefsc_survey, overwrite = T) 32.1.4 Plotting # Relative working directories data.dir &lt;- here::here(&#39;data&#39;) r.dir &lt;- here::here(&#39;R&#39;) # Load data load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) # Source plotting functions source(file.path(r.dir,&quot;BasePlot_source.R&quot;)) opar &lt;- par(mfrow = c(4, 1), mar = c(0, 0, 0, 0), oma = c(4, 6.5, 2, 6)) soe.plot(SOE.data, &quot;Time&quot;, &quot;Piscivore Fall Biomass Index MAB&quot;, stacked = &quot;A&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Planktivore Fall Biomass Index MAB&quot;, stacked = &quot;B&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Benthivore Fall Biomass Index MAB&quot;, stacked = &quot;C&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Benthos Fall Biomass Index MAB&quot;, stacked = &quot;D&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) #soe.plot(SOE.data, &quot;Time&quot;, &quot;Total Fall Biomass MAB&quot;, stacked = &quot;E&quot;,status = F, # endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 1) soe.stacked.axis(&quot;Year&quot;, expression(&quot;Biomass, kg tow&quot;^-1), y.line = 3.0, x.line = 2.7) opar &lt;- par(mfrow = c(4, 1), mar = c(0, 0, 0, 0), oma = c(4, 6.5, 2, 6)) soe.plot(SOE.data, &quot;Time&quot;, &quot;Piscivore Spring Biomass Index MAB&quot;, stacked = &quot;A&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Planktivore Spring Biomass Index MAB&quot;, stacked = &quot;B&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Benthivore Spring Biomass Index MAB&quot;, stacked = &quot;C&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) soe.plot(SOE.data, &quot;Time&quot;, &quot;Benthos Spring Biomass Index MAB&quot;, stacked = &quot;D&quot;, rel.y.num = 1.1, end.start = 2007,cex.stacked = 1.5) #soe.plot(SOE.data, &quot;Time&quot;, &quot;Total Spring Biomass MAB&quot;, stacked = &quot;E&quot;,status = F, # endshade = T, rel.y.num = 1.1, full.trend = T, scale.axis = 1) soe.stacked.axis(&quot;Year&quot;, expression(&quot;Biomass, kg tow&quot;^-1*&#39;&#39;), y.line = 3.0, x.line = 2.7) Figure 32.1: Fall (left) and spring (right) MAB Survey Biomass (A: Piscivore, B: Planktivore, C: Benthivore, D: Benthos). References "],
["thermal-habitat-projections.html", "33 Thermal Habitat Projections 33.1 Methods", " 33 Thermal Habitat Projections Description: Species Thermal Habitat Projections Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) Indicator category: Published methods Contributor(s): Vincent Saba Data steward: Vincent Saba, vincent.saba@noaa.gov Point of contact: Vincent Saba, vincent.saba@noaa.gov Public availability statement: Source data are available to the public. Model outputs for thermal habitat projections are available here. 33.1 Methods This indicator is based on work reported in Kleisner et al. (2017). 33.1.1 Data sources 33.1.1.1 Global Climate Model Projection We used NOAA GFDL’s CM2.6 simulation consisting of (1) a 1860 pre-industrial control, which brings the climate system into near-equilibrium with 1860 greenhouse gas concentrations, and (2) a transient climate response (2xCO2) simulation where atmospheric CO2 is increased by 1% per year, which results in a doubling of CO2 after 70 years. The climate change response from CM2.6 was based on the difference between these two experimental runs. Refer to Saba et al. (2016) for further details. 33.1.1.2 Modeling Changes in Suitable Thermal Habitat The NOAA NEFSC U.S. Northeast Shelf (NES) bottom trawl survey, which has been conducted for almost 50-years in the spring and fall, provides a rich source of data on historical and current marine species distribution, abundance, and habitat, as well as oceanographic conditions (Azarovitz 1981). The survey was implemented to meet several objectives: (1) monitor trends in abundance, biomass, and recruitment, (2) monitor the geographic distribution of species, (3) monitor ecosystem changes, (4) monitor changes in life history traits (e.g., trends in growth, longevity, mortality, and maturation, and food habits), and (5) collect baseline oceanographic and environmental data. These data can be leveraged for exploring future changes in the patterns of abundance and distribution of species in the region. 33.1.2 Data analysis 33.1.2.1 Global Climate Model Projection The CM2.6 80-year projections can be roughly assigned to a time period by using the IPCC Representative Concentration Pathways (RCPs), which describe four different 21st century pathways of anthropogenic greenhouse gas emissions, air pollutant emissions, and land use (IPCC 2014). There are four RCPs, ranging from a stringent mitigation scenario (RCP2.6), two intermediate scenarios (RCP4.5 and RCP6.0), and one scenario with very high greenhouse gas emissions (RCP8.5). For RCP8.5, the global average temperature at the surface warms by 2C by approximately 2060-2070 relative to the 1986-2005 climatology (see Figure SPM.7a in IPCC, 2013). For CM2.6, the global average temperature warms by 2C by approximately years 60-80 (see Fig. 1 in Winton et al. (2014)). Therefore, the last 20 years of the transient climate response simulation roughly corresponds to 2060-2080 of the RCP8.5 scenario. Here, the monthly differences in surface and bottom temperatures (‘deltas’) for spring (February-April) and fall (September- November) are added to an average annual temperature climatology for spring and fall, respectively, derived from observed surface and bottom temperatures to produce an 80-year time series of future bottom and surface temperatures in both seasons. The observed temperatures come from the NEFSC spring and fall bottom trawl surveys conducted from 1968 to 2013 and represent approximately 30,000 observations over the time series. 33.1.2.2 Modeling Changes in Suitable Thermal Habitat We modeled individual species thermal habitat across the whole U.S. NES and not by sub-region because we did not want to assume that species would necessarily maintain these assemblages in the future. Indeed, the goal here is to determine future patterns of thermal habitat availability for species on the U.S. NES in more broad terms. We fit one GAM based on both spring and fall data (i.e., an annual model as opposed to separate spring and fall models) and use it to project potential changes in distribution and magnitude of biomass separately for each season for each species. By creating a single annual model based on temperature data from both spring and fall, we ensure that the full thermal envelope of each species is represented. For example, if a species with a wide thermal tolerance has historically been found in cooler waters in the spring, and in warmer waters in the fall, an annual model will ensure that if there are warmer waters in the spring in the future, that species will have the potential to inhabit those areas. Additionally, because the trawl survey data are subject to many zero observations, we use delta-lognormal GAMs (Wood 2011), which model presence-absence separately from logged positive observations. The response variables in each of the GAMs are presence/absence and logged positive biomass of each assemblage or individual species, respectively. A binomial link function is used in the presence/absence models and a Gaussian link function is used in the models with logged positive biomass. The predictor variables are surface and bottom temperature and depth (all measured by the survey at each station), fit with penalized regression splines, and survey stratum, which accounts for differences in regional habitat quality across the survey region. Stratum may be considered to account for additional information not explicitly measured by the survey (e.g., bottom rugosity). Predictions of species abundance are calculated as the product of the predictions from the presence-absence model, the exponentiated predictions from the logged positive biomass model, and a correction factor to account for the retransformation bias associated with the log transformation (Duan 1983; and see Pinsky et al. 2013). We calculated the suitable thermal habitat both in terms of changes in ‘suitable thermal abundance’, defined as the species density possible given appropriate temperature, depth and bathymetric conditions, and changes in ‘suitable thermal area’, defined as the size of the physical area potentially occupied by a species given appropriate temperature, depth and bathymetric conditions. Suitable thermal abundance is determined from the predictions from the GAMs (i.e., a prediction of biomass). However, this quantity should not be interpreted directly as a change in future abundance or biomass, but instead as the potential abundance of a species in the future given changes in temperature and holding all else (e.g., fishing effort, species interactions, productivity, etc.) constant. Suitable thermal area is determined as a change in the suitable area that a species distribution occupies in the future and is derived from the area of the kernel density of the distribution. To ensure that the estimates are conservative, we select all points with values greater than one standard deviation above the mean. We then compute the area of these kernels using the gArea function from the ‘rgeos’ package in R (Bivand et al. 2011). 33.1.3 Plotting Figure 33.1: Current thermal habitat estimate (A), and 20-40 year thermal habitat projection (B) for summer flounder on the Northeast Continental Shelf. Note: The thermal habitat model output for all species presented in State of the Ecosystem reports is accessible through the NEFSC ERDDAP server. References "],
["trend-analysis.html", "34 Trend Analysis 34.1 Methods", " 34 Trend Analysis Description: Time series trend analysis Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) Indicator category: Extensive analysis, not yet published Contributor(s): Sean Hardison, Charles Perretti, Geret DePiper Data steward: NA Point of contact: Sean Hardison, sean.hardison@noaa.gov Public availability statement: NA 34.1 Methods Summarizing trends for ecosystem indicators is desirable, but the power of statistical tests to detect a trend is hampered by low sample size and autocorrelated observations (see Nicholson and Jennings 2004; Wagner et al. 2013; Storch 1999). Prior to 2018, time series indicators in State of the Ecosystem reports were presented with trend lines based on a Mann-Kendall test for monotonic trends to test significance (p &lt; 0.05) of both long term (full time series) and recent (2007–2016) trends, although not all time series were considered for trend analysis due to limited series lengths. There was also concern that a Mann-Kendall test would not account for any autocorrelation present in SOE indicators. In a simulation study (Hardison et al. 2019), we explored the effect of time series length and autocorrelation strength on statistical power of three trend detection methods: a generalized least squares model selection approach, the Mann-Kendall test, and Mann-Kendall test with trend-free pre-whitening. Methods were applied to simulated time series of varying trend and autocorrelation strengths. Overall, when sample size was low (N = 10) there were high rates of false trend detection, and similarly, low rates of true trend detection. Both of these forms of error were further amplified by autocorrelation in the trend residuals. Based on these findings, we selected a minimum series length of N = 30 for indicator time series before assessing trend. We also chose to use a GLS model selection (GLS-MS) approach to evaluate indicator trends in the 2018 (and future) State of the Ecosystem reports, as this approach performed best overall in the simulation study. GLS-MS also allowed for both linear and quadratic model fits and quantification of uncertainty in trend estimates. The model selection procedure for the GLS approach fits four models to each time series and selects the best fitting model using AICc. The models are, 1) linear trend with uncorrelated residuals, 2) linear trend with correlated residuals, 3) quadratic trend with uncorrelated residuals, and 4) quadratic trend with correlated residuals. I.e., the models are of the form \\[ Y_t = \\alpha_0 + \\alpha_1X_t + \\alpha_2X_t^2 + \\epsilon_t\\] \\[\\epsilon_t = \\rho\\epsilon_{t-1} + \\omega_t\\] \\[w_t \\sim N(0, \\sigma^2)\\] Where \\(Y_t\\) is the observation in time \\(t\\), \\(X_t\\) is the time index, \\(\\epsilon_t\\) is the residual in time \\(t\\), and \\(\\omega_t\\) is a normally distributed random variable. Setting \\(\\alpha_2 = 0\\) yields the linear trend model, and \\(\\rho = 0\\) yields the uncorrelated residuals model. The best fit model was tested against the null hypothesis of no trend through a likelihood ratio test (p &lt; 0.05). All models were fit using the R package nlme (Pinheiro et al. 2017) and AICc was calculated using the R package AICcmodavg (Mazerolle 2017). In SOE time series figures, significant positive trends were colored orange, and negative trends purple. 34.1.1 Data source(s) NA 34.1.2 Data extraction NA 34.1.3 Data analysis #R packages library(dplyr) library(nlme) library(AICcmodavg) library(data.table) # data.dir &lt;- &quot;./data&quot; # load(file.path(data.dir, &quot;SOE_data_2018.Rdata&quot;)) #--------------------------------GLS Model Selection-----------------------------# fit_lm &lt;- function(dat) { constant_norm &lt;- nlme::gls(series ~ 1, data = dat) constant_ar1 &lt;- try(nlme::gls(series ~ 1, data = dat, correlation = nlme::corAR1(form = ~time))) if (class(constant_ar1) == &quot;try-error&quot;){ return(best_lm &lt;- data.frame(model = NA, aicc = NA, coefs..Intercept = NA, coefs.time = NA, coefs.time2 = NA, pval = NA)) } # Linear model with normal error linear_norm &lt;- nlme::gls(series ~ time, data = dat) # Linear model with AR1 error linear_ar1 &lt;- try(nlme::gls(series ~ time, data = dat, correlation = nlme::corAR1(form = ~time))) if (class(linear_ar1) == &quot;try-error&quot;){ return(best_lm &lt;- data.frame(model = NA, aicc = NA, coefs..Intercept = NA, coefs.time = NA, coefs.time2 = NA, pval = NA)) } # Polynomial model with normal error dat$time2 &lt;- dat$time^2 poly_norm &lt;- nlme::gls(series ~ time + time2, data = dat) # Polynomial model with AR1 error poly_ar1 &lt;- try(nlme::gls(series ~ time + time2, data = dat, correlation = nlme::corAR1(form = ~time))) if (class(poly_ar1) == &quot;try-error&quot;){ return(best_lm &lt;- data.frame(model = NA, aicc = NA, coefs..Intercept = NA, coefs.time = NA, coefs.time2 = NA, pval = NA)) } # Calculate AICs for all models df_aicc &lt;- data.frame(model = c(&quot;poly_norm&quot;, &quot;poly_ar1&quot;, &quot;linear_norm&quot;, &quot;linear_ar1&quot;), aicc = c(AICc(poly_norm), AICc(poly_ar1), AICc(linear_norm), AICc(linear_ar1)), coefs = rbind(coef(poly_norm), coef(poly_ar1), c(coef(linear_norm), NA), c(coef(linear_ar1), NA)), # Calculate overall signifiance (need to use # ML not REML for this) pval = c(anova(update(constant_norm, method = &quot;ML&quot;), update(poly_norm, method = &quot;ML&quot;))$`p-value`[2], anova(update(constant_ar1, method = &quot;ML&quot;), update(poly_ar1, method = &quot;ML&quot;))$`p-value`[2], anova(update(constant_norm, method = &quot;ML&quot;), update(linear_norm, method = &quot;ML&quot;))$`p-value`[2], anova(update(constant_ar1, method = &quot;ML&quot;), update(linear_ar1, method = &quot;ML&quot;))$`p-value`[2])) best_lm &lt;- df_aicc %&gt;% dplyr::filter(aicc == min(aicc)) if (best_lm$model == &quot;poly_norm&quot;) { model &lt;- poly_norm } else if (best_lm$model == &quot;poly_ar1&quot;) { model &lt;- poly_ar1 } else if (best_lm$model == &quot;linear_norm&quot;) { model &lt;- linear_norm } else if (best_lm$model == &quot;linear_ar1&quot;) { model &lt;- linear_ar1 } return(list(p = best_lm$pval, model = model)) } #-------------------------------------Plotting code------------------------------------# soe.plot &lt;- function(data, x.var, y.var, x.label = &#39;&#39;, y.label = &#39;&#39;, tol = 0.1, x.start = NA, x.end = NA, end.start = 2008, bg.col = background, mean_line = T, end.col = recent, stacked = NA, x.line = 2.5, y.line = 3.5, scale.axis = 1, rel.y.num = 1.5, rel.y.text = 1.5, suppressAxis = FALSE,status = F,anomaly = F, endshade = TRUE, full.trend = TRUE, point.cex = 1.5, lwd = 2, ymax = TRUE,ymin = TRUE, y.upper = y.upper, y.lower = y.lower, extra = FALSE, x.var2 = x.var2, y.var2 = y.var2, line.forward = FALSE, mean_line.2 = T, cex.stacked = 1, website = T) { #Select Data x &lt;- data[Var == y.var, ] x &lt;- x[order(x[, get(x.var)]), ] setnames(x, x.var, &#39;X&#39;) #Set common time step if necessary if(is.na(x.start)) x.start &lt;- min(x[, X]) if(is.na(x.end)) x.end &lt;- max(x[, X]) x &lt;- x[X &gt;= x.start, ] #Set up plot parameters if (ymax == TRUE){ y.max &lt;- max(x[, Value]) + tol * max(x[, Value]) } else { y.max &lt;- as.numeric(y.upper) } if (ymin == TRUE){ y.min &lt;- min(x[, Value]) - tol * abs(min(x[, Value])) } else if (ymin == FALSE){ y.min &lt;- as.numeric(y.lower) } y.mean &lt;- mean(x[, Value]) y.sd &lt;- sd(x[, Value]) #Plot blank plot plot(x[X &gt;= x.start, list(X, Var)], xlim = c(x.start, x.end), ylim = c(y.min,y.max), xlab = &#39;&#39;, ylab = &#39;&#39;, axes = F, ty = &#39;n&#39;) #Add background u &lt;- par(&#39;usr&#39;) rect(u[1], u[3], u[2], u[4], border = NA, col = bg.col) #Add end period shading if (endshade == TRUE){ rect(end.start - 0.5, u[3], u[2], u[4], border = NA, col = end.col) } #Add mean line if (anomaly == F){ if (mean_line == TRUE){ abline(h = y.mean, col = &#39;grey&#39;, lwd = 3, lty = 2) } } else if (anomaly == TRUE){ abline(h = 0, col = &#39;grey&#39;, lwd = 3, lty = 2) } #Add x y lines abline(h = u[3], lwd=3) abline(v = u[1], lwd=3) #Add data points/lines points(x[, list(X, Value)], pch = 16, cex = point.cex) lines( x[, list(X, Value)], lwd = lwd) #extra lines if (extra == TRUE){ x2 &lt;- data[Var == y.var2, ] x2 &lt;- x2[order(x2[, get(x.var2)]), ] setnames(x2, x.var2, &#39;X2&#39;) x2 &lt;- x2[X2 &gt;= x.start, ] if (mean_line.2 == TRUE){ abline(h = mean(x2[, Value]), col = &#39;lightcoral&#39;, lwd = 3, lty = 2) } points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = &quot;indianred&quot;) lines( x2[, list(X2, Value)], lwd = lwd, col = &quot;indianred&quot;) } #Add axis if (suppressAxis == FALSE){ if(is.na(stacked)) axis(1, cex.axis = 1) if(!is.na(stacked)){ if(stacked!= &#39;A&#39;) axis(3, cex.axis = 1.5, tck = 0.1, labels = F) } } #Stacked axes with 0 overlap so need to remove labels &lt;- round((axTicks(2) / scale.axis), 5) if(labels[1] == 0) labels[1] &lt;- &#39;&#39; axis(2, at = axTicks(2), labels = labels, cex.axis = rel.y.num, las = T) #Add axis labels if(!website){ if(!is.na(stacked)) text(u[1], u[4], labels = stacked, cex = cex.stacked, adj = c(-0.5, 1.5)) } else if (website){ text(u[1], u[4], labels = &quot;&quot;, cex = cex.stacked, adj = c(-0.5, 1.5)) } if(is.na(stacked)){ mtext(1, text = x.label, line = x.line, cex = 1) mtext(2, text = y.label, line = y.line, cex = rel.y.text) } if (full.trend == T){ #Split data into past decade and full time series dat &lt;- as.data.frame(x[, list(X, Value)]) dat &lt;- dat %&gt;% dplyr::rename(series = Value) %&gt;% mutate(time = seq(1,nrow(dat),1)) # Fit linear model lm_out &lt;- fit_lm(dat = dat) p &lt;- lm_out$p if (p &lt; .05){ newtime &lt;- seq(min(dat$time), max(dat$time), length.out=length(dat$time)) newdata &lt;- data.frame(time = newtime, time2 = newtime^2) lm_pred &lt;- AICcmodavg::predictSE(lm_out$model, newdata = newdata, se.fit = TRUE) year &lt;- seq(x$X[1],x$X[length(x$X)],length.out = length(dat$time)) # Make plot if (lm_pred$fit[length(lm_pred$fit)] &gt; lm_pred$fit[1]){ lines(year, lm_pred$fit, col = main.pos, lwd = 7) points(x[, list(X, Value)], pch = 16, cex = point.cex) lines( x[, list(X, Value)], lwd = lwd) if (line.forward == TRUE){ lines(year, lm_pred$fit, col = main.pos, lwd = 7) } } else if (lm_pred$fit[length(lm_pred$fit)] &lt; lm_pred$fit[1]){ lines(year, lm_pred$fit, col = main.neg, lwd = 7) points(x[, list(X, Value)], pch = 16, cex = point.cex) lines( x[, list(X, Value)], lwd = lwd) if (line.forward == TRUE){ lines(year, lm_pred$fit, col = main.neg, lwd = 7) } } } if (extra == TRUE){ # Second variable dat &lt;- as.data.frame(x2[, list(X2, Value)]) dat &lt;- dat %&gt;% dplyr::rename(series = Value) %&gt;% mutate(time = seq(1,nrow(dat),1)) # Fit linear model lm_out &lt;- fit_lm(dat = dat) p &lt;- lm_out$p points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = &quot;indianred&quot;) lines( x2[, list(X2, Value)], lwd = lwd, col = &quot;indianred&quot;) if (p &lt; .05){ newtime &lt;- seq(min(dat$time), max(dat$time), length.out=length(dat$time)) newdata &lt;- data.frame(time = newtime, time2 = newtime^2) lm_pred &lt;- AICcmodavg::predictSE(lm_out$model, newdata = newdata, se.fit = TRUE) year &lt;- seq(x2$X2[1],x2$X2[length(x2$X2)],length.out =length(dat$time)) # Make plot if (lm_pred$fit[length(lm_pred$fit)] &gt; lm_pred$fit[1] ){ lines(year, lm_pred$fit, col = main.pos, lwd = 7) points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = &quot;indianred&quot;) lines( x2[, list(X2, Value)], lwd = lwd, col = &quot;indianred&quot;) } else if (lm_pred$fit[length(lm_pred$fit)] &lt; lm_pred$fit[1]){ lines(year, lm_pred$fit, col = main.neg, lwd = 7) points(x2[, list(X2, Value)], pch = 16, cex = point.cex, col = &quot;indianred&quot;) lines( x2[, list(X2, Value)], lwd = lwd, col = &quot;indianred&quot;) } } } } } #Add axis labels for stacked plots soe.stacked.axis &lt;- function(x.label, y.label, x.line = 2.5,rel.x.text = 1.5, y.line = 3.5, rel.y.text = 1.5, outer = TRUE){ axis(1, cex.axis = rel.x.text) mtext(1, text = x.label, line = x.line, cex = rel.x.text, outer = outer) mtext(2, text = y.label, line = y.line, cex = rel.y.text, outer = outer) } #Background colors background &lt;- &#39;white&#39; recent &lt;- &#39;#E6E6E6&#39; main.pos &lt;- rgb(253/255, 184/255, 99/255, alpha = .9) main.neg &lt;- rgb(178/255, 171/255, 210/255, alpha = .9) Example plot data.dir &lt;- &quot;data&quot; load(file.path(data.dir,&quot;SOE_data_erddap.Rdata&quot;)) opar &lt;- par(mar = c(4, 6, 2, 6)) soe.plot(SOE.data, &quot;Time&quot;, &quot;Mid-Atlantic Rec catch&quot;, scale.axis = 10^6, end.start = 2008, x.label = &#39;Year&#39;, rel.y.text = 1.5, rel.y.num = 1.1, y.line = 2.5, y.label = expression(&quot;Fish caught, 10&quot;^6 *&quot; n&quot;)) Figure 34.1: Example plot formatted for the report References "],
["zooabund.html", "35 Zooplankton 35.1 Methods", " 35 Zooplankton Description: Annual time series of zooplankton abundance Found in: State of the Ecosystem - Gulf of Maine &amp; Georges Bank (2017, 2018, 2019), State of the Ecosystem - Mid-Atlantic (2017, 2018, 2019) Indicator category: Database pull with analysis; Synthesis of published information; Extensive analysis, not yet published; Published methods Contributor(s): Ryan Morse, Kevin Friedland Data steward: Harvey Walsh, harvey.walsh@noaa.gov; Mike Jones, michael.jones@noaa.gov Point of contact: Ryan Morse, ryan.morse@noaa.gov; Harvey Walsh, harvey.walsh@noaa.gov; Kevin Friedland, kevin.friedland@noaa.gov Public availability statement: Source data are publicly available here. Derived data can be found here. 35.1 Methods 35.1.1 Data sources Zooplankton data are from the NOAA MARMAP and EcoMon cruises detailed extensively in Kane (2007), Kane (2011), and Morse et al. (2017). 35.1.2 Data extraction Data are from the publicly available zooplankton dataset on the NOAA FTP server. The excel file has a list of excluded samples and cruises based on Kane (2007) and Kane (2011). R code used in extraction process. # load data URL=&#39;ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/EcoMon_Plankton_Data_v3_0.xlsx&#39; ZPD=openxlsx::read.xlsx(URL, sheet=&#39;Data&#39;) 35.1.3 Data analysis Annual abundance anomalies Data are processed similarly to Kane (2007) and Perretti et al. (2017b), where a mean annual abundance by date is computed by area for each species meeting inclusion metrics set in Morse et al. (2017). This is accomplished by binning all samples for a given species to bi-monthly collection dates based on median cruise date and taking the mean, then fitting a spline interpolation between mean bi-monthly abundance to give expected abundance on any given day of the year. Abundance anomalies (Figure 35.1) are computed from the expected abundance on the day of sample collection. Abundance anomaly time series are constructed for Centropages typicus, Pseudocalanus spp., Calanus finmarchicus, and total zooplankton biovolume. The small-large copepod size index is computed by averaging the individual abundance anomalies of Pseudocalanus spp., Centropages hamatus, Centropages typicus, and Temora longicornis, and subtracting the abundance anomaly of Calanus finmarchicus. This index tracks the overall dominance of the small bodied copepods relative to the largest copepod in the NEUS region, Calanus finmarchicus. #libraries library(vegan) library(stats) library(mgcv) library(reshape2) library(readxl) library(lubridate) library(sp) library(maptools) library(marmap) library(rgeos) # load data URL=&#39;ftp://ftp.nefsc.noaa.gov/pub/hydro/zooplankton_data/EcoMon_Plankton_Data_v3_0.xlsx&#39; ZPD=openxlsx::read.xlsx(URL, sheet=&#39;Data&#39;) # Fix date, time dt=as_date(ZPD$date, origin = &quot;1899-12-30&quot;) DOY=yday(dt) #day of year month=as.numeric(format(dt, &#39;%m&#39;)) year=as.numeric(format(dt, &#39;%Y&#39;)) ZPD$year=year ZPD$month=month ZPD$dt=dt ZPD$DOY=DOY ZPD$day=as.numeric(format(dt, &#39;%d&#39;)) ZPD$lat2=ceiling(ZPD$lat) #use for binning into 1 degree bins for removal of undersampled bins ZPD$lon2=floor(ZPD$lon) #use for binning into 1 degree bins for removal of undersampled bins # ASSIGN EPU based on GPS data ## load shapefiles from EDAB EPU analysis ## not available here gbk=readShapeSpatial(&quot;EPU_GBKPoly.shp&quot;) gom=readShapeSpatial(&quot;EPU_GOMPoly.shp&quot;) mab=readShapeSpatial(&quot;EPU_MABPoly.shp&quot;) scs=readShapeSpatial(&quot;EPU_SCSPoly.shp&quot;) #combine shapefiles GOM and GBK gom.gbk.shp=gUnion(gom, gbk, byid=F, id=NULL) gom.gbk.shp=gUnion(gom, gbk, byid=F, id=NULL) gom.scs.shp=gUnion(gom, scs, byid=F, id=NULL) mab.gbk.shp=gUnion(mab, gbk, byid=F, id=NULL) NES.shp=gUnion(mab.gbk.shp, gom.scs.shp, byid=F, id=NULL) #extract just lat/lons for lines gbk.lonlat =as.data.frame(lapply(slot(gbk, &quot;polygons&quot;), function(x) lapply(slot(x, &quot;Polygons&quot;), function(y) slot(y, &quot;coords&quot;)))) gom.lonlat =as.data.frame(lapply(slot(gom, &quot;polygons&quot;), function(x) lapply(slot(x, &quot;Polygons&quot;), function(y) slot(y, &quot;coords&quot;)))) mab.lonlat =as.data.frame(lapply(slot(mab, &quot;polygons&quot;), function(x) lapply(slot(x, &quot;Polygons&quot;), function(y) slot(y, &quot;coords&quot;)))) scs.lonlat =as.data.frame(lapply(slot(scs, &quot;polygons&quot;), function(x) lapply(slot(x, &quot;Polygons&quot;), function(y) slot(y, &quot;coords&quot;)))) gom.gbk.lonlat =as.data.frame(lapply(slot(gom.gbk.shp, &quot;polygons&quot;), function(x) lapply(slot(x, &quot;Polygons&quot;), function(y) slot(y, &quot;coords&quot;)))) NES.lonlat =as.data.frame(NES.shp@polygons[[1]]@Polygons[[1]]@coords)# create matrix to use in in.out function # create matrix to use in in.out function from package &#39;mgcv&#39; gom.mat=as.matrix(gom.lonlat) gbk.mat=as.matrix(gbk.lonlat) mab.mat=as.matrix(mab.lonlat) scs.mat=as.matrix(scs.lonlat) gom.gbk.mat=as.matrix(gom.gbk.lonlat) # assign samples to EPU m4=as.matrix(ZPD[,6:5]) #lon,lat from ZPD ZPD$epu=NA ZPD$epu[which(in.out(gbk.mat, m4))]=&#39;GBK&#39; ZPD$epu[which(in.out(gom.mat, m4))]=&#39;GOM&#39; ZPD$epu[which(in.out(scs.mat, m4))]=&#39;SCS&#39; ZPD$epu[which(in.out(mab.mat, m4))]=&#39;MAB&#39; test=ZPD[is.na(ZPD$epu),] #unassigned nms=data.frame(colnames(ZPD)) # column names of orginal data # limit data set to zooplankton from 1977 on ZPDb=ZPD[,c(seq(1,14,1), seq(290,297,1), seq(106,197,1))] # check to make sure these are correct against &#39;nms&#39; if data source changes!!! ZPDb=ZPDb[order(ZPDb$date),] ZPDb=ZPDb[which(ZPDb$year &gt; 1976),] # remove NA data in years prior to 1977 # Select only taxa present in yearly data &gt; x percent of samples X=20 # percent criteria to use as minimum percent in samples ZPDa=ZPDb ZPDa=ZPDa[!is.na(ZPDa$zoo_gear),] # Remove NA in zooplankton rows # Reduce to taxa occurrance &gt; x percent in samples p.a=ZPDa[,24:114] p.a[p.a &gt; 0]=1 # presence/absence count=colSums(p.a) pct=(count/dim(ZPDa)[1])*100 crit=which(pct&gt;X) ZPDa=ZPDa[c(1:23,crit+23)] # data limited to taxa occurring in &gt; X percent of samples #Take median date from each cruise and assign cruise to bimonth for bi-monthly means aggregation cruises=unique(ZPDa$cruise_name) for (i in 1:length(cruises)){ ZPDa$medmonth[ZPDa$cruise_name == cruises[i]]=median(ZPDa$DOY[ZPDa$cruise_name == cruises[i]]) } ZPDa$bmm=NA ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(0,59))]=1 ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(60,120))]=3 ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(121,181))]=5 ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(182,243))]=7 ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(244,304))]=9 ZPDa$bmm[which(as.integer(ZPDa$medmonth) %in% seq(305,366))]=11 ZPDa[,14]=as.numeric(ZPDa[,14]) ZPDa[,23]=as.numeric(ZPDa[,23]) ZPDsave=ZPDa #from above routine, Yearly (all data) SEASON=&#39;Yearly&#39; ZPDa=ZPDsave # LOG transform data using ZPDa from above (select season first) test=log10(ZPDa[,24:50]+1) #choose columns with zooplankton data ZPDlog=ZPDa ZPDlog[,24:50]=test nm=matrix(colnames(ZPDlog)) area=&#39;GBK&#39; gbk.yr.spln=data.frame() for (i in 23:50){ num=i name=nm[num,1] mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T) func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method=&quot;natural&quot;, ties = mean) x.daily=func(seq(1, 12, 0.0302)) #365 days gbk.yr.spln=rbind(gbk.yr.spln,x.daily) } gbk.yr.spln=t(gbk.yr.spln) rownames(gbk.yr.spln)=seq(1:365); colnames(gbk.yr.spln)=nm[23:50,1] area=&#39;GOM&#39; gom.yr.spln=data.frame() for (i in 23:50){ num=i name=nm[num,1] mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T) func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method=&quot;natural&quot;, ties = mean) x.daily=func(seq(1, 12, 0.0302)) #365 days gom.yr.spln=rbind(gom.yr.spln,x.daily) } gom.yr.spln=t(gom.yr.spln) rownames(gom.yr.spln)=seq(1:365); colnames(gom.yr.spln)=nm[23:50,1] area=&#39;MAB&#39; mab.yr.spln=data.frame() for (i in 23:50){ num=i name=nm[num,1] mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T) func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method=&quot;natural&quot;, ties = mean) x.daily=func(seq(1, 12, 0.0302)) #365 days mab.yr.spln=rbind(mab.yr.spln,x.daily) } mab.yr.spln=t(mab.yr.spln) rownames(mab.yr.spln)=seq(1:365); colnames(mab.yr.spln)=nm[23:50,1] area=&#39;SCS&#39; scs.yr.spln=data.frame() for (i in 23:50){ num=i name=nm[num,1] mean.loc.x=aggregate(ZPDlog[which(ZPDlog$epu==area),num], by=list(ZPDlog$bmm[which(ZPDlog$epu==area)]), FUN=mean, na.rm=T) func = splinefun(mean.loc.x[,1], y=mean.loc.x[,2], method=&quot;natural&quot;, ties = mean) x.daily=func(seq(1, 12, 0.0302)) #365 days scs.yr.spln=rbind(scs.yr.spln,x.daily) } scs.yr.spln=t(scs.yr.spln) rownames(scs.yr.spln)=seq(1:365); colnames(scs.yr.spln)=nm[23:50,1] # Subtract mean expected value from observed abundance to get anomaly gbk.anom=ZPDlog[which(ZPDlog$epu==&#39;GBK&#39;),] gom.anom=ZPDlog[which(ZPDlog$epu==&#39;GOM&#39;),] mab.anom=ZPDlog[which(ZPDlog$epu==&#39;MAB&#39;),] scs.anom=ZPDlog[which(ZPDlog$epu==&#39;SCS&#39;),] gbk.anom.b=data.frame(matrix(NA, nrow = dim(gbk.anom)[1], ncol = dim(gbk.anom)[2])) for (i in 1:dim(gbk.anom)[1]){ gbk.anom.b[i,23:50]=gbk.anom[i,23:50]-gbk.yr.spln[which(gbk.anom$DOY[i]==rownames(gbk.yr.spln)),] } gom.anom.b=data.frame(matrix(NA, nrow = dim(gom.anom)[1], ncol = dim(gom.anom)[2])) for (i in 1:dim(gom.anom)[1]){ gom.anom.b[i,23:50]=gom.anom[i,23:50]-gom.yr.spln[which(gom.anom$DOY[i]==rownames(gom.yr.spln)),] } mab.anom.b=data.frame(matrix(NA, nrow = dim(mab.anom)[1], ncol = dim(mab.anom)[2])) for (i in 1:dim(mab.anom)[1]){ mab.anom.b[i,23:50]=mab.anom[i,23:50]-mab.yr.spln[which(mab.anom$DOY[i]==rownames(mab.yr.spln)),] } scs.anom.b=data.frame(matrix(NA, nrow = dim(scs.anom)[1], ncol = dim(scs.anom)[2])) for (i in 1:dim(scs.anom)[1]){ scs.anom.b[i,23:50]=scs.anom[i,23:50]-scs.yr.spln[which(scs.anom$DOY[i]==rownames(scs.yr.spln)),] } scs.anom.b=rbind(scs.anom.b,test) # Aggregrate by Year, Yearly anomaly by epu gbk.yr.anom=aggregate(gbk.anom.b, by=list(gbk.anom$year), FUN=mean, na.rm=T); rownames(gbk.yr.anom)=gbk.yr.anom[,1]; gbk.yr.anom[,1]=NULL; colnames(gbk.yr.anom)=colnames(gbk.anom) gom.yr.anom=aggregate(gom.anom.b, by=list(gom.anom$year), FUN=mean, na.rm=T); rownames(gom.yr.anom)=gom.yr.anom[,1]; gom.yr.anom[,1]=NULL; colnames(gom.yr.anom)=colnames(gom.anom) mab.yr.anom=aggregate(mab.anom.b, by=list(mab.anom$year), FUN=mean, na.rm=T); rownames(mab.yr.anom)=mab.yr.anom[,1]; mab.yr.anom[,1]=NULL; colnames(mab.yr.anom)=colnames(mab.anom) scs.yr.anom=aggregate(scs.anom.b, by=list(scs.anom$year), FUN=mean, na.rm=T); rownames(scs.yr.anom)=scs.yr.anom[,1]; scs.yr.anom[,1]=NULL; colnames(scs.yr.anom)=colnames(scs.anom) # Small-Large body copepod abundance anomaly lgtx=c(25) #column for Calanus finmarchicus smtx=c(26,24,28,27) #Pseudocalanus spp, Centropoges typicus, Centropages hamatus, Temora longicornis dataTsm=gbk.yr.anom[,smtx] dataTlg=gbk.yr.anom[,lgtx] anom.gbk=rowMeans(dataTsm)-(dataTlg) dataTsm=gom.yr.anom[,smtx] dataTlg=gom.yr.anom[,lgtx] anom.gom=rowMeans(dataTsm)-(dataTlg) dataTsm=mab.yr.anom[,smtx] dataTlg=mab.yr.anom[,lgtx] anom.mab=rowMeans(dataTsm)-(dataTlg) dataTsm=scs.yr.anom[,smtx] dataTlg=scs.yr.anom[,lgtx] anom.scs=rowMeans(dataTsm)-(dataTlg) Seasonal abundance Time series of zooplankton abundance in the spring and fall months have been presented in the 2019 Mid-Atlantic State of the Ecosystem report. Raw abundance data were sourced from the EcoMon cruises referenced above, and ordinary kriging was used to estimate seasonal abundance over the Northeast Shelf. These data were then aggregated further into time series of mean abundance by Ecological Production Unit. These data are presented in Figure 35.2. 35.1.4 Data processing Zooplankton abundances indicators were formatted for inclusion in the ecodata R package using the following code. Abundance anomaly # Process zooplankton abundance anomalies and small-large index library(dplyr) library(tidyr) raw.dir &lt;- here::here(&quot;data-raw&quot;) get_zoo_anom_sli &lt;- function(save_clean = F){ load(file.path(raw.dir, &quot;1977_2017_SLI_Calfin_Pseudo_Ctyp.Rdata&quot;)) zoo_anom_sli &lt;- Zooplankton_Primary_Prod %&gt;% gather(.,Var, Value, -year) %&gt;% dplyr::rename(Time = year) %&gt;% separate(., Var, c(&quot;Var&quot;,&quot;EPU&quot;)) %&gt;% mutate(EPU = plyr::mapvalues(EPU, from = c(&quot;gom&quot;,&quot;gbk&quot;,&quot;scs&quot;,&quot;mab&quot;), to = c(&quot;GOM&quot;,&quot;GB&quot;,&quot;SS&quot;,&quot;MAB&quot;)), Var = plyr::mapvalues(Var, from = c(&quot;PseAnom&quot;, &quot;CtyAnom&quot;, &quot;CalAnom&quot;, &quot;SLI&quot;), to = c(&quot;pseudocalanus anomaly&quot;, &quot;centropages anomaly&quot;, &quot;calanus anomaly&quot;, &quot;small-large index&quot;)), Units = &quot;anomaly&quot;) if (save_clean){ usethis::use_data(zoo_anom_sli, overwrite = T) } else { return(zoo_anom_sli) } } get_zoo_anom_sli(save_clean = T) Seasonal abundance # A general function to process various OI data sets library(dplyr) library(tidyr) library(lubridate) library(raster) library(rgdal) library(sf) process_oi &lt;- function(variable, type = NULL, season, genus = NULL, epu){ if(!is.null(type) &amp; !variable %in% c(&quot;salinity&quot;,&quot;temperature&quot;)){ stop(&#39;type only applicable for variables &quot;salinity&quot; and &quot;temperature&quot; as type = &quot;bottom&quot; or type = &quot;surface&quot;&#39;) } if(!is.null(genus) &amp; variable != &quot;zooplankton&quot;){ stop(&#39;genus only applicable for variable &quot;zooplankton&quot;&#39;) } #get compiled down-sampled raster of chosen strata from shapefile. epumask.raster &lt;- get(paste0(tolower(epu),&quot;_rast&quot;)) #get bottom temp data and find mean for stock area-------------------------------------- indir &lt;- here::here(&quot;inst&quot;,&quot;extdata&quot;,&quot;gridded&quot;) if (variable == &quot;salinity&quot;){ load(file.path(indir, paste0(&quot;sal_&quot;,type,&quot;_&quot;,season,&quot;_spdf.rdata&quot;))) } else if (variable == &quot;temperature&quot;){ load(file.path(indir, paste0(&quot;temp_&quot;,type,&quot;_&quot;,season,&quot;_spdf.rdata&quot;))) } else if (variable == &quot;chlorophyll&quot;){ load(file.path(indir, paste0(&quot;chl_&quot;,season,&quot;_1997-2018.rdata&quot;))) } else if (variable == &quot;zooplankton&quot;){ load(file.path(indir, paste0(genus,&quot;_&quot;,season,&quot;_zoo_1977-2016.rdata&quot;))) } #create null df to fill with results data = data.frame(array(NA,dim= c(raster::nlayers(ecsa_dat),5))) #loops through layers in raster brick for(i in 1:raster::nlayers(ecsa_dat)){ #load raster by year #get file information from title layer_id &lt;- stringr::str_extract(names(ecsa_dat)[[i]], &quot;\\d.*&quot;) layer_id &lt;- stringr::str_split(layer_id, &quot;_&quot;) data[i,1] &lt;- layer_id[[1]][[1]] data[i,2] &lt;- layer_id[[1]][[2]] data[i,3] &lt;- layer_id[[1]][[3]] #trim to stock area masked.raster = ecsa_dat[[i]]*epumask.raster #find mean BT of stock area data[i,4] = raster::cellStats(masked.raster, stat=&#39;mean&#39;, na.rm=TRUE) data[i,5] = raster::cellStats(masked.raster, stat = &#39;sd&#39;, na.rm=TRUE) # # if (layer_id[[1]][[1]] == &quot;1995&quot;){ # break # } } x &lt;- as.numeric(data$X1) y.out &lt;- data$X4 y.sd &lt;- data$X5 sd.low &lt;- y.out - y.sd sd.high &lt;- y.out + y.sd # remove if (variable == &quot;zooplankton&quot;){ if (season == &quot;spring&quot;){ y.out[x %in% c(1989, 1990, 1991, 1994)] &lt;- NA sd.low[x %in% c(1989, 1990, 1991, 1994)] &lt;- NA sd.high[x %in% c(1989, 1990, 1991, 1994)] &lt;- NA } else if (season == &quot;fall&quot;) { y.out[x %in% c(1989, 1990, 1992)] &lt;- NA sd.low[x %in% c(1989, 1990, 1992)] &lt;- NA sd.high[x %in% c(1989, 1990, 1992)] &lt;- NA } } if(variable == &quot;chlorophyll&quot;){ type &lt;- &quot;&quot; } else if (variable == &quot;zooplankton&quot;){ type &lt;- genus variable &lt;- &quot;zoo&quot; } out &lt;- data.frame(Var = paste(paste(type,variable),season), Time = as.numeric(x), Value = y.out, sd.low = sd.low, sd.high = sd.high, Season = season, epu = epu) out &lt;- out[out$Time &gt; 1968,] return(out) } # Process optimally interpolated EcoMon zooplankton data # # These data show estimated zooplankton abundances on the NE Shelf derived from Ecosystem Monitoring Program (EcoMon) # sampling. EcoMon conducts shelf-wide bimonthly surveys of the Northeast Large Marine Ecosystem, collecting # and ichthyoplankton to a depth of 200 m using paired Bongo samplers with 333 $mu$m mesh netting. Zooplankton # abundance data were interpolated across sampling locations using ordinary kriging to create a complete field. # Here we present abundance time series for three species: *Centropages typicus*, *Temora longicornis*, and # *Pseudocalanus* spp. These data are processed in a similar manner to optimally interpolated ocean temperature and # salinity data sets. More information about the source data and processing methods used to derive these data sets # can be found at https://noaa-edab.github.io/ECSA/#sec:methodszoo. library(tidyr) library(dplyr) r.dir &lt;- here::here(&quot;data-raw&quot;) source(file.path(r.dir, &quot;process_oi.R&quot;)) if (!all(exists(&quot;mab_rast&quot;), exists(&quot;gb_rast&quot;), exists(&quot;gom_rast&quot;))){ message(&quot;Loading EPU rasters.&quot;) source(file.path(r.dir, &quot;create_epu_mask_oi.R&quot;)) } else { message(&quot;All EPU rasters exist; skipping load step.&quot;) } get_zoo_oi &lt;- function(save_clean = F){ #MAB--------------------------------------------------------------------------------------------------- ctyp_fall_mab &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;centropages&quot;, season = &quot;fall&quot;, epu = &quot;MAB&quot;) ctyp_spring_mab &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;centropages&quot;, season = &quot;spring&quot;, epu = &quot;MAB&quot;) tlong_fall_mab &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;temora&quot;, season = &quot;fall&quot;, epu = &quot;MAB&quot;) tlong_spring_mab &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;temora&quot;, season = &quot;spring&quot;, epu = &quot;MAB&quot;) pseudo_fall_mab &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;pseudocalanus&quot;, season = &quot;fall&quot;, epu = &quot;MAB&quot;) pseudo_spring_mab &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;pseudocalanus&quot;, season = &quot;spring&quot;, epu = &quot;MAB&quot;) #GB--------------------------------------------------------------------------------------------------- ctyp_fall_gb &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;centropages&quot;, season = &quot;fall&quot;, epu = &quot;GB&quot;) ctyp_spring_gb &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;centropages&quot;, season = &quot;spring&quot;, epu = &quot;GB&quot;) tlong_fall_gb &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;temora&quot;, season = &quot;fall&quot;, epu = &quot;GB&quot;) tlong_spring_gb &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;temora&quot;, season = &quot;spring&quot;, epu = &quot;GB&quot;) pseudo_fall_gb &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;pseudocalanus&quot;, season = &quot;fall&quot;, epu = &quot;GB&quot;) pseudo_spring_gb &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;pseudocalanus&quot;, season = &quot;spring&quot;, epu = &quot;GB&quot;) #GOM--------------------------------------------------------------------------------------------------- ctyp_fall_gom &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;centropages&quot;, season = &quot;fall&quot;, epu = &quot;GOM&quot;) ctyp_spring_gom &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;centropages&quot;, season = &quot;spring&quot;, epu = &quot;GOM&quot;) tlong_fall_gom &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;temora&quot;, season = &quot;fall&quot;, epu = &quot;GOM&quot;) tlong_spring_gom &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;temora&quot;, season = &quot;spring&quot;, epu = &quot;GOM&quot;) pseudo_fall_gom &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;pseudocalanus&quot;, season = &quot;fall&quot;, epu = &quot;GOM&quot;) pseudo_spring_gom &lt;- process_oi(variable = &quot;zooplankton&quot;, genus = &quot;pseudocalanus&quot;, season = &quot;spring&quot;, epu = &quot;GOM&quot;) #Bind all zoo output. Be sure to include any new species here zoo_oi &lt;- NULL for (i in ls()){ if (stringr::str_detect(i,&quot;ctyp|tlong|pseudo&quot;)){ assign(&quot;zoo_oi&quot;, rbind(zoo_oi, get(i))) } } #Process output from masked rasters (mean, +/- 1 SD). Adjust output in process_oi(), not here. zoo_oi &lt;- zoo_oi %&gt;% dplyr::rename(EPU = epu) %&gt;% dplyr::select(-Season) %&gt;% mutate(Units = &quot;log N m^-3&quot;) #Split out +/- SD because too lazy to gather ;) sd.low &lt;- zoo_oi %&gt;% dplyr::select(-Value, -sd.high) %&gt;% mutate(Var = as.factor(paste(Var, &quot;- 1 SD&quot;))) %&gt;% dplyr::rename(Value = sd.low) sd.high &lt;- zoo_oi %&gt;% dplyr::select(-Value, -sd.low) %&gt;% mutate(Var = as.factor(paste(Var, &quot;+ 1 SD&quot;))) %&gt;% dplyr::rename(Value = sd.high) zoo_oi &lt;- zoo_oi %&gt;% dplyr::select(-sd.high,-sd.low) #Bind all data zoo_oi &lt;- rbind(zoo_oi, sd.high, sd.low) %&gt;% as.data.frame() if (save_clean){ usethis::use_data(zoo_oi, overwrite = T) } else { return(zoo_oi) } } 35.1.5 Plotting Abundance anomaly Figure 35.1: Centropages typicus abundance anomaly in the Mid-Atlantic Bight. Seasonal Abundance Figure 35.2: Seasonal abundance of key zooplankton species in the MAB. References "],
["references.html", "References", " References "]
]
